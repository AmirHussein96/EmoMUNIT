{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data: .wav -> Pitch contour (f0s), Harmonic spectral envelope (sps), Aperiodic spectral envelope (aps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from ops import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "from IPython.display import Audio\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# matplotlib.rcParams['figure.figsize'] = (16, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder: Style_Encoder, Content_Encoder, MLP, Decoder, Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Style_Encoder(inputs, style_dim=16, reuse=False, scope='style_encoder'):                                                            # [1, 24, 128] = [batch_size, feature_channel, time]\n",
    "\n",
    "    inputs = tf.transpose(inputs, perm=[0, 2, 1], name='input_transpose')                                                               # [1, 128, 24] = [batch_size, time, feature_channel]\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "\n",
    "        h1 = conv1d_layer(inputs=inputs, filters=128, kernel_size=15, strides=1, name='h1_conv')                                        # [1, 128, 128]\n",
    "        h1_gates = conv1d_layer(inputs=inputs, filters=128, kernel_size=15, strides=1, name='h1_conv_gates')\n",
    "        h1_glu = gated_linear_layer(inputs=h1, gates=h1_gates, name='h1_glu')\n",
    "\n",
    "        # Downsample\n",
    "        d1 = downsample1d_block_withoutIN(inputs=h1_glu, filters=256, kernel_size=5, strides=2, name_prefix='downsample1d_block1')      # [1, 64, 256]\n",
    "        d2 = downsample1d_block_withoutIN(inputs=d1, filters=512, kernel_size=5, strides=2, name_prefix='downsample1d_block2')          # [1, 32, 512]\n",
    "\n",
    "        d3 = downsample1d_block_withoutIN(inputs=d2, filters=512, kernel_size=3, strides=2, name_prefix='downsample1d_block3')          # [1, 16, 512]\n",
    "        d4 = downsample1d_block_withoutIN(inputs=d3, filters=512, kernel_size=3, strides=2, name_prefix='downsample1d_block4')          # [1, 8, 512]\n",
    "\n",
    "        # Global Average Pooling\n",
    "        p1 = adaptive_avg_pooling(d4)                                                                                                   # [1, 1, 512]\n",
    "        style = conv1d_layer(inputs=p1, filters=style_dim, kernel_size=1, strides=1, name='SE_logit')                                   # [1, 1, 16]\n",
    "\n",
    "        return style                                                                                                                    # [1, 1, 16]\n",
    "\n",
    "\n",
    "def Content_Encoder(inputs, reuse=False, scope='content_encoder'):\n",
    "    # IN removes the original feature mean and variance that represent important style information\n",
    "    inputs = tf.transpose(inputs, perm=[0, 2, 1], name='input_transpose')                                                               # [1, 24, 128] = [batch_size, time, feature_channel]\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "\n",
    "        h1 = conv1d_layer(inputs=inputs, filters=128, kernel_size=15, strides=1, name='h1_conv')                                        # [1, 128, 128]\n",
    "        h1_norm = instance_norm_layer(inputs=h1, name='h1_norm')\n",
    "        h1_gates = conv1d_layer(inputs=inputs, filters=128, kernel_size=15, strides=1, name='h1_gates')\n",
    "        h1_norm_gates = instance_norm_layer(inputs=h1_gates, name='h1_norm_gates')\n",
    "        h1_glu = gated_linear_layer(inputs=h1_norm, gates=h1_norm_gates, name='h1_glu')\n",
    "\n",
    "        # downsample\n",
    "        d1 = downsample1d_block(inputs=h1_glu, filters=256, kernel_size=5, strides=2, name_prefix='downsample1d_block1')                # [1, 64, 256]\n",
    "        d2 = downsample1d_block(inputs=d1, filters=512, kernel_size=5, strides=2, name_prefix='downsample1d_block2')                    # [1, 32, 512]\n",
    "               \n",
    "        # Residual blocks\n",
    "        r1 = residual1d_block(inputs=d2, filters = 512, kernel_size=3, strides=1, name_prefix='residual1d_block1')                      # [1, 32, 512]\n",
    "        r2 = residual1d_block(inputs=r1, filters = 512, kernel_size=3, strides=1, name_prefix='residual1d_block2')\n",
    "        r3 = residual1d_block(inputs=r2, filters = 512, kernel_size=3, strides=1, name_prefix='residual1d_block3')\n",
    "        content = residual1d_block(inputs=r3, filters=512, kernel_size=3, strides=1, name_prefix='residual1d_block4')\n",
    "\n",
    "        return content                                                                                                                  # [1, 32, 512]\n",
    "\n",
    "\n",
    "def MLP(style, reuse=False, scope='MLP'):                                                                                               # [1, 1, 16]\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "\n",
    "        x1 = linear(style, 512, scope='linear_1')                                                                                       # [1, 1, 512]\n",
    "        x1_gates = linear(x1, 512, scope='linear_1_gates')\n",
    "        x1_glu = gated_linear_layer(inputs=x1, gates=x1_gates, name='x1_glu')\n",
    "\n",
    "        x2 = linear(x1_glu, 512, scope='linear_2')\n",
    "        x2_gates = linear(x2, 512, scope='linear_2_gates')\n",
    "        x2_glu = gated_linear_layer(inputs=x2, gates=x2_gates, name='x2_glu')\n",
    "\n",
    "        mu = linear(x2_glu, 512, scope='mu')\n",
    "        sigma = linear(x2_glu, 512, scope='sigma')\n",
    "\n",
    "        mu = tf.reshape(mu, shape=[-1, 1, 512])                                                                                         # [1, 1, 512]\n",
    "        sigma = tf.reshape(sigma, shape=[-1, 1, 512])                                                                                   # [1, 1, 512]\n",
    "\n",
    "        return mu, sigma                                                                                                                # [1, 1, 512]\n",
    "\n",
    "\n",
    "def Decoder(content, style, reuse=False, scope=\"decoder\"):\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "\n",
    "        mu, sigma = MLP(style, reuse)                                                                                                   # [1, 1, 512]\n",
    "        x = content                                                                                                                     # [1, 32, 512]\n",
    "\n",
    "        # Adaptive Residual blocks\n",
    "        r1 = residual1d_block_adaptive(inputs=x, filters=512, mu=mu, sigma=sigma, kernel_size=3, strides=1, name_prefix='residual1d_block1')        # [1, 32, 512]\n",
    "        r2 = residual1d_block_adaptive(inputs=r1, filters=512, mu=mu, sigma=sigma, kernel_size=3, strides=1, name_prefix='residual1d_block2')\n",
    "        r3 = residual1d_block_adaptive(inputs=r2, filters=512, mu=mu, sigma=sigma, kernel_size=3, strides=1, name_prefix='residual1d_block3')\n",
    "\n",
    "        # Upsample\n",
    "        u1 = upsample1d_block(inputs=r3, filters=512, kernel_size=5, strides=1, shuffle_size=2, name_prefix='upsample1d_block1')        # [1, 64, 512]\n",
    "        u2 = upsample1d_block(inputs=u1, filters=256, kernel_size=5, strides=1, shuffle_size=2, name_prefix='upsample1d_block2')        # [1, 128, 256]\n",
    "\n",
    "        # Output\n",
    "        o1 = conv1d_layer(inputs=u2, filters=24, kernel_size=15, strides=1, name='o1_conv')                                             # [1, 128, 24]\n",
    "        o2 = tf.transpose(o1, perm=[0, 2, 1], name='output_transpose')                                                                  # [1, 24, 128]\n",
    "\n",
    "        return o2                                                                                                                       # [1, 24, 128] = [batch_size, feature_channel, time]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator(inputs, reuse=False, scope='discriminator'):\n",
    "\n",
    "    # inputs = [batch_size, num_features, time]\n",
    "    # add channel for 2D convolution [batch_size, num_features, time, 1]\n",
    "    inputs = tf.expand_dims(inputs, -1)                                                                                                 # [1, 24, 128, 1]\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "\n",
    "        h1 = conv2d_layer(inputs=inputs, filters=128, kernel_size=[3, 3], strides=[1, 2], name='h1_conv')                               # [1, 24, 64, 128]\n",
    "        h1_gates = conv2d_layer(inputs=inputs, filters=128, kernel_size=[3, 3], strides=[1, 2], name='h1_conv_gates')\n",
    "        h1_glu = gated_linear_layer(inputs=h1, gates=h1_gates, name='h1_glu')\n",
    "\n",
    "        # Downsample\n",
    "        d1 = downsample2d_block(inputs=h1_glu, filters=256, kernel_size=[3, 3], strides=[2, 2], name_prefix='downsample2d_block1')      # [1, 12, 32, 256]\n",
    "        d2 = downsample2d_block(inputs=d1, filters=512, kernel_size=[3, 3], strides=[2, 2], name_prefix='downsample2d_block2')          # [1, 6, 16, 512]\n",
    "        d3 = downsample2d_block(inputs=d2, filters=1024, kernel_size=[6, 3], strides=[1, 2], name_prefix='downsample2d_block3')         # [1, 6, 8, 1024]\n",
    "\n",
    "        # Output\n",
    "        o1 = tf.layers.dense(inputs=d3, units=1, activation=tf.nn.sigmoid)\n",
    "\n",
    "        return [o1]                                                                                                                       # [1, 6, 8, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# Model\n",
    "##################################################################################\n",
    "\n",
    "def Encoder_A(x_A, reuse=False):\n",
    "    style_A = Style_Encoder(x_A, reuse=reuse, scope='style_encoder_A')\n",
    "    content_A = Content_Encoder(x_A, reuse=reuse, scope='content_encoder_A')\n",
    "\n",
    "    return content_A, style_A\n",
    "\n",
    "def Encoder_B(x_B, reuse=False):\n",
    "    style_B = Style_Encoder(x_B, reuse=reuse, scope='style_encoder_B')\n",
    "    content_B = Content_Encoder(x_B, reuse=reuse, scope='content_encoder_B')\n",
    "\n",
    "    return content_B, style_B\n",
    "\n",
    "def Decoder_A(content_B, style_A, reuse=False):\n",
    "    x_ba = Decoder(content=content_B, style=style_A, reuse=reuse, scope='decoder_A')\n",
    "\n",
    "    return x_ba\n",
    "\n",
    "def Decoder_B(content_A, style_B, reuse=False):\n",
    "    x_ab = Decoder(content=content_A, style=style_B, reuse=reuse, scope='decoder_B')\n",
    "\n",
    "    return x_ab\n",
    "\n",
    "def discriminate_real(x_A, x_B):\n",
    "    real_A_logit = Discriminator(x_A, scope=\"discriminator_A\")\n",
    "    real_B_logit = Discriminator(x_B, scope=\"discriminator_B\")\n",
    "\n",
    "    return real_A_logit, real_B_logit\n",
    "\n",
    "def discriminate_fake(x_ba, x_ab):\n",
    "    fake_A_logit = Discriminator(x_ba, reuse=True, scope=\"discriminator_A\")\n",
    "    fake_B_logit = Discriminator(x_ab, reuse=True, scope=\"discriminator_B\")\n",
    "\n",
    "    return fake_A_logit, fake_B_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module: EmoMUNIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoMUNIT(object):\n",
    "    def __init__(self, sess):\n",
    "        \n",
    "        self.train_A_dir = './../../../Database/Emotion/hap_neu/hap'\n",
    "        self.train_B_dir = './../../../Database/Emotion/hap_neu/neu'\n",
    "        self.validation_A_dir = './../../../Database/Emotion/hap_neu/val_hap'\n",
    "        self.validation_B_dir = './../../../Database/Emotion/hap_neu/val_neu'\n",
    "#         self.max_samples = 1000\n",
    "        \n",
    "        self.batch_size = 1\n",
    "        self.style_dim = 16\n",
    "        \n",
    "        self.Encoder_A = Encoder_A\n",
    "        self.Encoder_B = Encoder_B\n",
    "        self.Decoder_A = Decoder_A\n",
    "        self.Decoder_B = Decoder_B\n",
    "        self.discriminate_real = discriminate_real\n",
    "        self.discriminate_fake = discriminate_fake\n",
    "        \n",
    "        self.recon_x_cyc_w = 0.0\n",
    "        self.gan_type = 'lsgan'\n",
    "        \n",
    "        self.gan_w = 1.0\n",
    "        \n",
    "        self.recon_x_w = 10.0\n",
    "        self.recon_s_w = 1.0\n",
    "        self.recon_c_w = 1.0\n",
    "        self.recon_x_cyc_w = 0.0\n",
    "               \n",
    "        self.audio_len = 128    # = n_frames, time_length\n",
    "        self.audio_ch = 24      # = num_mcep, num_features\n",
    "        \n",
    "        self.direction = 'A2B'\n",
    "        \n",
    "        self.model_name = 'EmoMUNIT'\n",
    "        self.gan_type = 'lsgan'\n",
    "        self.dataset_name = 'hap2neu'\n",
    "        self.log_dir = 'logs'\n",
    "        self.sample_dir = 'samples'\n",
    "        self.checkpoint_dir = 'checkpoint'\n",
    "        self.iteration = 1000\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.epoch = 1000\n",
    "        self.init_lr_D = 0.00005\n",
    "        self.init_lr_G = 0.0001\n",
    "        \n",
    "        self.print_freq = 1000\n",
    "        self.save_freq = 1000\n",
    "        \n",
    "    \n",
    "    def build_model(self):\n",
    "        self.lr_D = tf.placeholder(tf.float32, name='learning_rate_D')\n",
    "        self.lr_G = tf.placeholder(tf.float32, name='learning_rate_G')\n",
    "        \n",
    "        # Iterate from train_data_A and train_data_A\n",
    "        self.domain_A = tf.placeholder(tf.float32, shape=[self.batch_size, self.audio_ch, self.audio_len], name='domain_a')\n",
    "        self.domain_B = tf.placeholder(tf.float32, shape=[self.batch_size, self.audio_ch, self.audio_len], name='domain_b')\n",
    "    \n",
    "        self.style_a = tf.placeholder(tf.float32, shape=[self.batch_size, 1, self.style_dim], name='style_a')\n",
    "        self.style_b = tf.placeholder(tf.float32, shape=[self.batch_size, 1, self.style_dim], name='style_b')  \n",
    "    \n",
    "        # encode\n",
    "        content_a, style_a_prime = self.Encoder_A(self.domain_A)\n",
    "        content_b, style_b_prime = self.Encoder_B(self.domain_B)\n",
    "\n",
    "        # decode (within domain)\n",
    "        x_aa = self.Decoder_A(content_B=content_a, style_A=style_a_prime)\n",
    "        x_bb = self.Decoder_B(content_A=content_b, style_B=style_b_prime)\n",
    "    \n",
    "        # decode (cross domain)\n",
    "        x_ba = self.Decoder_A(content_B=content_b, style_A=self.style_a, reuse=True)\n",
    "        x_ab = self.Decoder_B(content_A=content_a, style_B=self.style_b, reuse=True)   \n",
    "    \n",
    "        # encode again\n",
    "        content_b_, style_a_ = self.Encoder_A(x_ba, reuse=True)\n",
    "        content_a_, style_b_ = self.Encoder_B(x_ab, reuse=True)    \n",
    "    \n",
    "        # decode again (if needed)\n",
    "        if self.recon_x_cyc_w > 0 :\n",
    "            x_aba = self.Decoder_A(content_B=content_a_, style_A=style_a_prime, reuse=True)\n",
    "            x_bab = self.Decoder_B(content_A=content_b_, style_B=style_b_prime, reuse=True)\n",
    "\n",
    "            cyc_recon_A = L1_loss(x_aba, self.domain_A)\n",
    "            cyc_recon_B = L1_loss(x_bab, self.domain_B)\n",
    "\n",
    "        else :\n",
    "            cyc_recon_A = 0.0\n",
    "            cyc_recon_B = 0.0    \n",
    "      \n",
    "        real_A_logit, real_B_logit = self.discriminate_real(self.domain_A, self.domain_B)\n",
    "        fake_A_logit, fake_B_logit = self.discriminate_fake(x_ba, x_ab)   \n",
    "    \n",
    "    \n",
    "        \"\"\" Define Loss \"\"\"\n",
    "        # Adversarial Loss\n",
    "        G_ad_loss_a = generator_loss(self.gan_type, fake_A_logit)\n",
    "        G_ad_loss_b = generator_loss(self.gan_type, fake_B_logit)\n",
    "    \n",
    "        # Discrimination Loss (real/fake)\n",
    "        D_ad_loss_a = discriminator_loss(self.gan_type, real_A_logit, fake_A_logit)\n",
    "        D_ad_loss_b = discriminator_loss(self.gan_type, real_B_logit, fake_B_logit)\n",
    "    \n",
    "        # Reconstruction Loss\n",
    "        recon_A = L1_loss(x_aa, self.domain_A) # reconstruction\n",
    "        recon_B = L1_loss(x_bb, self.domain_B) # reconstruction   \n",
    "    \n",
    "        # Semi-CycleGAN Loss\n",
    "        # For style, encourages diverse outputs given different style codes\n",
    "        recon_style_A = L1_loss(style_a_, self.style_a)\n",
    "        recon_style_B = L1_loss(style_b_, self.style_b)\n",
    "    \n",
    "        # For content, encourages the translated image to preserve semantic content of the input image\n",
    "        recon_content_A = L1_loss(content_a_, content_a)\n",
    "        recon_content_B = L1_loss(content_b_, content_b)   \n",
    "    \n",
    "        # Attacker Loss\n",
    "        Generator_A_loss = self.gan_w * G_ad_loss_a + \\\n",
    "                                   self.recon_x_w * recon_A + \\\n",
    "                                   self.recon_s_w * recon_style_A + \\\n",
    "                                   self.recon_c_w * recon_content_A + \\\n",
    "                                   self.recon_x_cyc_w * cyc_recon_A\n",
    "\n",
    "        Generator_B_loss = self.gan_w * G_ad_loss_b + \\\n",
    "                           self.recon_x_w * recon_B + \\\n",
    "                           self.recon_s_w * recon_style_B + \\\n",
    "                           self.recon_c_w * recon_content_B + \\\n",
    "                           self.recon_x_cyc_w * cyc_recon_B   \n",
    "    \n",
    "        # Defender Loss\n",
    "        Discriminator_A_loss = self.gan_w * D_ad_loss_a\n",
    "        Discriminator_B_loss = self.gan_w * D_ad_loss_b\n",
    "    \n",
    "        # Total Loss\n",
    "        self.Generator_loss = Generator_A_loss + Generator_B_loss\n",
    "        self.Discriminator_loss = Discriminator_A_loss + Discriminator_B_loss\n",
    "    \n",
    "    \n",
    "        \"\"\" Training Variables \"\"\"\n",
    "        t_vars = tf.trainable_variables()\n",
    "        G_vars = [var for var in t_vars if 'decoder' in var.name or 'encoder' in var.name]\n",
    "        D_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
    "    \n",
    "        self.G_optim = tf.train.AdamOptimizer(self.lr_G, beta1=0.5, beta2=0.999).minimize(self.Generator_loss, var_list=G_vars)\n",
    "        self.D_optim = tf.train.AdamOptimizer(self.lr_D, beta1=0.5, beta2=0.999).minimize(self.Discriminator_loss, var_list=D_vars)\n",
    "    \n",
    "        \"\"\"\" Summary \"\"\"\n",
    "        self.all_G_loss = tf.summary.scalar(\"Generator_loss\", self.Generator_loss)\n",
    "        self.all_D_loss = tf.summary.scalar(\"Discriminator_loss\", self.Discriminator_loss)\n",
    "        self.R_A_loss = tf.summary.scalar(\"Reconstruction_A_loss\", recon_A)\n",
    "        self.R_B_loss = tf.summary.scalar(\"Reconstruction_B_loss\", recon_B)\n",
    "        self.G_A_loss = tf.summary.scalar(\"G_A_loss\", Generator_A_loss)\n",
    "        self.G_B_loss = tf.summary.scalar(\"G_B_loss\", Generator_B_loss)\n",
    "        self.D_A_loss = tf.summary.scalar(\"D_A_loss\", Discriminator_A_loss)\n",
    "        self.D_B_loss = tf.summary.scalar(\"D_B_loss\", Discriminator_B_loss)\n",
    "\n",
    "        self.G_loss = tf.summary.merge([self.R_A_loss, self.R_B_loss, self.G_A_loss, self.G_B_loss, self.all_G_loss])\n",
    "        self.D_loss = tf.summary.merge([self.D_A_loss, self.D_B_loss, self.all_D_loss])\n",
    "    \n",
    "    \n",
    "        \"\"\" Speech: real and fake \"\"\"\n",
    "        self.real_A = self.domain_A\n",
    "        self.real_B = self.domain_B\n",
    "\n",
    "        self.fake_A = x_ba\n",
    "        self.fake_B = x_ab \n",
    "    \n",
    "        \"\"\" Test Variables \"\"\"\n",
    "        self.test_audio = tf.placeholder(tf.float32, [1, self.audio_ch, self.audio_len], name='test_audio') # [1 24 128]\n",
    "        self.test_style = tf.placeholder(tf.float32, [1, 1, self.style_dim], name='test_style')             # [1 1 16]\n",
    "\n",
    "        test_content_a, _ = self.Encoder_A(self.test_audio, reuse=True)\n",
    "        test_content_b, _ = self.Encoder_B(self.test_audio, reuse=True)\n",
    "\n",
    "        self.test_fake_A = self.Decoder_A(content_B=test_content_b, style_A=self.test_style, reuse=True)\n",
    "        self.test_fake_B = self.Decoder_B(content_A=test_content_a, style_B=self.test_style, reuse=True)\n",
    "\n",
    "        \"\"\" Guided Speech Translation \"\"\"\n",
    "        self.content_audio = tf.placeholder(tf.float32, [1, self.audio_ch, self.audio_len], name='content_audio')\n",
    "        self.style_audio = tf.placeholder(tf.float32, [1, self.audio_ch, self.audio_len], name='guide_style_audio_ch')\n",
    "\n",
    "        if self.direction == 'A2B' :\n",
    "            guide_content_A, guide_style_A = self.Encoder_A(self.content_audio, reuse=True)\n",
    "            guide_content_B, guide_style_B = self.Encoder_B(self.style_audio, reuse=True)\n",
    "\n",
    "        else :\n",
    "            guide_content_B, guide_style_B = self.Encoder_B(self.content_audio, reuse=True)\n",
    "            guide_content_A, guide_style_A = self.Encoder_A(self.style_audio, reuse=True)\n",
    "\n",
    "        self.guide_fake_A = self.Decoder_A(content_B=guide_content_B, style_A=guide_style_A, reuse=True)\n",
    "        self.guide_fake_B = self.Decoder_B(content_A=guide_content_A, style_B=guide_style_B, reuse=True)\n",
    "    \n",
    "    \n",
    "    def data_prepare(self, f0s_A, f0s_B, coded_sps_norm_A, coded_sps_norm_B):\n",
    "        \n",
    "        train_data_A = sample_train_data03(sps=list(coded_sps_norm_A), f0s=list(f0s_A), n_frames=self.audio_len)\n",
    "        train_data_B = sample_train_data03(sps=list(coded_sps_norm_B), f0s=list(f0s_B), n_frames=self.audio_len)\n",
    "\n",
    "        minlen = min(len(train_data_A), len(train_data_B))\n",
    "        np.random.shuffle(train_data_A)\n",
    "        np.random.shuffle(train_data_B)\n",
    "        train_data_A = np.array(train_data_A[0:minlen])\n",
    "        train_data_B = np.array(train_data_B[0:minlen])\n",
    "\n",
    "        return train_data_A, train_data_B\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        # saver to save model\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        # summary writer\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_dir, self.sess.graph)\n",
    "        \n",
    "        # restore check-point if it exits\n",
    "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
    "        if could_load:\n",
    "            start_epoch = (int)(checkpoint_counter / self.iteration)\n",
    "            start_batch_id = checkpoint_counter - start_epoch * self.iteration\n",
    "            counter = checkpoint_counter\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            start_batch_id = 0\n",
    "            counter = 1\n",
    "            print(\" [!] Load FAILED...\")\n",
    "            \n",
    "        # Training loop for epoch\n",
    "        \n",
    "        # load data and extract features\n",
    "        f0s_A, coded_sps_norm_A, _, _ = vocoder_extract(self.train_A_dir)\n",
    "        f0s_B, coded_sps_norm_B, _, _ = vocoder_extract(self.train_B_dir)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(start_epoch, self.epoch):\n",
    "            \n",
    "            train_data_A, train_data_B = self.data_prepare(f0s_A, f0s_B, coded_sps_norm_A, coded_sps_norm_B)\n",
    "            print('Epoch[%d]: Input data sampled: train_data_A' %epoch, np.shape(train_data_A), 'train_data_B', np.shape(train_data_B))\n",
    "\n",
    "            lr_D, lr_G = self.init_lr_D * pow(0.995, epoch), self.init_lr_G * pow(0.995, epoch)\n",
    "            for idx in range(start_batch_id, self.iteration):\n",
    "                style_a = np.random.normal(loc=0.0, scale=1.0, size=[self.batch_size, 1, self.style_dim])\n",
    "                style_b = np.random.normal(loc=0.0, scale=1.0, size=[self.batch_size, 1, self.style_dim])\n",
    "                \n",
    "                idx_A = idx%len(train_data_A)\n",
    "                idx_B = idx%len(train_data_B)\n",
    "                domain_A = train_data_A[idx_A:idx_A+1].astype('float32')\n",
    "                domain_B = train_data_B[idx_B:idx_B+1].astype('float32')\n",
    "                \n",
    "                train_feed_dict = {\n",
    "                    self.style_a : style_a,\n",
    "                    self.style_b : style_b,\n",
    "                    self.lr_D : lr_D,\n",
    "                    self.lr_G : lr_G,\n",
    "                    self.domain_A : domain_A,\n",
    "                    self.domain_B : domain_B\n",
    "                }\n",
    "                \n",
    "                # Update D\n",
    "                _, d_loss, summary_str = self.sess.run([self.D_optim, self.Discriminator_loss, self.D_loss], feed_dict = train_feed_dict)\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "            \n",
    "                # Update G\n",
    "                batch_A_audios, batch_B_audios, fake_A, fake_B, _, g_loss, summary_str = \\\n",
    "                self.sess.run([self.real_A, self.real_B, self.fake_A, self.fake_B, self.G_optim, \\\n",
    "                               self.Generator_loss, self.G_loss], feed_dict = train_feed_dict)\n",
    "                self.writer.add_summary(summary_str, counter)           \n",
    "            \n",
    "                # display training status\n",
    "                counter += 1\n",
    "                print(\"Epoch: [%2d] [%6d/%6d] time: %4.4f d_loss: %.8f, g_loss: %.8f\" \\\n",
    "                      % (epoch, idx, self.iteration, time.time() - start_time, d_loss, g_loss), end='\\r')\n",
    "            \n",
    "                # save test samples\n",
    "                if np.mod(counter+1, self.print_freq) == 0 :\n",
    "                    save_audios(batch_A_audios, self.batch_size,\n",
    "                                './{}/real_A_{:02d}_{:06d}.npy'.format(self.sample_dir, epoch, counter+1))\n",
    "                    # save_audios(batch_B_audios, self.batch_size,\n",
    "                    #             './{}/real_B_{}_{:02d}_{:06d}.jpg'.format(self.sample_dir, gpu_id, epoch, idx+1))\n",
    "\n",
    "                    # save_audios(fake_A, self.batch_size,\n",
    "                    #             './{}/fake_A_{}_{:02d}_{:06d}.jpg'.format(self.sample_dir, gpu_id, epoch, idx+1))\n",
    "                    save_audios(fake_B, self.batch_size,\n",
    "                                './{}/fake_B_{:02d}_{:06d}.npy'.format(self.sample_dir, epoch, counter+1))          \n",
    "                \n",
    "                # save checkpoints\n",
    "                if np.mod(counter+1, self.save_freq) == 0 :\n",
    "                    self.save(self.checkpoint_dir, counter)\n",
    "        \n",
    "            # After an epoch, start_batch_id reset to zero\n",
    "            # non-zero value is only for the first epoch after loading pre-trained model\n",
    "            start_batch_id = 0\n",
    "\n",
    "            # save model for final step\n",
    "            self.save(self.checkpoint_dir, counter)     \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        return \"{}_{}_{}\".format(self.model_name, self.dataset_name, self.gan_type)\n",
    "    \n",
    "    \n",
    "    def load(self, checkpoint_dir):\n",
    "        import re\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\", ckpt_name)).group(0))\n",
    "            print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "            return True, counter\n",
    "        else:\n",
    "            print(\" [*] Failed to find a checkpoint\")\n",
    "            return False, 0\n",
    "        \n",
    "        \n",
    "    def save(self, checkpoint_dir, step):\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess, os.path.join(checkpoint_dir, self.model_name + '.model'), global_step=step)        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] Reading checkpoints...\n",
      " [*] Failed to find a checkpoint\n",
      " [!] Load FAILED...\n",
      "Epoch[0]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_00_001000.npyloss: 0.16079304, g_loss: 14.64988804\n",
      "(1, 24, 128) 1 ./samples/fake_B_00_001000.npy\n",
      "Epoch[1]: Input data sampled: train_data_A (442, 24, 128) train_data_B (442, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_01_002000.npyloss: 0.27265120, g_loss: 13.22365570\n",
      "(1, 24, 128) 1 ./samples/fake_B_01_002000.npy\n",
      "Epoch[2]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_02_003000.npyloss: 0.27301833, g_loss: 11.01823997\n",
      "(1, 24, 128) 1 ./samples/fake_B_02_003000.npy\n",
      "Epoch[3]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_03_004000.npyloss: 0.45006368, g_loss: 11.14856720\n",
      "(1, 24, 128) 1 ./samples/fake_B_03_004000.npy\n",
      "Epoch[4]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_04_005000.npyloss: 0.30741310, g_loss: 11.21933556\n",
      "(1, 24, 128) 1 ./samples/fake_B_04_005000.npy\n",
      "Epoch[5]: Input data sampled: train_data_A (439, 24, 128) train_data_B (439, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_05_006000.npy_loss: 0.28418899, g_loss: 9.778375639\n",
      "(1, 24, 128) 1 ./samples/fake_B_05_006000.npy\n",
      "Epoch[6]: Input data sampled: train_data_A (450, 24, 128) train_data_B (450, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_06_007000.npy_loss: 0.26466537, g_loss: 9.195197114\n",
      "(1, 24, 128) 1 ./samples/fake_B_06_007000.npy\n",
      "Epoch[7]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_07_008000.npy_loss: 0.29020292, g_loss: 8.393007287\n",
      "(1, 24, 128) 1 ./samples/fake_B_07_008000.npy\n",
      "Epoch[8]: Input data sampled: train_data_A (444, 24, 128) train_data_B (444, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_08_009000.npy_loss: 0.52053595, g_loss: 8.195384987\n",
      "(1, 24, 128) 1 ./samples/fake_B_08_009000.npy\n",
      "Epoch[9]: Input data sampled: train_data_A (450, 24, 128) train_data_B (450, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_09_010000.npy_loss: 0.17641482, g_loss: 7.262052543\n",
      "(1, 24, 128) 1 ./samples/fake_B_09_010000.npy\n",
      "Epoch[10]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_10_011000.npy_loss: 0.33799636, g_loss: 8.595933917\n",
      "(1, 24, 128) 1 ./samples/fake_B_10_011000.npy\n",
      "Epoch[11]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_11_012000.npy_loss: 0.34715194, g_loss: 8.184751516\n",
      "(1, 24, 128) 1 ./samples/fake_B_11_012000.npy\n",
      "Epoch[12]: Input data sampled: train_data_A (436, 24, 128) train_data_B (436, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_12_013000.npy_loss: 0.36889875, g_loss: 7.354954726\n",
      "(1, 24, 128) 1 ./samples/fake_B_12_013000.npy\n",
      "Epoch[13]: Input data sampled: train_data_A (455, 24, 128) train_data_B (455, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_13_014000.npy_loss: 0.33307141, g_loss: 7.063513764\n",
      "(1, 24, 128) 1 ./samples/fake_B_13_014000.npy\n",
      "Epoch[14]: Input data sampled: train_data_A (443, 24, 128) train_data_B (443, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_14_015000.npy_loss: 0.46055520, g_loss: 7.218709478\n",
      "(1, 24, 128) 1 ./samples/fake_B_14_015000.npy\n",
      "Epoch[15]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_15_016000.npy_loss: 0.24125987, g_loss: 7.03567505\n",
      "(1, 24, 128) 1 ./samples/fake_B_15_016000.npy\n",
      "Epoch[16]: Input data sampled: train_data_A (435, 24, 128) train_data_B (435, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_16_017000.npy_loss: 0.39596727, g_loss: 7.00228786\n",
      "(1, 24, 128) 1 ./samples/fake_B_16_017000.npy\n",
      "Epoch[17]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_17_018000.npy_loss: 0.33323151, g_loss: 7.07311296\n",
      "(1, 24, 128) 1 ./samples/fake_B_17_018000.npy\n",
      "Epoch[18]: Input data sampled: train_data_A (458, 24, 128) train_data_B (458, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_18_019000.npy_loss: 0.50019372, g_loss: 7.52496243\n",
      "(1, 24, 128) 1 ./samples/fake_B_18_019000.npy\n",
      "Epoch[19]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_19_020000.npy_loss: 0.25731593, g_loss: 6.51502991\n",
      "(1, 24, 128) 1 ./samples/fake_B_19_020000.npy\n",
      "Epoch[20]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_20_021000.npy_loss: 0.41987252, g_loss: 6.38912296\n",
      "(1, 24, 128) 1 ./samples/fake_B_20_021000.npy\n",
      "Epoch[21]: Input data sampled: train_data_A (456, 24, 128) train_data_B (456, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_21_022000.npy_loss: 0.47659498, g_loss: 6.57595348\n",
      "(1, 24, 128) 1 ./samples/fake_B_21_022000.npy\n",
      "Epoch[22]: Input data sampled: train_data_A (441, 24, 128) train_data_B (441, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_22_023000.npy_loss: 0.21958098, g_loss: 6.32401371\n",
      "(1, 24, 128) 1 ./samples/fake_B_22_023000.npy\n",
      "Epoch[23]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_23_024000.npy_loss: 0.31893691, g_loss: 6.48794222\n",
      "(1, 24, 128) 1 ./samples/fake_B_23_024000.npy\n",
      "Epoch[24]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_24_025000.npy_loss: 0.38934147, g_loss: 6.30318975\n",
      "(1, 24, 128) 1 ./samples/fake_B_24_025000.npy\n",
      "Epoch[25]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_25_026000.npy_loss: 0.26211199, g_loss: 7.70998764\n",
      "(1, 24, 128) 1 ./samples/fake_B_25_026000.npy\n",
      "Epoch[26]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_26_027000.npy_loss: 0.35357738, g_loss: 6.34986115\n",
      "(1, 24, 128) 1 ./samples/fake_B_26_027000.npy\n",
      "Epoch[27]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_27_028000.npy_loss: 0.24544378, g_loss: 5.62063074\n",
      "(1, 24, 128) 1 ./samples/fake_B_27_028000.npy\n",
      "Epoch[28]: Input data sampled: train_data_A (455, 24, 128) train_data_B (455, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_28_029000.npy_loss: 0.24482056, g_loss: 6.43149567\n",
      "(1, 24, 128) 1 ./samples/fake_B_28_029000.npy\n",
      "Epoch[29]: Input data sampled: train_data_A (441, 24, 128) train_data_B (441, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_29_030000.npy_loss: 0.34445989, g_loss: 6.50218105\n",
      "(1, 24, 128) 1 ./samples/fake_B_29_030000.npy\n",
      "Epoch[30]: Input data sampled: train_data_A (455, 24, 128) train_data_B (455, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_30_031000.npy_loss: 0.28526869, g_loss: 6.48363256\n",
      "(1, 24, 128) 1 ./samples/fake_B_30_031000.npy\n",
      "Epoch[31]: Input data sampled: train_data_A (450, 24, 128) train_data_B (450, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_31_032000.npy_loss: 0.41005519, g_loss: 5.86982536\n",
      "(1, 24, 128) 1 ./samples/fake_B_31_032000.npy\n",
      "Epoch[32]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_32_033000.npy_loss: 0.26172602, g_loss: 5.97597456\n",
      "(1, 24, 128) 1 ./samples/fake_B_32_033000.npy\n",
      "Epoch[33]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_33_034000.npy_loss: 0.18325254, g_loss: 6.47932386\n",
      "(1, 24, 128) 1 ./samples/fake_B_33_034000.npy\n",
      "Epoch[34]: Input data sampled: train_data_A (457, 24, 128) train_data_B (457, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_34_035000.npy_loss: 0.34477168, g_loss: 6.23860502\n",
      "(1, 24, 128) 1 ./samples/fake_B_34_035000.npy\n",
      "Epoch[35]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_35_036000.npy_loss: 0.15319517, g_loss: 5.88369322\n",
      "(1, 24, 128) 1 ./samples/fake_B_35_036000.npy\n",
      "Epoch[36]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_36_037000.npy_loss: 0.23345584, g_loss: 6.07638454\n",
      "(1, 24, 128) 1 ./samples/fake_B_36_037000.npy\n",
      "Epoch[37]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 24, 128) 1 ./samples/real_A_37_038000.npy_loss: 0.33850524, g_loss: 6.09710884\n",
      "(1, 24, 128) 1 ./samples/fake_B_37_038000.npy\n",
      "Epoch[38]: Input data sampled: train_data_A (459, 24, 128) train_data_B (459, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_38_039000.npy_loss: 0.49752584, g_loss: 5.58679485\n",
      "(1, 24, 128) 1 ./samples/fake_B_38_039000.npy\n",
      "Epoch[39]: Input data sampled: train_data_A (450, 24, 128) train_data_B (450, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_39_040000.npy_loss: 0.47011578, g_loss: 5.26527405\n",
      "(1, 24, 128) 1 ./samples/fake_B_39_040000.npy\n",
      "Epoch[40]: Input data sampled: train_data_A (444, 24, 128) train_data_B (444, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_40_041000.npy_loss: 0.33079532, g_loss: 6.10876083\n",
      "(1, 24, 128) 1 ./samples/fake_B_40_041000.npy\n",
      "Epoch[41]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_41_042000.npy_loss: 0.21888784, g_loss: 5.80195141\n",
      "(1, 24, 128) 1 ./samples/fake_B_41_042000.npy\n",
      "Epoch[42]: Input data sampled: train_data_A (456, 24, 128) train_data_B (456, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_42_043000.npy_loss: 0.18330738, g_loss: 5.85967064\n",
      "(1, 24, 128) 1 ./samples/fake_B_42_043000.npy\n",
      "Epoch[43]: Input data sampled: train_data_A (459, 24, 128) train_data_B (459, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_43_044000.npy_loss: 0.26777756, g_loss: 5.62445927\n",
      "(1, 24, 128) 1 ./samples/fake_B_43_044000.npy\n",
      "Epoch[44]: Input data sampled: train_data_A (441, 24, 128) train_data_B (441, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_44_045000.npy_loss: 0.25430581, g_loss: 5.83687496\n",
      "(1, 24, 128) 1 ./samples/fake_B_44_045000.npy\n",
      "Epoch[45]: Input data sampled: train_data_A (459, 24, 128) train_data_B (459, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_45_046000.npy_loss: 0.27158546, g_loss: 4.94980526\n",
      "(1, 24, 128) 1 ./samples/fake_B_45_046000.npy\n",
      "Epoch[46]: Input data sampled: train_data_A (441, 24, 128) train_data_B (441, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_46_047000.npy_loss: 0.18098328, g_loss: 6.22183037\n",
      "(1, 24, 128) 1 ./samples/fake_B_46_047000.npy\n",
      "Epoch[47]: Input data sampled: train_data_A (459, 24, 128) train_data_B (459, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_47_048000.npy_loss: 0.21456033, g_loss: 5.85231304\n",
      "(1, 24, 128) 1 ./samples/fake_B_47_048000.npy\n",
      "Epoch[48]: Input data sampled: train_data_A (440, 24, 128) train_data_B (440, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_48_049000.npy_loss: 0.14039811, g_loss: 5.36169672\n",
      "(1, 24, 128) 1 ./samples/fake_B_48_049000.npy\n",
      "Epoch[49]: Input data sampled: train_data_A (457, 24, 128) train_data_B (457, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_49_050000.npyd_loss: 0.23285902, g_loss: 5.15866089\n",
      "(1, 24, 128) 1 ./samples/fake_B_49_050000.npy\n",
      "Epoch[50]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_50_051000.npyd_loss: 0.31089321, g_loss: 5.06583786\n",
      "(1, 24, 128) 1 ./samples/fake_B_50_051000.npy\n",
      "Epoch[51]: Input data sampled: train_data_A (440, 24, 128) train_data_B (440, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_51_052000.npyd_loss: 0.15246014, g_loss: 5.85186291\n",
      "(1, 24, 128) 1 ./samples/fake_B_51_052000.npy\n",
      "Epoch[52]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_52_053000.npyd_loss: 0.24245894, g_loss: 5.30296516\n",
      "(1, 24, 128) 1 ./samples/fake_B_52_053000.npy\n",
      "Epoch[53]: Input data sampled: train_data_A (447, 24, 128) train_data_B (447, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_53_054000.npyd_loss: 0.24717277, g_loss: 5.30745363\n",
      "(1, 24, 128) 1 ./samples/fake_B_53_054000.npy\n",
      "Epoch[54]: Input data sampled: train_data_A (455, 24, 128) train_data_B (455, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_54_055000.npyd_loss: 0.26756084, g_loss: 5.18656254\n",
      "(1, 24, 128) 1 ./samples/fake_B_54_055000.npy\n",
      "Epoch[55]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_55_056000.npyd_loss: 0.23684093, g_loss: 5.61096764\n",
      "(1, 24, 128) 1 ./samples/fake_B_55_056000.npy\n",
      "Epoch[56]: Input data sampled: train_data_A (444, 24, 128) train_data_B (444, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_56_057000.npyd_loss: 0.22143914, g_loss: 5.08955193\n",
      "(1, 24, 128) 1 ./samples/fake_B_56_057000.npy\n",
      "Epoch[57]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_57_058000.npyd_loss: 0.11697114, g_loss: 5.01594734\n",
      "(1, 24, 128) 1 ./samples/fake_B_57_058000.npy\n",
      "Epoch[58]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_58_059000.npyd_loss: 0.17888565, g_loss: 5.47528028\n",
      "(1, 24, 128) 1 ./samples/fake_B_58_059000.npy\n",
      "Epoch[59]: Input data sampled: train_data_A (455, 24, 128) train_data_B (455, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_59_060000.npyd_loss: 0.17552008, g_loss: 5.14155960\n",
      "(1, 24, 128) 1 ./samples/fake_B_59_060000.npy\n",
      "Epoch[60]: Input data sampled: train_data_A (444, 24, 128) train_data_B (444, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_60_061000.npyd_loss: 0.23486429, g_loss: 5.28855896\n",
      "(1, 24, 128) 1 ./samples/fake_B_60_061000.npy\n",
      "Epoch[61]: Input data sampled: train_data_A (441, 24, 128) train_data_B (441, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_61_062000.npyd_loss: 0.11257765, g_loss: 5.28900337\n",
      "(1, 24, 128) 1 ./samples/fake_B_61_062000.npy\n",
      "Epoch[62]: Input data sampled: train_data_A (439, 24, 128) train_data_B (439, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_62_063000.npyd_loss: 0.18500322, g_loss: 4.97161674\n",
      "(1, 24, 128) 1 ./samples/fake_B_62_063000.npy\n",
      "Epoch[63]: Input data sampled: train_data_A (442, 24, 128) train_data_B (442, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_63_064000.npyd_loss: 0.07375411, g_loss: 5.33787203\n",
      "(1, 24, 128) 1 ./samples/fake_B_63_064000.npy\n",
      "Epoch[64]: Input data sampled: train_data_A (447, 24, 128) train_data_B (447, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_64_065000.npyd_loss: 0.15520762, g_loss: 5.31929016\n",
      "(1, 24, 128) 1 ./samples/fake_B_64_065000.npy\n",
      "Epoch[65]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_65_066000.npyd_loss: 0.20311898, g_loss: 4.91201591\n",
      "(1, 24, 128) 1 ./samples/fake_B_65_066000.npy\n",
      "Epoch[66]: Input data sampled: train_data_A (456, 24, 128) train_data_B (456, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_66_067000.npyd_loss: 0.07364927, g_loss: 5.09534550\n",
      "(1, 24, 128) 1 ./samples/fake_B_66_067000.npy\n",
      "Epoch[67]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_67_068000.npyd_loss: 0.12375510, g_loss: 5.20252609\n",
      "(1, 24, 128) 1 ./samples/fake_B_67_068000.npy\n",
      "Epoch[68]: Input data sampled: train_data_A (447, 24, 128) train_data_B (447, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_68_069000.npyd_loss: 0.12830694, g_loss: 5.13082314\n",
      "(1, 24, 128) 1 ./samples/fake_B_68_069000.npy\n",
      "Epoch[69]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_69_070000.npyd_loss: 0.11760678, g_loss: 5.27513504\n",
      "(1, 24, 128) 1 ./samples/fake_B_69_070000.npy\n",
      "Epoch[70]: Input data sampled: train_data_A (445, 24, 128) train_data_B (445, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_70_071000.npyd_loss: 0.19875422, g_loss: 5.57875252\n",
      "(1, 24, 128) 1 ./samples/fake_B_70_071000.npy\n",
      "Epoch[71]: Input data sampled: train_data_A (434, 24, 128) train_data_B (434, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_71_072000.npyd_loss: 0.08098064, g_loss: 5.10274601\n",
      "(1, 24, 128) 1 ./samples/fake_B_71_072000.npy\n",
      "Epoch[72]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_72_073000.npyd_loss: 0.16649100, g_loss: 4.87491798\n",
      "(1, 24, 128) 1 ./samples/fake_B_72_073000.npy\n",
      "Epoch[73]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_73_074000.npyd_loss: 0.10232732, g_loss: 4.56276703\n",
      "(1, 24, 128) 1 ./samples/fake_B_73_074000.npy\n",
      "Epoch[74]: Input data sampled: train_data_A (439, 24, 128) train_data_B (439, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_74_075000.npyd_loss: 0.03400239, g_loss: 4.90847301\n",
      "(1, 24, 128) 1 ./samples/fake_B_74_075000.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[75]: Input data sampled: train_data_A (440, 24, 128) train_data_B (440, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_75_076000.npyd_loss: 0.07738177, g_loss: 4.68108273\n",
      "(1, 24, 128) 1 ./samples/fake_B_75_076000.npy\n",
      "Epoch[76]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_76_077000.npyd_loss: 0.15083960, g_loss: 5.03250027\n",
      "(1, 24, 128) 1 ./samples/fake_B_76_077000.npy\n",
      "Epoch[77]: Input data sampled: train_data_A (457, 24, 128) train_data_B (457, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_77_078000.npyd_loss: 0.17208706, g_loss: 4.85974026\n",
      "(1, 24, 128) 1 ./samples/fake_B_77_078000.npy\n",
      "Epoch[78]: Input data sampled: train_data_A (444, 24, 128) train_data_B (444, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_78_079000.npyd_loss: 0.09208462, g_loss: 6.15769100\n",
      "(1, 24, 128) 1 ./samples/fake_B_78_079000.npy\n",
      "Epoch[79]: Input data sampled: train_data_A (435, 24, 128) train_data_B (435, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_79_080000.npyd_loss: 0.08360395, g_loss: 5.13009977\n",
      "(1, 24, 128) 1 ./samples/fake_B_79_080000.npy\n",
      "Epoch[80]: Input data sampled: train_data_A (456, 24, 128) train_data_B (456, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_80_081000.npyd_loss: 0.14586957, g_loss: 5.09884405\n",
      "(1, 24, 128) 1 ./samples/fake_B_80_081000.npy\n",
      "Epoch[81]: Input data sampled: train_data_A (455, 24, 128) train_data_B (455, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_81_082000.npyd_loss: 0.09068648, g_loss: 4.59546614\n",
      "(1, 24, 128) 1 ./samples/fake_B_81_082000.npy\n",
      "Epoch[82]: Input data sampled: train_data_A (438, 24, 128) train_data_B (438, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_82_083000.npyd_loss: 0.12609187, g_loss: 4.66576385\n",
      "(1, 24, 128) 1 ./samples/fake_B_82_083000.npy\n",
      "Epoch[83]: Input data sampled: train_data_A (444, 24, 128) train_data_B (444, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_83_084000.npyd_loss: 0.08224855, g_loss: 4.95773411\n",
      "(1, 24, 128) 1 ./samples/fake_B_83_084000.npy\n",
      "Epoch[84]: Input data sampled: train_data_A (450, 24, 128) train_data_B (450, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_84_085000.npyd_loss: 0.06522354, g_loss: 5.47963333\n",
      "(1, 24, 128) 1 ./samples/fake_B_84_085000.npy\n",
      "Epoch[85]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_85_086000.npyd_loss: 0.11872630, g_loss: 4.96764183\n",
      "(1, 24, 128) 1 ./samples/fake_B_85_086000.npy\n",
      "Epoch[86]: Input data sampled: train_data_A (445, 24, 128) train_data_B (445, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_86_087000.npyd_loss: 0.05156264, g_loss: 4.81084347\n",
      "(1, 24, 128) 1 ./samples/fake_B_86_087000.npy\n",
      "Epoch[87]: Input data sampled: train_data_A (443, 24, 128) train_data_B (443, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_87_088000.npyd_loss: 0.09347471, g_loss: 4.77578354\n",
      "(1, 24, 128) 1 ./samples/fake_B_87_088000.npy\n",
      "Epoch[88]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_88_089000.npyd_loss: 0.09144093, g_loss: 5.14732552\n",
      "(1, 24, 128) 1 ./samples/fake_B_88_089000.npy\n",
      "Epoch[89]: Input data sampled: train_data_A (437, 24, 128) train_data_B (437, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_89_090000.npyd_loss: 0.04673402, g_loss: 5.08092880\n",
      "(1, 24, 128) 1 ./samples/fake_B_89_090000.npy\n",
      "Epoch[90]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_90_091000.npyd_loss: 0.10220054, g_loss: 4.70652103\n",
      "(1, 24, 128) 1 ./samples/fake_B_90_091000.npy\n",
      "Epoch[91]: Input data sampled: train_data_A (444, 24, 128) train_data_B (444, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_91_092000.npyd_loss: 0.11829045, g_loss: 4.59184790\n",
      "(1, 24, 128) 1 ./samples/fake_B_91_092000.npy\n",
      "Epoch[92]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_92_093000.npyd_loss: 0.04396879, g_loss: 5.18453550\n",
      "(1, 24, 128) 1 ./samples/fake_B_92_093000.npy\n",
      "Epoch[93]: Input data sampled: train_data_A (455, 24, 128) train_data_B (455, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_93_094000.npyd_loss: 0.05662965, g_loss: 4.90993690\n",
      "(1, 24, 128) 1 ./samples/fake_B_93_094000.npy\n",
      "Epoch[94]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_94_095000.npyd_loss: 0.05097356, g_loss: 4.63560390\n",
      "(1, 24, 128) 1 ./samples/fake_B_94_095000.npy\n",
      "Epoch[95]: Input data sampled: train_data_A (445, 24, 128) train_data_B (445, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_95_096000.npyd_loss: 0.09924045, g_loss: 4.74938345\n",
      "(1, 24, 128) 1 ./samples/fake_B_95_096000.npy\n",
      "Epoch[96]: Input data sampled: train_data_A (456, 24, 128) train_data_B (456, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_96_097000.npyd_loss: 0.07383479, g_loss: 4.86867714\n",
      "(1, 24, 128) 1 ./samples/fake_B_96_097000.npy\n",
      "Epoch[97]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_97_098000.npyd_loss: 0.19913056, g_loss: 4.49198246\n",
      "(1, 24, 128) 1 ./samples/fake_B_97_098000.npy\n",
      "Epoch[98]: Input data sampled: train_data_A (438, 24, 128) train_data_B (438, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_98_099000.npyd_loss: 0.05933846, g_loss: 4.48923111\n",
      "(1, 24, 128) 1 ./samples/fake_B_98_099000.npy\n",
      "Epoch[99]: Input data sampled: train_data_A (455, 24, 128) train_data_B (455, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_99_100000.npyd_loss: 0.04645746, g_loss: 5.71265507\n",
      "(1, 24, 128) 1 ./samples/fake_B_99_100000.npy\n",
      "Epoch[100]: Input data sampled: train_data_A (445, 24, 128) train_data_B (445, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_100_101000.npyd_loss: 0.07294273, g_loss: 4.65314770\n",
      "(1, 24, 128) 1 ./samples/fake_B_100_101000.npy\n",
      "Epoch[101]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_101_102000.npyd_loss: 0.02510609, g_loss: 4.84694004\n",
      "(1, 24, 128) 1 ./samples/fake_B_101_102000.npy\n",
      "Epoch[102]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_102_103000.npyd_loss: 0.04099072, g_loss: 4.86665154\n",
      "(1, 24, 128) 1 ./samples/fake_B_102_103000.npy\n",
      "Epoch[103]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_103_104000.npyd_loss: 0.05366482, g_loss: 4.72622776\n",
      "(1, 24, 128) 1 ./samples/fake_B_103_104000.npy\n",
      "Epoch[104]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_104_105000.npyd_loss: 0.09164538, g_loss: 4.57136154\n",
      "(1, 24, 128) 1 ./samples/fake_B_104_105000.npy\n",
      "Epoch[105]: Input data sampled: train_data_A (440, 24, 128) train_data_B (440, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_105_106000.npyd_loss: 0.04056057, g_loss: 4.57959890\n",
      "(1, 24, 128) 1 ./samples/fake_B_105_106000.npy\n",
      "Epoch[106]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_106_107000.npyd_loss: 0.12008903, g_loss: 4.52920437\n",
      "(1, 24, 128) 1 ./samples/fake_B_106_107000.npy\n",
      "Epoch[107]: Input data sampled: train_data_A (442, 24, 128) train_data_B (442, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_107_108000.npyd_loss: 0.02904895, g_loss: 4.70599270\n",
      "(1, 24, 128) 1 ./samples/fake_B_107_108000.npy\n",
      "Epoch[108]: Input data sampled: train_data_A (458, 24, 128) train_data_B (458, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_108_109000.npyd_loss: 0.13317961, g_loss: 4.50810575\n",
      "(1, 24, 128) 1 ./samples/fake_B_108_109000.npy\n",
      "Epoch[109]: Input data sampled: train_data_A (445, 24, 128) train_data_B (445, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_109_110000.npyd_loss: 0.14623523, g_loss: 4.46635151\n",
      "(1, 24, 128) 1 ./samples/fake_B_109_110000.npy\n",
      "Epoch[110]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_110_111000.npyd_loss: 0.07986331, g_loss: 4.59060860\n",
      "(1, 24, 128) 1 ./samples/fake_B_110_111000.npy\n",
      "Epoch[111]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_111_112000.npyd_loss: 0.09526624, g_loss: 4.49172306\n",
      "(1, 24, 128) 1 ./samples/fake_B_111_112000.npy\n",
      "Epoch[112]: Input data sampled: train_data_A (450, 24, 128) train_data_B (450, 24, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 24, 128) 1 ./samples/real_A_112_113000.npyd_loss: 0.03765126, g_loss: 4.66820765\n",
      "(1, 24, 128) 1 ./samples/fake_B_112_113000.npy\n",
      "Epoch[113]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_113_114000.npyd_loss: 0.06271875, g_loss: 4.82805538\n",
      "(1, 24, 128) 1 ./samples/fake_B_113_114000.npy\n",
      "Epoch[114]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_114_115000.npyd_loss: 0.08714643, g_loss: 4.56814194\n",
      "(1, 24, 128) 1 ./samples/fake_B_114_115000.npy\n",
      "Epoch[115]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_115_116000.npyd_loss: 0.03217275, g_loss: 4.45003748\n",
      "(1, 24, 128) 1 ./samples/fake_B_115_116000.npy\n",
      "Epoch[116]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_116_117000.npyd_loss: 0.02582293, g_loss: 4.65190125\n",
      "(1, 24, 128) 1 ./samples/fake_B_116_117000.npy\n",
      "Epoch[117]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_117_118000.npyd_loss: 0.08989021, g_loss: 4.62542391\n",
      "(1, 24, 128) 1 ./samples/fake_B_117_118000.npy\n",
      "Epoch[118]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_118_119000.npyd_loss: 0.01470394, g_loss: 4.39982510\n",
      "(1, 24, 128) 1 ./samples/fake_B_118_119000.npy\n",
      "Epoch[119]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_119_120000.npyd_loss: 0.03842583, g_loss: 4.38853741\n",
      "(1, 24, 128) 1 ./samples/fake_B_119_120000.npy\n",
      "Epoch[120]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_120_121000.npyd_loss: 0.05185865, g_loss: 4.51993084\n",
      "(1, 24, 128) 1 ./samples/fake_B_120_121000.npy\n",
      "Epoch[121]: Input data sampled: train_data_A (458, 24, 128) train_data_B (458, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_121_122000.npyd_loss: 0.07006591, g_loss: 4.52335644\n",
      "(1, 24, 128) 1 ./samples/fake_B_121_122000.npy\n",
      "Epoch[122]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_122_123000.npyd_loss: 0.03355448, g_loss: 4.46018791\n",
      "(1, 24, 128) 1 ./samples/fake_B_122_123000.npy\n",
      "Epoch[123]: Input data sampled: train_data_A (447, 24, 128) train_data_B (447, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_123_124000.npyd_loss: 0.03737346, g_loss: 4.66349697\n",
      "(1, 24, 128) 1 ./samples/fake_B_123_124000.npy\n",
      "Epoch[124]: Input data sampled: train_data_A (465, 24, 128) train_data_B (465, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_124_125000.npyd_loss: 0.15300755, g_loss: 4.61633396\n",
      "(1, 24, 128) 1 ./samples/fake_B_124_125000.npy\n",
      "Epoch[125]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_125_126000.npyd_loss: 0.01750496, g_loss: 4.43077660\n",
      "(1, 24, 128) 1 ./samples/fake_B_125_126000.npy\n",
      "Epoch[126]: Input data sampled: train_data_A (455, 24, 128) train_data_B (455, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_126_127000.npyd_loss: 0.03171699, g_loss: 4.44742870\n",
      "(1, 24, 128) 1 ./samples/fake_B_126_127000.npy\n",
      "Epoch[127]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_127_128000.npyd_loss: 0.05418960, g_loss: 4.56871414\n",
      "(1, 24, 128) 1 ./samples/fake_B_127_128000.npy\n",
      "Epoch[128]: Input data sampled: train_data_A (445, 24, 128) train_data_B (445, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_128_129000.npyd_loss: 0.05337165, g_loss: 4.74593353\n",
      "(1, 24, 128) 1 ./samples/fake_B_128_129000.npy\n",
      "Epoch[129]: Input data sampled: train_data_A (450, 24, 128) train_data_B (450, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_129_130000.npyd_loss: 0.05590065, g_loss: 4.25216866\n",
      "(1, 24, 128) 1 ./samples/fake_B_129_130000.npy\n",
      "Epoch[130]: Input data sampled: train_data_A (447, 24, 128) train_data_B (447, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_130_131000.npyd_loss: 0.06131291, g_loss: 4.23651600\n",
      "(1, 24, 128) 1 ./samples/fake_B_130_131000.npy\n",
      "Epoch[131]: Input data sampled: train_data_A (442, 24, 128) train_data_B (442, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_131_132000.npyd_loss: 0.02599865, g_loss: 4.19926071\n",
      "(1, 24, 128) 1 ./samples/fake_B_131_132000.npy\n",
      "Epoch[132]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_132_133000.npyd_loss: 0.02700936, g_loss: 4.37795353\n",
      "(1, 24, 128) 1 ./samples/fake_B_132_133000.npy\n",
      "Epoch[133]: Input data sampled: train_data_A (457, 24, 128) train_data_B (457, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_133_134000.npyd_loss: 0.03566659, g_loss: 4.55321312\n",
      "(1, 24, 128) 1 ./samples/fake_B_133_134000.npy\n",
      "Epoch[134]: Input data sampled: train_data_A (455, 24, 128) train_data_B (455, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_134_135000.npyd_loss: 0.14113441, g_loss: 4.29938507\n",
      "(1, 24, 128) 1 ./samples/fake_B_134_135000.npy\n",
      "Epoch[135]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_135_136000.npyd_loss: 0.00289879, g_loss: 5.12878656\n",
      "(1, 24, 128) 1 ./samples/fake_B_135_136000.npy\n",
      "Epoch[136]: Input data sampled: train_data_A (441, 24, 128) train_data_B (441, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_136_137000.npyd_loss: 0.06807961, g_loss: 4.54355383\n",
      "(1, 24, 128) 1 ./samples/fake_B_136_137000.npy\n",
      "Epoch[137]: Input data sampled: train_data_A (455, 24, 128) train_data_B (455, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_137_138000.npyd_loss: 0.05609515, g_loss: 4.52886391\n",
      "(1, 24, 128) 1 ./samples/fake_B_137_138000.npy\n",
      "Epoch[138]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_138_139000.npyd_loss: 0.01491434, g_loss: 4.35327005\n",
      "(1, 24, 128) 1 ./samples/fake_B_138_139000.npy\n",
      "Epoch[139]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_139_140000.npyd_loss: 0.08604127, g_loss: 4.48511696\n",
      "(1, 24, 128) 1 ./samples/fake_B_139_140000.npy\n",
      "Epoch[140]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_140_141000.npyd_loss: 0.04742711, g_loss: 4.83582783\n",
      "(1, 24, 128) 1 ./samples/fake_B_140_141000.npy\n",
      "Epoch[141]: Input data sampled: train_data_A (459, 24, 128) train_data_B (459, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_141_142000.npyd_loss: 0.02339739, g_loss: 4.27600765\n",
      "(1, 24, 128) 1 ./samples/fake_B_141_142000.npy\n",
      "Epoch[142]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_142_143000.npyd_loss: 0.00879505, g_loss: 4.29386330\n",
      "(1, 24, 128) 1 ./samples/fake_B_142_143000.npy\n",
      "Epoch[143]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_143_144000.npyd_loss: 0.03815470, g_loss: 4.45415354\n",
      "(1, 24, 128) 1 ./samples/fake_B_143_144000.npy\n",
      "Epoch[144]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_144_145000.npyd_loss: 0.02883230, g_loss: 4.30791950\n",
      "(1, 24, 128) 1 ./samples/fake_B_144_145000.npy\n",
      "Epoch[145]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_145_146000.npyd_loss: 0.02161796, g_loss: 4.60308361\n",
      "(1, 24, 128) 1 ./samples/fake_B_145_146000.npy\n",
      "Epoch[146]: Input data sampled: train_data_A (444, 24, 128) train_data_B (444, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_146_147000.npyd_loss: 0.01321851, g_loss: 4.58038044\n",
      "(1, 24, 128) 1 ./samples/fake_B_146_147000.npy\n",
      "Epoch[147]: Input data sampled: train_data_A (442, 24, 128) train_data_B (442, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_147_148000.npyd_loss: 0.02072829, g_loss: 4.72765207\n",
      "(1, 24, 128) 1 ./samples/fake_B_147_148000.npy\n",
      "Epoch[148]: Input data sampled: train_data_A (450, 24, 128) train_data_B (450, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_148_149000.npyd_loss: 0.00404209, g_loss: 4.30312443\n",
      "(1, 24, 128) 1 ./samples/fake_B_148_149000.npy\n",
      "Epoch[149]: Input data sampled: train_data_A (447, 24, 128) train_data_B (447, 24, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 24, 128) 1 ./samples/real_A_149_150000.npyd_loss: 0.02649475, g_loss: 4.31218815\n",
      "(1, 24, 128) 1 ./samples/fake_B_149_150000.npy\n",
      "Epoch[150]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_150_151000.npyd_loss: 0.00806118, g_loss: 4.64591455\n",
      "(1, 24, 128) 1 ./samples/fake_B_150_151000.npy\n",
      "Epoch[151]: Input data sampled: train_data_A (443, 24, 128) train_data_B (443, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_151_152000.npyd_loss: 0.01337068, g_loss: 4.10827827\n",
      "(1, 24, 128) 1 ./samples/fake_B_151_152000.npy\n",
      "Epoch[152]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_152_153000.npyd_loss: 0.03258700, g_loss: 4.19422054\n",
      "(1, 24, 128) 1 ./samples/fake_B_152_153000.npy\n",
      "Epoch[153]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_153_154000.npyd_loss: 0.04768686, g_loss: 4.29781532\n",
      "(1, 24, 128) 1 ./samples/fake_B_153_154000.npy\n",
      "Epoch[154]: Input data sampled: train_data_A (437, 24, 128) train_data_B (437, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_154_155000.npyd_loss: 0.02651489, g_loss: 4.16795015\n",
      "(1, 24, 128) 1 ./samples/fake_B_154_155000.npy\n",
      "Epoch[155]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_155_156000.npyd_loss: 0.07385840, g_loss: 4.13004398\n",
      "(1, 24, 128) 1 ./samples/fake_B_155_156000.npy\n",
      "Epoch[156]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_156_157000.npyd_loss: 0.01996839, g_loss: 4.06184769\n",
      "(1, 24, 128) 1 ./samples/fake_B_156_157000.npy\n",
      "Epoch[157]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_157_158000.npyd_loss: 0.01841101, g_loss: 4.34165478\n",
      "(1, 24, 128) 1 ./samples/fake_B_157_158000.npy\n",
      "Epoch[158]: Input data sampled: train_data_A (458, 24, 128) train_data_B (458, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_158_159000.npyd_loss: 0.00733162, g_loss: 4.17204857\n",
      "(1, 24, 128) 1 ./samples/fake_B_158_159000.npy\n",
      "Epoch[159]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_159_160000.npyd_loss: 0.01576455, g_loss: 4.34743261\n",
      "(1, 24, 128) 1 ./samples/fake_B_159_160000.npy\n",
      "Epoch[160]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_160_161000.npyd_loss: 0.00750204, g_loss: 4.34863377\n",
      "(1, 24, 128) 1 ./samples/fake_B_160_161000.npy\n",
      "Epoch[161]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_161_162000.npyd_loss: 0.02557442, g_loss: 4.23803329\n",
      "(1, 24, 128) 1 ./samples/fake_B_161_162000.npy\n",
      "Epoch[162]: Input data sampled: train_data_A (441, 24, 128) train_data_B (441, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_162_163000.npyd_loss: 0.00941475, g_loss: 4.21201372\n",
      "(1, 24, 128) 1 ./samples/fake_B_162_163000.npy\n",
      "Epoch[163]: Input data sampled: train_data_A (444, 24, 128) train_data_B (444, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_163_164000.npyd_loss: 0.09788421, g_loss: 4.45483589\n",
      "(1, 24, 128) 1 ./samples/fake_B_163_164000.npy\n",
      "Epoch[164]: Input data sampled: train_data_A (459, 24, 128) train_data_B (459, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_164_165000.npyd_loss: 0.02140296, g_loss: 4.42883873\n",
      "(1, 24, 128) 1 ./samples/fake_B_164_165000.npy\n",
      "Epoch[165]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_165_166000.npyd_loss: 0.00949480, g_loss: 4.11884928\n",
      "(1, 24, 128) 1 ./samples/fake_B_165_166000.npy\n",
      "Epoch[166]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_166_167000.npyd_loss: 0.09821816, g_loss: 4.16845512\n",
      "(1, 24, 128) 1 ./samples/fake_B_166_167000.npy\n",
      "Epoch[167]: Input data sampled: train_data_A (450, 24, 128) train_data_B (450, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_167_168000.npyd_loss: 0.01893699, g_loss: 4.61924934\n",
      "(1, 24, 128) 1 ./samples/fake_B_167_168000.npy\n",
      "Epoch[168]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_168_169000.npyd_loss: 0.03958850, g_loss: 4.08003092\n",
      "(1, 24, 128) 1 ./samples/fake_B_168_169000.npy\n",
      "Epoch[169]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_169_170000.npyd_loss: 0.00636554, g_loss: 4.10033464\n",
      "(1, 24, 128) 1 ./samples/fake_B_169_170000.npy\n",
      "Epoch[170]: Input data sampled: train_data_A (440, 24, 128) train_data_B (440, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_170_171000.npyd_loss: 0.00739430, g_loss: 4.15014172\n",
      "(1, 24, 128) 1 ./samples/fake_B_170_171000.npy\n",
      "Epoch[171]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_171_172000.npyd_loss: 0.03710870, g_loss: 4.24870968\n",
      "(1, 24, 128) 1 ./samples/fake_B_171_172000.npy\n",
      "Epoch[172]: Input data sampled: train_data_A (455, 24, 128) train_data_B (455, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_172_173000.npyd_loss: 0.00597853, g_loss: 4.29325342\n",
      "(1, 24, 128) 1 ./samples/fake_B_172_173000.npy\n",
      "Epoch[173]: Input data sampled: train_data_A (447, 24, 128) train_data_B (447, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_173_174000.npyd_loss: 0.00228309, g_loss: 4.34687138\n",
      "(1, 24, 128) 1 ./samples/fake_B_173_174000.npy\n",
      "Epoch[174]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_174_175000.npyd_loss: 0.02860324, g_loss: 4.31793642\n",
      "(1, 24, 128) 1 ./samples/fake_B_174_175000.npy\n",
      "Epoch[175]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_175_176000.npyd_loss: 0.01111262, g_loss: 4.11364603\n",
      "(1, 24, 128) 1 ./samples/fake_B_175_176000.npy\n",
      "Epoch[176]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_176_177000.npyd_loss: 0.00344615, g_loss: 4.46686220\n",
      "(1, 24, 128) 1 ./samples/fake_B_176_177000.npy\n",
      "Epoch[177]: Input data sampled: train_data_A (457, 24, 128) train_data_B (457, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_177_178000.npyd_loss: 0.01622860, g_loss: 4.10619974\n",
      "(1, 24, 128) 1 ./samples/fake_B_177_178000.npy\n",
      "Epoch[178]: Input data sampled: train_data_A (458, 24, 128) train_data_B (458, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_178_179000.npyd_loss: 0.00978787, g_loss: 4.30585194\n",
      "(1, 24, 128) 1 ./samples/fake_B_178_179000.npy\n",
      "Epoch[179]: Input data sampled: train_data_A (443, 24, 128) train_data_B (443, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_179_180000.npyd_loss: 0.02911810, g_loss: 4.40230274\n",
      "(1, 24, 128) 1 ./samples/fake_B_179_180000.npy\n",
      "Epoch[180]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_180_181000.npyd_loss: 0.03477659, g_loss: 4.14439344\n",
      "(1, 24, 128) 1 ./samples/fake_B_180_181000.npy\n",
      "Epoch[181]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_181_182000.npyd_loss: 0.01809207, g_loss: 4.21347904\n",
      "(1, 24, 128) 1 ./samples/fake_B_181_182000.npy\n",
      "Epoch[182]: Input data sampled: train_data_A (458, 24, 128) train_data_B (458, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_182_183000.npyd_loss: 0.00994584, g_loss: 4.15310049\n",
      "(1, 24, 128) 1 ./samples/fake_B_182_183000.npy\n",
      "Epoch[183]: Input data sampled: train_data_A (447, 24, 128) train_data_B (447, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_183_184000.npyd_loss: 0.01002986, g_loss: 4.25610447\n",
      "(1, 24, 128) 1 ./samples/fake_B_183_184000.npy\n",
      "Epoch[184]: Input data sampled: train_data_A (444, 24, 128) train_data_B (444, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_184_185000.npyd_loss: 0.01227709, g_loss: 4.10494518\n",
      "(1, 24, 128) 1 ./samples/fake_B_184_185000.npy\n",
      "Epoch[185]: Input data sampled: train_data_A (457, 24, 128) train_data_B (457, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_185_186000.npyd_loss: 0.00768453, g_loss: 4.20605564\n",
      "(1, 24, 128) 1 ./samples/fake_B_185_186000.npy\n",
      "Epoch[186]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 24, 128) 1 ./samples/real_A_186_187000.npyd_loss: 0.00251315, g_loss: 4.17813587\n",
      "(1, 24, 128) 1 ./samples/fake_B_186_187000.npy\n",
      "Epoch[187]: Input data sampled: train_data_A (438, 24, 128) train_data_B (438, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_187_188000.npyd_loss: 0.01521396, g_loss: 4.36996508\n",
      "(1, 24, 128) 1 ./samples/fake_B_187_188000.npy\n",
      "Epoch[188]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_188_189000.npyd_loss: 0.00704692, g_loss: 4.23286057\n",
      "(1, 24, 128) 1 ./samples/fake_B_188_189000.npy\n",
      "Epoch[189]: Input data sampled: train_data_A (447, 24, 128) train_data_B (447, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_189_190000.npyd_loss: 0.00478990, g_loss: 3.96832466\n",
      "(1, 24, 128) 1 ./samples/fake_B_189_190000.npy\n",
      "Epoch[190]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_190_191000.npyd_loss: 0.02743014, g_loss: 4.02299547\n",
      "(1, 24, 128) 1 ./samples/fake_B_190_191000.npy\n",
      "Epoch[191]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_191_192000.npyd_loss: 0.01403493, g_loss: 4.17743874\n",
      "(1, 24, 128) 1 ./samples/fake_B_191_192000.npy\n",
      "Epoch[192]: Input data sampled: train_data_A (447, 24, 128) train_data_B (447, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_192_193000.npyd_loss: 0.02218175, g_loss: 4.31095791\n",
      "(1, 24, 128) 1 ./samples/fake_B_192_193000.npy\n",
      "Epoch[193]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_193_194000.npyd_loss: 0.08518205, g_loss: 4.08727455\n",
      "(1, 24, 128) 1 ./samples/fake_B_193_194000.npy\n",
      "Epoch[194]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_194_195000.npyd_loss: 0.00422018, g_loss: 4.02418900\n",
      "(1, 24, 128) 1 ./samples/fake_B_194_195000.npy\n",
      "Epoch[195]: Input data sampled: train_data_A (460, 24, 128) train_data_B (460, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_195_196000.npyd_loss: 0.00919036, g_loss: 4.00438595\n",
      "(1, 24, 128) 1 ./samples/fake_B_195_196000.npy\n",
      "Epoch[196]: Input data sampled: train_data_A (450, 24, 128) train_data_B (450, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_196_197000.npyd_loss: 0.01464030, g_loss: 4.03368425\n",
      "(1, 24, 128) 1 ./samples/fake_B_196_197000.npy\n",
      "Epoch[197]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_197_198000.npyd_loss: 0.02851860, g_loss: 4.12920570\n",
      "(1, 24, 128) 1 ./samples/fake_B_197_198000.npy\n",
      "Epoch[198]: Input data sampled: train_data_A (456, 24, 128) train_data_B (456, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_198_199000.npyd_loss: 0.01042053, g_loss: 4.08621073\n",
      "(1, 24, 128) 1 ./samples/fake_B_198_199000.npy\n",
      "Epoch[199]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_199_200000.npyd_loss: 0.00116272, g_loss: 3.98707438\n",
      "(1, 24, 128) 1 ./samples/fake_B_199_200000.npy\n",
      "Epoch[200]: Input data sampled: train_data_A (455, 24, 128) train_data_B (455, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_200_201000.npyd_loss: 0.01739866, g_loss: 4.28622818\n",
      "(1, 24, 128) 1 ./samples/fake_B_200_201000.npy\n",
      "Epoch[201]: Input data sampled: train_data_A (456, 24, 128) train_data_B (456, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_201_202000.npyd_loss: 0.02333002, g_loss: 3.96932173\n",
      "(1, 24, 128) 1 ./samples/fake_B_201_202000.npy\n",
      "Epoch[202]: Input data sampled: train_data_A (445, 24, 128) train_data_B (445, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_202_203000.npyd_loss: 0.02437669, g_loss: 4.09423304\n",
      "(1, 24, 128) 1 ./samples/fake_B_202_203000.npy\n",
      "Epoch[203]: Input data sampled: train_data_A (447, 24, 128) train_data_B (447, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_203_204000.npyd_loss: 0.00713692, g_loss: 4.12474585\n",
      "(1, 24, 128) 1 ./samples/fake_B_203_204000.npy\n",
      "Epoch[204]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_204_205000.npyd_loss: 0.22440135, g_loss: 4.09983873\n",
      "(1, 24, 128) 1 ./samples/fake_B_204_205000.npy\n",
      "Epoch[205]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_205_206000.npyd_loss: 0.00173606, g_loss: 4.05483532\n",
      "(1, 24, 128) 1 ./samples/fake_B_205_206000.npy\n",
      "Epoch[206]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_206_207000.npyd_loss: 0.00157599, g_loss: 4.09510612\n",
      "(1, 24, 128) 1 ./samples/fake_B_206_207000.npy\n",
      "Epoch[207]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_207_208000.npyd_loss: 0.00389327, g_loss: 3.89717770\n",
      "(1, 24, 128) 1 ./samples/fake_B_207_208000.npy\n",
      "Epoch[208]: Input data sampled: train_data_A (443, 24, 128) train_data_B (443, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_208_209000.npyd_loss: 0.01059001, g_loss: 4.24691248\n",
      "(1, 24, 128) 1 ./samples/fake_B_208_209000.npy\n",
      "Epoch[209]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_209_210000.npyd_loss: 0.01016781, g_loss: 3.95783854\n",
      "(1, 24, 128) 1 ./samples/fake_B_209_210000.npy\n",
      "Epoch[210]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_210_211000.npyd_loss: 0.00968955, g_loss: 3.96259379\n",
      "(1, 24, 128) 1 ./samples/fake_B_210_211000.npy\n",
      "Epoch[211]: Input data sampled: train_data_A (442, 24, 128) train_data_B (442, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_211_212000.npyd_loss: 0.00564356, g_loss: 4.16231203\n",
      "(1, 24, 128) 1 ./samples/fake_B_211_212000.npy\n",
      "Epoch[212]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_212_213000.npyd_loss: 0.00391202, g_loss: 4.04211569\n",
      "(1, 24, 128) 1 ./samples/fake_B_212_213000.npy\n",
      "Epoch[213]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_213_214000.npyd_loss: 0.00532226, g_loss: 3.90939927\n",
      "(1, 24, 128) 1 ./samples/fake_B_213_214000.npy\n",
      "Epoch[214]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_214_215000.npyd_loss: 0.01580030, g_loss: 3.97065163\n",
      "(1, 24, 128) 1 ./samples/fake_B_214_215000.npy\n",
      "Epoch[215]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_215_216000.npyd_loss: 0.01261343, g_loss: 3.86398554\n",
      "(1, 24, 128) 1 ./samples/fake_B_215_216000.npy\n",
      "Epoch[216]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_216_217000.npyd_loss: 0.00595802, g_loss: 3.96397066\n",
      "(1, 24, 128) 1 ./samples/fake_B_216_217000.npy\n",
      "Epoch[217]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_217_218000.npyd_loss: 0.01726489, g_loss: 4.24211645\n",
      "(1, 24, 128) 1 ./samples/fake_B_217_218000.npy\n",
      "Epoch[218]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_218_219000.npyd_loss: 0.00409373, g_loss: 4.12508774\n",
      "(1, 24, 128) 1 ./samples/fake_B_218_219000.npy\n",
      "Epoch[219]: Input data sampled: train_data_A (461, 24, 128) train_data_B (461, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_219_220000.npyd_loss: 0.03324068, g_loss: 4.05095768\n",
      "(1, 24, 128) 1 ./samples/fake_B_219_220000.npy\n",
      "Epoch[220]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_220_221000.npyd_loss: 0.00837714, g_loss: 3.92111015\n",
      "(1, 24, 128) 1 ./samples/fake_B_220_221000.npy\n",
      "Epoch[221]: Input data sampled: train_data_A (441, 24, 128) train_data_B (441, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_221_222000.npyd_loss: 0.00085242, g_loss: 4.15344858\n",
      "(1, 24, 128) 1 ./samples/fake_B_221_222000.npy\n",
      "Epoch[222]: Input data sampled: train_data_A (450, 24, 128) train_data_B (450, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_222_223000.npyd_loss: 0.01172583, g_loss: 3.89131880\n",
      "(1, 24, 128) 1 ./samples/fake_B_222_223000.npy\n",
      "Epoch[223]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 24, 128) 1 ./samples/real_A_223_224000.npyd_loss: 0.01797605, g_loss: 3.96469688\n",
      "(1, 24, 128) 1 ./samples/fake_B_223_224000.npy\n",
      "Epoch[224]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_224_225000.npyd_loss: 0.00830517, g_loss: 4.13102245\n",
      "(1, 24, 128) 1 ./samples/fake_B_224_225000.npy\n",
      "Epoch[225]: Input data sampled: train_data_A (450, 24, 128) train_data_B (450, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_225_226000.npyd_loss: 0.00544142, g_loss: 4.03244734\n",
      "(1, 24, 128) 1 ./samples/fake_B_225_226000.npy\n",
      "Epoch[226]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_226_227000.npyd_loss: 0.00100443, g_loss: 3.91516638\n",
      "(1, 24, 128) 1 ./samples/fake_B_226_227000.npy\n",
      "Epoch[227]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_227_228000.npyd_loss: 0.00542038, g_loss: 4.12665510\n",
      "(1, 24, 128) 1 ./samples/fake_B_227_228000.npy\n",
      "Epoch[228]: Input data sampled: train_data_A (443, 24, 128) train_data_B (443, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_228_229000.npyd_loss: 0.00956665, g_loss: 4.04869318\n",
      "(1, 24, 128) 1 ./samples/fake_B_228_229000.npy\n",
      "Epoch[229]: Input data sampled: train_data_A (438, 24, 128) train_data_B (438, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_229_230000.npyd_loss: 0.01144761, g_loss: 3.98637867\n",
      "(1, 24, 128) 1 ./samples/fake_B_229_230000.npy\n",
      "Epoch[230]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_230_231000.npyd_loss: 0.04672495, g_loss: 3.81133270\n",
      "(1, 24, 128) 1 ./samples/fake_B_230_231000.npy\n",
      "Epoch[231]: Input data sampled: train_data_A (444, 24, 128) train_data_B (444, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_231_232000.npyd_loss: 0.00769327, g_loss: 3.87466955\n",
      "(1, 24, 128) 1 ./samples/fake_B_231_232000.npy\n",
      "Epoch[232]: Input data sampled: train_data_A (460, 24, 128) train_data_B (460, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_232_233000.npyd_loss: 0.00146301, g_loss: 4.22844410\n",
      "(1, 24, 128) 1 ./samples/fake_B_232_233000.npy\n",
      "Epoch[233]: Input data sampled: train_data_A (447, 24, 128) train_data_B (447, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_233_234000.npyd_loss: 0.01679676, g_loss: 3.85545683\n",
      "(1, 24, 128) 1 ./samples/fake_B_233_234000.npy\n",
      "Epoch[234]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_234_235000.npyd_loss: 0.00905832, g_loss: 3.97418022\n",
      "(1, 24, 128) 1 ./samples/fake_B_234_235000.npy\n",
      "Epoch[235]: Input data sampled: train_data_A (445, 24, 128) train_data_B (445, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_235_236000.npyd_loss: 0.00242434, g_loss: 3.85835004\n",
      "(1, 24, 128) 1 ./samples/fake_B_235_236000.npy\n",
      "Epoch[236]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_236_237000.npyd_loss: 0.00091200, g_loss: 3.84862113\n",
      "(1, 24, 128) 1 ./samples/fake_B_236_237000.npy\n",
      "Epoch[237]: Input data sampled: train_data_A (457, 24, 128) train_data_B (457, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_237_238000.npyd_loss: 0.00561531, g_loss: 4.06176138\n",
      "(1, 24, 128) 1 ./samples/fake_B_237_238000.npy\n",
      "Epoch[238]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_238_239000.npyd_loss: 0.00335302, g_loss: 4.01049709\n",
      "(1, 24, 128) 1 ./samples/fake_B_238_239000.npy\n",
      "Epoch[239]: Input data sampled: train_data_A (445, 24, 128) train_data_B (445, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_239_240000.npyd_loss: 0.00744985, g_loss: 3.89880133\n",
      "(1, 24, 128) 1 ./samples/fake_B_239_240000.npy\n",
      "Epoch[240]: Input data sampled: train_data_A (459, 24, 128) train_data_B (459, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_240_241000.npyd_loss: 0.00533076, g_loss: 3.77780437\n",
      "(1, 24, 128) 1 ./samples/fake_B_240_241000.npy\n",
      "Epoch[241]: Input data sampled: train_data_A (444, 24, 128) train_data_B (444, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_241_242000.npyd_loss: 0.00269418, g_loss: 3.67735910\n",
      "(1, 24, 128) 1 ./samples/fake_B_241_242000.npy\n",
      "Epoch[242]: Input data sampled: train_data_A (438, 24, 128) train_data_B (438, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_242_243000.npyd_loss: 0.00567942, g_loss: 3.82353950\n",
      "(1, 24, 128) 1 ./samples/fake_B_242_243000.npy\n",
      "Epoch[243]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_243_244000.npyd_loss: 0.00711923, g_loss: 3.71646023\n",
      "(1, 24, 128) 1 ./samples/fake_B_243_244000.npy\n",
      "Epoch[244]: Input data sampled: train_data_A (443, 24, 128) train_data_B (443, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_244_245000.npyd_loss: 0.00035597, g_loss: 3.91387248\n",
      "(1, 24, 128) 1 ./samples/fake_B_244_245000.npy\n",
      "Epoch[245]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_245_246000.npyd_loss: 0.01174601, g_loss: 3.83478355\n",
      "(1, 24, 128) 1 ./samples/fake_B_245_246000.npy\n",
      "Epoch[246]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_246_247000.npyd_loss: 0.01603192, g_loss: 4.09260321\n",
      "(1, 24, 128) 1 ./samples/fake_B_246_247000.npy\n",
      "Epoch[247]: Input data sampled: train_data_A (441, 24, 128) train_data_B (441, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_247_248000.npyd_loss: 0.01076289, g_loss: 4.01696587\n",
      "(1, 24, 128) 1 ./samples/fake_B_247_248000.npy\n",
      "Epoch[248]: Input data sampled: train_data_A (450, 24, 128) train_data_B (450, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_248_249000.npyd_loss: 0.00251417, g_loss: 4.04832220\n",
      "(1, 24, 128) 1 ./samples/fake_B_248_249000.npy\n",
      "Epoch[249]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_249_250000.npyd_loss: 0.00194700, g_loss: 3.94435024\n",
      "(1, 24, 128) 1 ./samples/fake_B_249_250000.npy\n",
      "Epoch[250]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_250_251000.npyd_loss: 0.00807019, g_loss: 4.02989292\n",
      "(1, 24, 128) 1 ./samples/fake_B_250_251000.npy\n",
      "Epoch[251]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_251_252000.npyd_loss: 0.00419502, g_loss: 3.94048548\n",
      "(1, 24, 128) 1 ./samples/fake_B_251_252000.npy\n",
      "Epoch[252]: Input data sampled: train_data_A (440, 24, 128) train_data_B (440, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_252_253000.npyd_loss: 0.00310877, g_loss: 3.74231386\n",
      "(1, 24, 128) 1 ./samples/fake_B_252_253000.npy\n",
      "Epoch[253]: Input data sampled: train_data_A (458, 24, 128) train_data_B (458, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_253_254000.npyd_loss: 0.00390635, g_loss: 3.90965223\n",
      "(1, 24, 128) 1 ./samples/fake_B_253_254000.npy\n",
      "Epoch[254]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_254_255000.npyd_loss: 0.00103358, g_loss: 3.88302302\n",
      "(1, 24, 128) 1 ./samples/fake_B_254_255000.npy\n",
      "Epoch[255]: Input data sampled: train_data_A (440, 24, 128) train_data_B (440, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_255_256000.npyd_loss: 0.00131065, g_loss: 4.03797960\n",
      "(1, 24, 128) 1 ./samples/fake_B_255_256000.npy\n",
      "Epoch[256]: Input data sampled: train_data_A (444, 24, 128) train_data_B (444, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_256_257000.npyd_loss: 0.00792624, g_loss: 3.69714808\n",
      "(1, 24, 128) 1 ./samples/fake_B_256_257000.npy\n",
      "Epoch[257]: Input data sampled: train_data_A (456, 24, 128) train_data_B (456, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_257_258000.npyd_loss: 0.00271197, g_loss: 3.71462893\n",
      "(1, 24, 128) 1 ./samples/fake_B_257_258000.npy\n",
      "Epoch[258]: Input data sampled: train_data_A (461, 24, 128) train_data_B (461, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_258_259000.npyd_loss: 0.00740220, g_loss: 3.99100018\n",
      "(1, 24, 128) 1 ./samples/fake_B_258_259000.npy\n",
      "Epoch[259]: Input data sampled: train_data_A (444, 24, 128) train_data_B (444, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_259_260000.npyd_loss: 0.00222968, g_loss: 3.87132406\n",
      "(1, 24, 128) 1 ./samples/fake_B_259_260000.npy\n",
      "Epoch[260]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 24, 128) 1 ./samples/real_A_260_261000.npyd_loss: 0.00134818, g_loss: 3.96303725\n",
      "(1, 24, 128) 1 ./samples/fake_B_260_261000.npy\n",
      "Epoch[261]: Input data sampled: train_data_A (459, 24, 128) train_data_B (459, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_261_262000.npyd_loss: 0.00324058, g_loss: 3.62743735\n",
      "(1, 24, 128) 1 ./samples/fake_B_261_262000.npy\n",
      "Epoch[262]: Input data sampled: train_data_A (442, 24, 128) train_data_B (442, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_262_263000.npyd_loss: 0.00696255, g_loss: 3.84911132\n",
      "(1, 24, 128) 1 ./samples/fake_B_262_263000.npy\n",
      "Epoch[263]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_263_264000.npyd_loss: 0.00408752, g_loss: 3.75738668\n",
      "(1, 24, 128) 1 ./samples/fake_B_263_264000.npy\n",
      "Epoch[264]: Input data sampled: train_data_A (455, 24, 128) train_data_B (455, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_264_265000.npyd_loss: 0.02781518, g_loss: 3.89371848\n",
      "(1, 24, 128) 1 ./samples/fake_B_264_265000.npy\n",
      "Epoch[265]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_265_266000.npyd_loss: 0.01124246, g_loss: 3.74915195\n",
      "(1, 24, 128) 1 ./samples/fake_B_265_266000.npy\n",
      "Epoch[266]: Input data sampled: train_data_A (450, 24, 128) train_data_B (450, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_266_267000.npyd_loss: 0.00127531, g_loss: 3.90698791\n",
      "(1, 24, 128) 1 ./samples/fake_B_266_267000.npy\n",
      "Epoch[267]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_267_268000.npyd_loss: 0.00213361, g_loss: 3.91220331\n",
      "(1, 24, 128) 1 ./samples/fake_B_267_268000.npy\n",
      "Epoch[268]: Input data sampled: train_data_A (435, 24, 128) train_data_B (435, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_268_269000.npyd_loss: 0.00107168, g_loss: 3.76237512\n",
      "(1, 24, 128) 1 ./samples/fake_B_268_269000.npy\n",
      "Epoch[269]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_269_270000.npyd_loss: 0.00662572, g_loss: 3.74393082\n",
      "(1, 24, 128) 1 ./samples/fake_B_269_270000.npy\n",
      "Epoch[270]: Input data sampled: train_data_A (456, 24, 128) train_data_B (456, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_270_271000.npyd_loss: 0.00634402, g_loss: 3.82265043\n",
      "(1, 24, 128) 1 ./samples/fake_B_270_271000.npy\n",
      "Epoch[271]: Input data sampled: train_data_A (442, 24, 128) train_data_B (442, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_271_272000.npyd_loss: 0.00093492, g_loss: 4.00914669\n",
      "(1, 24, 128) 1 ./samples/fake_B_271_272000.npy\n",
      "Epoch[272]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_272_273000.npyd_loss: 0.00433680, g_loss: 4.08697367\n",
      "(1, 24, 128) 1 ./samples/fake_B_272_273000.npy\n",
      "Epoch[273]: Input data sampled: train_data_A (442, 24, 128) train_data_B (442, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_273_274000.npyd_loss: 0.00374900, g_loss: 3.88230491\n",
      "(1, 24, 128) 1 ./samples/fake_B_273_274000.npy\n",
      "Epoch[274]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_274_275000.npyd_loss: 0.00197502, g_loss: 3.67716527\n",
      "(1, 24, 128) 1 ./samples/fake_B_274_275000.npy\n",
      "Epoch[275]: Input data sampled: train_data_A (457, 24, 128) train_data_B (457, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_275_276000.npyd_loss: 0.00643349, g_loss: 3.75858593\n",
      "(1, 24, 128) 1 ./samples/fake_B_275_276000.npy\n",
      "Epoch[276]: Input data sampled: train_data_A (447, 24, 128) train_data_B (447, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_276_277000.npyd_loss: 0.00102782, g_loss: 3.70508957\n",
      "(1, 24, 128) 1 ./samples/fake_B_276_277000.npy\n",
      "Epoch[277]: Input data sampled: train_data_A (450, 24, 128) train_data_B (450, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_277_278000.npyd_loss: 0.00101717, g_loss: 3.79395485\n",
      "(1, 24, 128) 1 ./samples/fake_B_277_278000.npy\n",
      "Epoch[278]: Input data sampled: train_data_A (441, 24, 128) train_data_B (441, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_278_279000.npyd_loss: 0.00682249, g_loss: 3.79121780\n",
      "(1, 24, 128) 1 ./samples/fake_B_278_279000.npy\n",
      "Epoch[279]: Input data sampled: train_data_A (444, 24, 128) train_data_B (444, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_279_280000.npyd_loss: 0.00138227, g_loss: 3.85347939\n",
      "(1, 24, 128) 1 ./samples/fake_B_279_280000.npy\n",
      "Epoch[280]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_280_281000.npyd_loss: 0.00172293, g_loss: 3.89338970\n",
      "(1, 24, 128) 1 ./samples/fake_B_280_281000.npy\n",
      "Epoch[281]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_281_282000.npyd_loss: 0.00061791, g_loss: 3.65693188\n",
      "(1, 24, 128) 1 ./samples/fake_B_281_282000.npy\n",
      "Epoch[282]: Input data sampled: train_data_A (456, 24, 128) train_data_B (456, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_282_283000.npyd_loss: 0.00222656, g_loss: 3.74819183\n",
      "(1, 24, 128) 1 ./samples/fake_B_282_283000.npy\n",
      "Epoch[283]: Input data sampled: train_data_A (459, 24, 128) train_data_B (459, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_283_284000.npyd_loss: 0.00365326, g_loss: 3.94933987\n",
      "(1, 24, 128) 1 ./samples/fake_B_283_284000.npy\n",
      "Epoch[284]: Input data sampled: train_data_A (455, 24, 128) train_data_B (455, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_284_285000.npyd_loss: 0.00379511, g_loss: 3.80006504\n",
      "(1, 24, 128) 1 ./samples/fake_B_284_285000.npy\n",
      "Epoch[285]: Input data sampled: train_data_A (455, 24, 128) train_data_B (455, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_285_286000.npyd_loss: 0.00114225, g_loss: 3.70291758\n",
      "(1, 24, 128) 1 ./samples/fake_B_285_286000.npy\n",
      "Epoch[286]: Input data sampled: train_data_A (457, 24, 128) train_data_B (457, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_286_287000.npyd_loss: 0.00117062, g_loss: 3.83732295\n",
      "(1, 24, 128) 1 ./samples/fake_B_286_287000.npy\n",
      "Epoch[287]: Input data sampled: train_data_A (454, 24, 128) train_data_B (454, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_287_288000.npyd_loss: 0.00124999, g_loss: 3.66358638\n",
      "(1, 24, 128) 1 ./samples/fake_B_287_288000.npy\n",
      "Epoch[288]: Input data sampled: train_data_A (456, 24, 128) train_data_B (456, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_288_289000.npyd_loss: 0.00073614, g_loss: 3.72235084\n",
      "(1, 24, 128) 1 ./samples/fake_B_288_289000.npy\n",
      "Epoch[289]: Input data sampled: train_data_A (445, 24, 128) train_data_B (445, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_289_290000.npyd_loss: 0.00968940, g_loss: 3.76343393\n",
      "(1, 24, 128) 1 ./samples/fake_B_289_290000.npy\n",
      "Epoch[290]: Input data sampled: train_data_A (444, 24, 128) train_data_B (444, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_290_291000.npyd_loss: 0.00831648, g_loss: 3.69668937\n",
      "(1, 24, 128) 1 ./samples/fake_B_290_291000.npy\n",
      "Epoch[291]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_291_292000.npyd_loss: 0.00327802, g_loss: 3.88041449\n",
      "(1, 24, 128) 1 ./samples/fake_B_291_292000.npy\n",
      "Epoch[292]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_292_293000.npyd_loss: 0.00025805, g_loss: 3.72178650\n",
      "(1, 24, 128) 1 ./samples/fake_B_292_293000.npy\n",
      "Epoch[293]: Input data sampled: train_data_A (461, 24, 128) train_data_B (461, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_293_294000.npyd_loss: 0.00243112, g_loss: 3.81037712\n",
      "(1, 24, 128) 1 ./samples/fake_B_293_294000.npy\n",
      "Epoch[294]: Input data sampled: train_data_A (444, 24, 128) train_data_B (444, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_294_295000.npyd_loss: 0.02854825, g_loss: 3.79551578\n",
      "(1, 24, 128) 1 ./samples/fake_B_294_295000.npy\n",
      "Epoch[295]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_295_296000.npyd_loss: 0.01117452, g_loss: 3.64803743\n",
      "(1, 24, 128) 1 ./samples/fake_B_295_296000.npy\n",
      "Epoch[296]: Input data sampled: train_data_A (447, 24, 128) train_data_B (447, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_296_297000.npyd_loss: 0.00049919, g_loss: 3.85931468\n",
      "(1, 24, 128) 1 ./samples/fake_B_296_297000.npy\n",
      "Epoch[297]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 24, 128) 1 ./samples/real_A_297_298000.npyd_loss: 0.00453631, g_loss: 3.81627274\n",
      "(1, 24, 128) 1 ./samples/fake_B_297_298000.npy\n",
      "Epoch[298]: Input data sampled: train_data_A (443, 24, 128) train_data_B (443, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_298_299000.npyd_loss: 0.00116046, g_loss: 3.80257177\n",
      "(1, 24, 128) 1 ./samples/fake_B_298_299000.npy\n",
      "Epoch[299]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_299_300000.npyd_loss: 0.00123146, g_loss: 3.82566214\n",
      "(1, 24, 128) 1 ./samples/fake_B_299_300000.npy\n",
      "Epoch[300]: Input data sampled: train_data_A (449, 24, 128) train_data_B (449, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_300_301000.npyd_loss: 0.01357111, g_loss: 3.81202841\n",
      "(1, 24, 128) 1 ./samples/fake_B_300_301000.npy\n",
      "Epoch[301]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_301_302000.npyd_loss: 0.00019057, g_loss: 3.59310007\n",
      "(1, 24, 128) 1 ./samples/fake_B_301_302000.npy\n",
      "Epoch[302]: Input data sampled: train_data_A (460, 24, 128) train_data_B (460, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_302_303000.npyd_loss: 0.00023665, g_loss: 3.82614422\n",
      "(1, 24, 128) 1 ./samples/fake_B_302_303000.npy\n",
      "Epoch[303]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_303_304000.npyd_loss: 0.00108699, g_loss: 3.89723825\n",
      "(1, 24, 128) 1 ./samples/fake_B_303_304000.npy\n",
      "Epoch[304]: Input data sampled: train_data_A (450, 24, 128) train_data_B (450, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_304_305000.npyd_loss: 0.00494036, g_loss: 3.76321793\n",
      "(1, 24, 128) 1 ./samples/fake_B_304_305000.npy\n",
      "Epoch[305]: Input data sampled: train_data_A (442, 24, 128) train_data_B (442, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_305_306000.npyd_loss: 0.00045181, g_loss: 3.99734545\n",
      "(1, 24, 128) 1 ./samples/fake_B_305_306000.npy\n",
      "Epoch[306]: Input data sampled: train_data_A (440, 24, 128) train_data_B (440, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_306_307000.npyd_loss: 0.00167968, g_loss: 3.71934366\n",
      "(1, 24, 128) 1 ./samples/fake_B_306_307000.npy\n",
      "Epoch[307]: Input data sampled: train_data_A (459, 24, 128) train_data_B (459, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_307_308000.npyd_loss: 0.00054770, g_loss: 3.70512676\n",
      "(1, 24, 128) 1 ./samples/fake_B_307_308000.npy\n",
      "Epoch[308]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_308_309000.npyd_loss: 0.00041974, g_loss: 3.66976476\n",
      "(1, 24, 128) 1 ./samples/fake_B_308_309000.npy\n",
      "Epoch[309]: Input data sampled: train_data_A (439, 24, 128) train_data_B (439, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_309_310000.npyd_loss: 0.00813641, g_loss: 3.70779610\n",
      "(1, 24, 128) 1 ./samples/fake_B_309_310000.npy\n",
      "Epoch[310]: Input data sampled: train_data_A (452, 24, 128) train_data_B (452, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_310_311000.npyd_loss: 0.00039289, g_loss: 3.70853043\n",
      "(1, 24, 128) 1 ./samples/fake_B_310_311000.npy\n",
      "Epoch[311]: Input data sampled: train_data_A (448, 24, 128) train_data_B (448, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_311_312000.npyd_loss: 0.00145471, g_loss: 3.74612188\n",
      "(1, 24, 128) 1 ./samples/fake_B_311_312000.npy\n",
      "Epoch[312]: Input data sampled: train_data_A (451, 24, 128) train_data_B (451, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_312_313000.npyd_loss: 0.00030373, g_loss: 3.74441433\n",
      "(1, 24, 128) 1 ./samples/fake_B_312_313000.npy\n",
      "Epoch[313]: Input data sampled: train_data_A (453, 24, 128) train_data_B (453, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_313_314000.npyd_loss: 0.00046785, g_loss: 3.63872623\n",
      "(1, 24, 128) 1 ./samples/fake_B_313_314000.npy\n",
      "Epoch[314]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_314_315000.npyd_loss: 0.00210123, g_loss: 3.85577488\n",
      "(1, 24, 128) 1 ./samples/fake_B_314_315000.npy\n",
      "Epoch[315]: Input data sampled: train_data_A (446, 24, 128) train_data_B (446, 24, 128)\n",
      "(1, 24, 128) 1 ./samples/real_A_315_316000.npyd_loss: 0.00106201, g_loss: 3.69362903\n",
      "(1, 24, 128) 1 ./samples/fake_B_315_316000.npy\n",
      "Epoch[316]: Input data sampled: train_data_A (440, 24, 128) train_data_B (440, 24, 128)\n",
      "Epoch: [316] [   950/  1000] time: 63833.1951 d_loss: 0.00169549, g_loss: 3.72209215\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fb736d884352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmoMUNIT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" [*] Training finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-39029ed56a38>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0;31m# Update D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiscriminator_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    gan = EmoMUNIT(sess)\n",
    "    gan.build_model()\n",
    "    gan.train()\n",
    "    print(\" [*] Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf18_p35)",
   "language": "python",
   "name": "tf18_p35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
