{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "# from datetime import datetime\n",
    "# import argparse\n",
    "# import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data: .wav -> Pitch contour (f0s), Harmonic spectral envelope (sps), Aperiodic spectral envelope (aps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from ops import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "from IPython.display import Audio\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# matplotlib.rcParams['figure.figsize'] = (16, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder: Style_Encoder, Content_Encoder, MLP, Decoder, Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Style_Encoder(inputs, style_dim=16, reuse=False, scope='style_encoder'):                                                            # [1, 24, 128] = [batch_size, feature_channel, time]\n",
    "\n",
    "    inputs = tf.transpose(inputs, perm=[0, 2, 1], name='input_transpose')                                                               # [1, 128, 24] = [batch_size, time, feature_channel]\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "\n",
    "        h1 = conv1d_layer(inputs=inputs, filters=128, kernel_size=15, strides=1, name='h1_conv')                                        # [1, 128, 128]\n",
    "        h1_gates = conv1d_layer(inputs=inputs, filters=128, kernel_size=15, strides=1, name='h1_conv_gates')\n",
    "        h1_glu = gated_linear_layer(inputs=h1, gates=h1_gates, name='h1_glu')\n",
    "\n",
    "        # Downsample\n",
    "        d1 = downsample1d_block_withoutIN(inputs=h1_glu, filters=256, kernel_size=5, strides=2, name_prefix='downsample1d_block1')      # [1, 64, 256]\n",
    "        d2 = downsample1d_block_withoutIN(inputs=d1, filters=512, kernel_size=5, strides=2, name_prefix='downsample1d_block2')          # [1, 32, 512]\n",
    "\n",
    "        d3 = downsample1d_block_withoutIN(inputs=d2, filters=512, kernel_size=3, strides=2, name_prefix='downsample1d_block3')          # [1, 16, 512]\n",
    "        d4 = downsample1d_block_withoutIN(inputs=d3, filters=512, kernel_size=3, strides=2, name_prefix='downsample1d_block4')          # [1, 8, 512]\n",
    "\n",
    "        # Global Average Pooling\n",
    "        p1 = adaptive_avg_pooling(d4)                                                                                                   # [1, 1, 512]\n",
    "        style = conv1d_layer(inputs=p1, filters=style_dim, kernel_size=1, strides=1, name='SE_logit')                                   # [1, 1, 16]\n",
    "\n",
    "        return style                                                                                                                    # [1, 1, 16]\n",
    "\n",
    "\n",
    "def Content_Encoder(inputs, reuse=False, scope='content_encoder'):\n",
    "    # IN removes the original feature mean and variance that represent important style information\n",
    "    inputs = tf.transpose(inputs, perm=[0, 2, 1], name='input_transpose')                                                               # [1, 24, 128] = [batch_size, time, feature_channel]\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "\n",
    "        h1 = conv1d_layer(inputs=inputs, filters=128, kernel_size=15, strides=1, name='h1_conv')                                        # [1, 128, 128]\n",
    "        h1_norm = instance_norm_layer(inputs=h1, name='h1_norm')\n",
    "        h1_gates = conv1d_layer(inputs=inputs, filters=128, kernel_size=15, strides=1, name='h1_gates')\n",
    "        h1_norm_gates = instance_norm_layer(inputs=h1_gates, name='h1_norm_gates')\n",
    "        h1_glu = gated_linear_layer(inputs=h1_norm, gates=h1_norm_gates, name='h1_glu')\n",
    "\n",
    "        # downsample\n",
    "        d1 = downsample1d_block(inputs=h1_glu, filters=256, kernel_size=5, strides=2, name_prefix='downsample1d_block1')                # [1, 64, 256]\n",
    "        d2 = downsample1d_block(inputs=d1, filters=512, kernel_size=5, strides=2, name_prefix='downsample1d_block2')                    # [1, 32, 512]\n",
    "               \n",
    "        # Residual blocks\n",
    "        r1 = residual1d_block(inputs=d2, filters = 512, kernel_size=3, strides=1, name_prefix='residual1d_block1')                      # [1, 32, 512]\n",
    "        r2 = residual1d_block(inputs=r1, filters = 512, kernel_size=3, strides=1, name_prefix='residual1d_block2')\n",
    "        r3 = residual1d_block(inputs=r2, filters = 512, kernel_size=3, strides=1, name_prefix='residual1d_block3')\n",
    "        content = residual1d_block(inputs=r3, filters=512, kernel_size=3, strides=1, name_prefix='residual1d_block4')\n",
    "\n",
    "        return content                                                                                                                  # [1, 32, 512]\n",
    "\n",
    "\n",
    "def MLP(style, reuse=False, scope='MLP'):                                                                                               # [1, 1, 16]\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "\n",
    "        x1 = linear(style, 512, scope='linear_1')                                                                                       # [1, 1, 512]\n",
    "        x1_gates = linear(x1, 512, scope='linear_1_gates')\n",
    "        x1_glu = gated_linear_layer(inputs=x1, gates=x1_gates, name='x1_glu')\n",
    "\n",
    "        x2 = linear(x1_glu, 512, scope='linear_2')\n",
    "        x2_gates = linear(x2, 512, scope='linear_2_gates')\n",
    "        x2_glu = gated_linear_layer(inputs=x2, gates=x2_gates, name='x2_glu')\n",
    "\n",
    "        mu = linear(x2_glu, 512, scope='mu')\n",
    "        sigma = linear(x2_glu, 512, scope='sigma')\n",
    "\n",
    "        mu = tf.reshape(mu, shape=[-1, 1, 512])                                                                                         # [1, 1, 512]\n",
    "        sigma = tf.reshape(sigma, shape=[-1, 1, 512])                                                                                   # [1, 1, 512]\n",
    "\n",
    "        return mu, sigma                                                                                                                # [1, 1, 512]\n",
    "\n",
    "\n",
    "def Decoder(content, style, reuse=False, scope=\"decoder\"):\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "\n",
    "        mu, sigma = MLP(style, reuse)                                                                                                   # [1, 1, 512]\n",
    "        x = content                                                                                                                     # [1, 32, 512]\n",
    "\n",
    "        # Adaptive Residual blocks\n",
    "        r1 = residual1d_block_adaptive(inputs=x, filters=512, mu=mu, sigma=sigma, kernel_size=3, strides=1, name_prefix='residual1d_block1')        # [1, 32, 512]\n",
    "        r2 = residual1d_block_adaptive(inputs=r1, filters=512, mu=mu, sigma=sigma, kernel_size=3, strides=1, name_prefix='residual1d_block2')\n",
    "        r3 = residual1d_block_adaptive(inputs=r2, filters=512, mu=mu, sigma=sigma, kernel_size=3, strides=1, name_prefix='residual1d_block3')\n",
    "\n",
    "        # Upsample\n",
    "        u1 = upsample1d_block(inputs=r3, filters=512, kernel_size=5, strides=1, shuffle_size=2, name_prefix='upsample1d_block1')        # [1, 64, 512]\n",
    "        u2 = upsample1d_block(inputs=u1, filters=256, kernel_size=5, strides=1, shuffle_size=2, name_prefix='upsample1d_block2')        # [1, 128, 256]\n",
    "\n",
    "        # Output\n",
    "        o1 = conv1d_layer(inputs=u2, filters=24, kernel_size=15, strides=1, name='o1_conv')                                             # [1, 128, 24]\n",
    "        o2 = tf.transpose(o1, perm=[0, 2, 1], name='output_transpose')                                                                  # [1, 24, 128]\n",
    "\n",
    "        return o2                                                                                                                       # [1, 24, 128] = [batch_size, feature_channel, time]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator(inputs, reuse=False, scope='discriminator'):\n",
    "\n",
    "    # inputs = [batch_size, num_features, time]\n",
    "    # add channel for 2D convolution [batch_size, num_features, time, 1]\n",
    "    inputs = tf.expand_dims(inputs, -1)                                                                                                 # [1, 24, 128, 1]\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "\n",
    "        h1 = conv2d_layer(inputs=inputs, filters=128, kernel_size=[3, 3], strides=[1, 2], name='h1_conv')                               # [1, 24, 64, 128]\n",
    "        h1_gates = conv2d_layer(inputs=inputs, filters=128, kernel_size=[3, 3], strides=[1, 2], name='h1_conv_gates')\n",
    "        h1_glu = gated_linear_layer(inputs=h1, gates=h1_gates, name='h1_glu')\n",
    "\n",
    "        # Downsample\n",
    "        d1 = downsample2d_block(inputs=h1_glu, filters=256, kernel_size=[3, 3], strides=[2, 2], name_prefix='downsample2d_block1')      # [1, 12, 32, 256]\n",
    "        d2 = downsample2d_block(inputs=d1, filters=512, kernel_size=[3, 3], strides=[2, 2], name_prefix='downsample2d_block2')          # [1, 6, 16, 512]\n",
    "        d3 = downsample2d_block(inputs=d2, filters=1024, kernel_size=[6, 3], strides=[1, 2], name_prefix='downsample2d_block3')         # [1, 6, 8, 1024]\n",
    "\n",
    "        # Output\n",
    "        o1 = tf.layers.dense(inputs=d3, units=1, activation=tf.nn.sigmoid)\n",
    "\n",
    "        return [o1]                                                                                                                       # [1, 6, 8, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# Model\n",
    "##################################################################################\n",
    "\n",
    "def Encoder_A(x_A, reuse=False):\n",
    "    style_A = Style_Encoder(x_A, reuse=reuse, scope='style_encoder_A')\n",
    "    content_A = Content_Encoder(x_A, reuse=reuse, scope='content_encoder_A')\n",
    "\n",
    "    return content_A, style_A\n",
    "\n",
    "def Encoder_B(x_B, reuse=False):\n",
    "    style_B = Style_Encoder(x_B, reuse=reuse, scope='style_encoder_B')\n",
    "    content_B = Content_Encoder(x_B, reuse=reuse, scope='content_encoder_B')\n",
    "\n",
    "    return content_B, style_B\n",
    "\n",
    "def Decoder_A(content_B, style_A, reuse=False):\n",
    "    x_ba = Decoder(content=content_B, style=style_A, reuse=reuse, scope='decoder_A')\n",
    "\n",
    "    return x_ba\n",
    "\n",
    "def Decoder_B(content_A, style_B, reuse=False):\n",
    "    x_ab = Decoder(content=content_A, style=style_B, reuse=reuse, scope='decoder_B')\n",
    "\n",
    "    return x_ab\n",
    "\n",
    "def discriminate_real(x_A, x_B):\n",
    "    real_A_logit = Discriminator(x_A, scope=\"discriminator_A\")\n",
    "    real_B_logit = Discriminator(x_B, scope=\"discriminator_B\")\n",
    "\n",
    "    return real_A_logit, real_B_logit\n",
    "\n",
    "def discriminate_fake(x_ba, x_ab):\n",
    "    fake_A_logit = Discriminator(x_ba, reuse=True, scope=\"discriminator_A\")\n",
    "    fake_B_logit = Discriminator(x_ab, reuse=True, scope=\"discriminator_B\")\n",
    "\n",
    "    return fake_A_logit, fake_B_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module: EmoMUNIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoMUNIT(object):\n",
    "    def __init__(self, sess):\n",
    "        \n",
    "        self.train_A_dir = './../../../Database/Emotion/ang_neu/ang'\n",
    "        self.train_B_dir = './../../../Database/Emotion/ang_neu/neu'\n",
    "        self.validation_A_dir = './../../../Database/Emotion/ang_neu/val_ang'\n",
    "        self.validation_B_dir = './../../../Database/Emotion/ang_neu/val_neu'\n",
    "#         self.max_samples = 1000\n",
    "        \n",
    "        self.batch_size = 1\n",
    "        self.style_dim = 16\n",
    "        \n",
    "        self.Encoder_A = Encoder_A\n",
    "        self.Encoder_B = Encoder_B\n",
    "        self.Decoder_A = Decoder_A\n",
    "        self.Decoder_B = Decoder_B\n",
    "        self.discriminate_real = discriminate_real\n",
    "        self.discriminate_fake = discriminate_fake\n",
    "        \n",
    "        self.recon_x_cyc_w = 0.0\n",
    "        self.gan_type = 'lsgan'\n",
    "        \n",
    "        self.gan_w = 1.0\n",
    "        self.recon_x_w = 1.0\n",
    "        self.recon_s_w = 1.0\n",
    "        self.recon_c_w = 1.0\n",
    "        self.recon_x_cyc_w = 0.0\n",
    "               \n",
    "        self.audio_len = 128    # = n_frames, time_length\n",
    "        self.audio_ch = 24      # = num_mcep, num_features\n",
    "        \n",
    "        self.direction = 'A2B'\n",
    "        \n",
    "        self.model_name = 'EmoMUNIT'\n",
    "        self.gan_type = 'lsgan'\n",
    "        self.dataset_name = 'ang2neu'\n",
    "        self.log_dir = 'logs'\n",
    "        self.sample_dir = 'samples'\n",
    "        self.checkpoint_dir = 'checkpoint'\n",
    "        self.result_dir = 'results'\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.epoch = 300\n",
    "        self.iteration = 1000\n",
    "        self.init_lr_D = 0.0001\n",
    "        self.init_lr_G = 0.0002\n",
    "        \n",
    "        self.sample_freq = 1000\n",
    "        self.save_freq = 1000\n",
    "        \n",
    "        self.sampling_rate = 16000\n",
    "        self.frame_period = 5.0\n",
    "        self.num_mcep = 24\n",
    "        \n",
    "    \n",
    "    def build_model(self):\n",
    "        self.lr_D = tf.placeholder(tf.float32, name='learning_rate_D')\n",
    "        self.lr_G = tf.placeholder(tf.float32, name='learning_rate_G')\n",
    "        \n",
    "        # Iterate from train_data_A and train_data_A\n",
    "        self.domain_A = tf.placeholder(tf.float32, shape=[self.batch_size, self.audio_ch, self.audio_len], name='domain_a')\n",
    "        self.domain_B = tf.placeholder(tf.float32, shape=[self.batch_size, self.audio_ch, self.audio_len], name='domain_b')\n",
    "    \n",
    "        self.style_a = tf.placeholder(tf.float32, shape=[self.batch_size, 1, self.style_dim], name='style_a')\n",
    "        self.style_b = tf.placeholder(tf.float32, shape=[self.batch_size, 1, self.style_dim], name='style_b')  \n",
    "    \n",
    "        # encode\n",
    "        content_a, style_a_prime = self.Encoder_A(self.domain_A)\n",
    "        content_b, style_b_prime = self.Encoder_B(self.domain_B)\n",
    "\n",
    "        # decode (within domain)\n",
    "        x_aa = self.Decoder_A(content_B=content_a, style_A=style_a_prime)\n",
    "        x_bb = self.Decoder_B(content_A=content_b, style_B=style_b_prime)\n",
    "    \n",
    "        # decode (cross domain)\n",
    "        x_ba = self.Decoder_A(content_B=content_b, style_A=self.style_a, reuse=True)\n",
    "        x_ab = self.Decoder_B(content_A=content_a, style_B=self.style_b, reuse=True)   \n",
    "    \n",
    "        # encode again\n",
    "        content_b_, style_a_ = self.Encoder_A(x_ba, reuse=True)\n",
    "        content_a_, style_b_ = self.Encoder_B(x_ab, reuse=True)    \n",
    "    \n",
    "        # decode again (if needed)\n",
    "        if self.recon_x_cyc_w > 0 :\n",
    "            x_aba = self.Decoder_A(content_B=content_a_, style_A=style_a_prime, reuse=True)\n",
    "            x_bab = self.Decoder_B(content_A=content_b_, style_B=style_b_prime, reuse=True)\n",
    "\n",
    "            cyc_recon_A = L1_loss(x_aba, self.domain_A)\n",
    "            cyc_recon_B = L1_loss(x_bab, self.domain_B)\n",
    "\n",
    "        else :\n",
    "            cyc_recon_A = 0.0\n",
    "            cyc_recon_B = 0.0    \n",
    "      \n",
    "        real_A_logit, real_B_logit = self.discriminate_real(self.domain_A, self.domain_B)\n",
    "        fake_A_logit, fake_B_logit = self.discriminate_fake(x_ba, x_ab)   \n",
    "    \n",
    "    \n",
    "        \"\"\" Define Loss \"\"\"\n",
    "        # Adversarial Loss\n",
    "        G_ad_loss_a = generator_loss(self.gan_type, fake_A_logit)\n",
    "        G_ad_loss_b = generator_loss(self.gan_type, fake_B_logit)\n",
    "    \n",
    "        # Discrimination Loss (real/fake)\n",
    "        D_ad_loss_a = discriminator_loss(self.gan_type, real_A_logit, fake_A_logit)\n",
    "        D_ad_loss_b = discriminator_loss(self.gan_type, real_B_logit, fake_B_logit)\n",
    "    \n",
    "        # Reconstruction Loss\n",
    "        recon_A = L1_loss(x_aa, self.domain_A) # reconstruction\n",
    "        recon_B = L1_loss(x_bb, self.domain_B) # reconstruction   \n",
    "    \n",
    "        # Semi-CycleGAN Loss\n",
    "        # For style, encourages diverse outputs given different style codes\n",
    "        recon_style_A = L1_loss(style_a_, self.style_a)\n",
    "        recon_style_B = L1_loss(style_b_, self.style_b)\n",
    "    \n",
    "        # For content, encourages the translated image to preserve semantic content of the input image\n",
    "        recon_content_A = L1_loss(content_a_, content_a)\n",
    "        recon_content_B = L1_loss(content_b_, content_b)   \n",
    "    \n",
    "        # Attacker Loss\n",
    "        Generator_A_loss = self.gan_w * G_ad_loss_a + \\\n",
    "                                   self.recon_x_w * recon_A + \\\n",
    "                                   self.recon_s_w * recon_style_A + \\\n",
    "                                   self.recon_c_w * recon_content_A + \\\n",
    "                                   self.recon_x_cyc_w * cyc_recon_A\n",
    "\n",
    "        Generator_B_loss = self.gan_w * G_ad_loss_b + \\\n",
    "                           self.recon_x_w * recon_B + \\\n",
    "                           self.recon_s_w * recon_style_B + \\\n",
    "                           self.recon_c_w * recon_content_B + \\\n",
    "                           self.recon_x_cyc_w * cyc_recon_B   \n",
    "    \n",
    "        # Defender Loss\n",
    "        Discriminator_A_loss = self.gan_w * D_ad_loss_a\n",
    "        Discriminator_B_loss = self.gan_w * D_ad_loss_b\n",
    "    \n",
    "        # Total Loss\n",
    "        self.Generator_loss = Generator_A_loss + Generator_B_loss\n",
    "        self.Discriminator_loss = Discriminator_A_loss + Discriminator_B_loss\n",
    "    \n",
    "    \n",
    "        \"\"\" Training Variables \"\"\"\n",
    "        t_vars = tf.trainable_variables()\n",
    "        G_vars = [var for var in t_vars if 'decoder' in var.name or 'encoder' in var.name]\n",
    "        D_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
    "    \n",
    "        self.G_optim = tf.train.AdamOptimizer(self.lr_G, beta1=0.5, beta2=0.999).minimize(self.Generator_loss, var_list=G_vars)\n",
    "        self.D_optim = tf.train.AdamOptimizer(self.lr_D, beta1=0.5, beta2=0.999).minimize(self.Discriminator_loss, var_list=D_vars)\n",
    "    \n",
    "        \"\"\"\" Summary \"\"\"\n",
    "        self.all_G_loss = tf.summary.scalar(\"Generator_loss\", self.Generator_loss)\n",
    "        self.all_D_loss = tf.summary.scalar(\"Discriminator_loss\", self.Discriminator_loss)\n",
    "        \n",
    "        self.R_A_loss = tf.summary.scalar(\"Reconstruction_A_loss\", recon_A)\n",
    "        self.R_B_loss = tf.summary.scalar(\"Reconstruction_B_loss\", recon_B)\n",
    "        self.recon_style_A = tf.summary.scalar(\"recon_style_A\", recon_style_A)\n",
    "        self.recon_style_B = tf.summary.scalar(\"recon_style_B\", recon_style_B)        \n",
    "        self.recon_content_A = tf.summary.scalar(\"recon_content_A\", recon_content_A)\n",
    "        self.recon_content_B = tf.summary.scalar(\"recon_content_B\", recon_content_B)         \n",
    "        \n",
    "        self.G_A_loss = tf.summary.scalar(\"G_A_loss\", Generator_A_loss)\n",
    "        self.G_B_loss = tf.summary.scalar(\"G_B_loss\", Generator_B_loss)\n",
    "        self.G_ad_loss_a = tf.summary.scalar(\"G_ad_loss_a\", G_ad_loss_a)\n",
    "        self.G_ad_loss_b = tf.summary.scalar(\"G_ad_loss_b\", G_ad_loss_b)\n",
    "        \n",
    "        self.D_A_loss = tf.summary.scalar(\"D_A_loss\", Discriminator_A_loss)\n",
    "        self.D_B_loss = tf.summary.scalar(\"D_B_loss\", Discriminator_B_loss)\n",
    "\n",
    "        self.G_loss = tf.summary.merge([self.R_A_loss, self.R_B_loss, self.recon_style_A, self.recon_style_B, self.recon_content_A, self.recon_content_B, self.G_ad_loss_a, self.G_ad_loss_b, self.G_A_loss, self.G_B_loss, self.all_G_loss])\n",
    "        self.D_loss = tf.summary.merge([self.D_A_loss, self.D_B_loss, self.all_D_loss])\n",
    "    \n",
    "    \n",
    "        \"\"\" Speech: real and fake \"\"\"\n",
    "        self.real_A = self.domain_A\n",
    "        self.real_B = self.domain_B\n",
    "\n",
    "        self.fake_A = x_ba\n",
    "        self.fake_B = x_ab \n",
    "    \n",
    "        \"\"\" Test Variables \"\"\"\n",
    "        self.test_domain_A = tf.placeholder(tf.float32, [1, self.audio_ch, None], name='test_domain_a') # [1 24 None]\n",
    "        self.test_domain_B = tf.placeholder(tf.float32, [1, self.audio_ch, None], name='test_domain_b') # [1 24 None]\n",
    "        \n",
    "        self.test_style_a = tf.placeholder(tf.float32, [1, 1, self.style_dim], name='test_style_a')   # [1 1 16]\n",
    "        self.test_style_b = tf.placeholder(tf.float32, [1, 1, self.style_dim], name='test_style_b')   # [1 1 16]\n",
    "        \n",
    "        test_content_a, test_style_a = self.Encoder_A(self.test_domain_A, reuse=True)\n",
    "        test_content_b, test_style_b = self.Encoder_B(self.test_domain_B, reuse=True)\n",
    "\n",
    "        self.test_fake_A = self.Decoder_A(content_B=test_content_b, style_A=self.test_style_a, reuse=True)\n",
    "        self.test_fake_B = self.Decoder_B(content_A=test_content_a, style_B=self.test_style_b, reuse=True)\n",
    "\n",
    "        self.test_recon_A = self.Decoder_A(content_B=test_content_a, style_A=test_style_a, reuse=True)\n",
    "        self.test_recon_B = self.Decoder_B(content_A=test_content_b, style_B=test_style_b, reuse=True)\n",
    "        \n",
    "        \n",
    "        \"\"\" Guided Speech Translation \"\"\"\n",
    "        self.content_audio = tf.placeholder(tf.float32, [1, self.audio_ch, self.audio_len], name='content_audio')\n",
    "        self.style_audio = tf.placeholder(tf.float32, [1, self.audio_ch, self.audio_len], name='guide_style_audio_ch')\n",
    "\n",
    "        if self.direction == 'A2B' :\n",
    "            guide_content_A, guide_style_A = self.Encoder_A(self.content_audio, reuse=True)\n",
    "            guide_content_B, guide_style_B = self.Encoder_B(self.style_audio, reuse=True)\n",
    "\n",
    "        else :\n",
    "            guide_content_B, guide_style_B = self.Encoder_B(self.content_audio, reuse=True)\n",
    "            guide_content_A, guide_style_A = self.Encoder_A(self.style_audio, reuse=True)\n",
    "\n",
    "        self.guide_fake_A = self.Decoder_A(content_B=guide_content_B, style_A=guide_style_A, reuse=True)\n",
    "        self.guide_fake_B = self.Decoder_B(content_A=guide_content_A, style_B=guide_style_B, reuse=True)\n",
    "    \n",
    "    \n",
    "    def data_prepare(self, f0s_A, f0s_B, coded_sps_norm_A, coded_sps_norm_B):\n",
    "        \n",
    "        train_data_A = sample_train_data03(sps=list(coded_sps_norm_A), f0s=list(f0s_A), n_frames=self.audio_len)\n",
    "        train_data_B = sample_train_data03(sps=list(coded_sps_norm_B), f0s=list(f0s_B), n_frames=self.audio_len)\n",
    "\n",
    "        minlen = min(len(train_data_A), len(train_data_B))\n",
    "        np.random.shuffle(train_data_A)\n",
    "        np.random.shuffle(train_data_B)\n",
    "        train_data_A = np.array(train_data_A[0:minlen])\n",
    "        train_data_B = np.array(train_data_B[0:minlen])\n",
    "\n",
    "        return train_data_A, train_data_B\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        # saver to save model\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        # summary writer\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_dir, self.sess.graph)\n",
    "        \n",
    "        # restore check-point if it exits\n",
    "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
    "        if could_load:\n",
    "            start_epoch = (int)(checkpoint_counter / self.iteration)\n",
    "            start_batch_id = checkpoint_counter - start_epoch * self.iteration\n",
    "            counter = checkpoint_counter\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            start_batch_id = 0\n",
    "            counter = 1\n",
    "            print(\" [!] Load FAILED...\")\n",
    "            \n",
    "        # check sample_dir    \n",
    "        check_folder(self.sample_dir)\n",
    "        \n",
    "        \n",
    "        '''Training loop for epoch'''\n",
    "        \n",
    "        # load data and extract features\n",
    "        f0s_A, coded_sps_norm_A, log_f0s_mean_A, log_f0s_std_A, coded_sps_mean_A, coded_sps_std_A = vocoder_extract(self.train_A_dir)\n",
    "        f0s_B, coded_sps_norm_B, log_f0s_mean_B, log_f0s_std_B, coded_sps_mean_B, coded_sps_std_B = vocoder_extract(self.train_B_dir)\n",
    "        \n",
    "        # load validation data\n",
    "        wavs_val_A = load_wavs(wav_dir=self.validation_A_dir, sr=self.sampling_rate)\n",
    "        wavs_val_B = load_wavs(wav_dir=self.validation_B_dir, sr=self.sampling_rate)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(start_epoch, self.epoch):\n",
    "            \n",
    "            train_data_A, train_data_B = self.data_prepare(f0s_A, f0s_B, coded_sps_norm_A, coded_sps_norm_B)\n",
    "            print('Epoch[%d]: Input data sampled from %d A and %d B audio files: train_data_A' %(epoch, len(f0s_A), len(f0s_B)), np.shape(train_data_A), 'train_data_B', np.shape(train_data_B))\n",
    "\n",
    "            lr_D, lr_G = self.init_lr_D * pow(0.995, epoch), self.init_lr_G * pow(0.995, epoch)\n",
    "            for idx in range(start_batch_id, self.iteration):\n",
    "                style_a = np.random.normal(loc=0.0, scale=1.0, size=[self.batch_size, 1, self.style_dim])\n",
    "                style_b = np.random.normal(loc=0.0, scale=1.0, size=[self.batch_size, 1, self.style_dim])\n",
    "                \n",
    "                idx_A = idx%len(train_data_A)\n",
    "                idx_B = idx%len(train_data_B)\n",
    "                domain_A = train_data_A[idx_A:idx_A+1].astype('float32')\n",
    "                domain_B = train_data_B[idx_B:idx_B+1].astype('float32')\n",
    "                \n",
    "                train_feed_dict = {\n",
    "                    self.style_a : style_a,\n",
    "                    self.style_b : style_b,\n",
    "                    self.lr_D : lr_D,\n",
    "                    self.lr_G : lr_G,\n",
    "                    self.domain_A : domain_A,\n",
    "                    self.domain_B : domain_B\n",
    "                }\n",
    "                \n",
    "                # Update D\n",
    "                _, d_loss, summary_str = self.sess.run([self.D_optim, self.Discriminator_loss, self.D_loss], feed_dict = train_feed_dict)\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "            \n",
    "                # Update G\n",
    "                batch_A_audios, batch_B_audios, fake_A, fake_B, _, g_loss, summary_str = \\\n",
    "                self.sess.run([self.real_A, self.real_B, self.fake_A, self.fake_B, self.G_optim, \\\n",
    "                               self.Generator_loss, self.G_loss], feed_dict = train_feed_dict)\n",
    "                self.writer.add_summary(summary_str, counter)           \n",
    "            \n",
    "                # display training status\n",
    "                counter += 1\n",
    "                print(\"Epoch: [%2d] [%6d/%6d] time: %4.4f d_loss: %.8f, g_loss: %.8f\" \\\n",
    "                      % (epoch, idx, self.iteration, time.time() - start_time, d_loss, g_loss), end='\\r')\n",
    "            \n",
    "                # save generated samples\n",
    "                if np.mod(counter+1, self.sample_freq) == 0:\n",
    "                    # A2B\n",
    "                    idx_val_A = (counter//self.sample_freq)%len(wavs_val_A)\n",
    "                    wav = wavs_val_A[idx_val_A]\n",
    "                    wav = wav_padding(wav = wav, sr = self.sampling_rate, frame_period = self.frame_period, multiple = 4)\n",
    "                    # f0 conversion\n",
    "                    f0, timeaxis, sp, ap = world_decompose(wav = wav, fs = self.sampling_rate, frame_period = self.frame_period)\n",
    "                    f0_converted = pitch_conversion(f0 = f0, mean_log_src = log_f0s_mean_A, std_log_src = log_f0s_std_A, mean_log_target = log_f0s_mean_B, std_log_target = log_f0s_std_B)\n",
    "                    # sp normalization\n",
    "                    coded_sp = world_encode_spectral_envelop(sp = sp, fs = self.sampling_rate, dim = self.num_mcep)\n",
    "                    coded_sp_transposed = coded_sp.T\n",
    "                    coded_sp_norm = (coded_sp_transposed - coded_sps_mean_A) / coded_sps_std_A\n",
    "                    # random sampled style\n",
    "                    test_style_b = np.random.normal(loc=0.0, scale=1.0, size=[1, 1, self.style_dim])\n",
    "                    # sp conversion (A2B)\n",
    "                    coded_sp_converted_norm = self.sess.run(self.test_fake_B, feed_dict = {self.test_domain_A: np.array([coded_sp_norm]), self.test_style_b : test_style_b})\n",
    "                    coded_sp_converted_norm_recon = self.sess.run(self.test_recon_A, feed_dict = {self.test_domain_A: np.array([coded_sp_norm])})\n",
    "                    # [1,24,None]\n",
    "                    # de-normalization\n",
    "                    coded_sp_converted = coded_sp_converted_norm[0] * coded_sps_std_B + coded_sps_mean_B\n",
    "                    coded_sp_converted = coded_sp_converted.T\n",
    "                    coded_sp_converted = np.ascontiguousarray(coded_sp_converted)\n",
    "                    coded_sp_converted_recon = coded_sp_converted_norm_recon[0] * coded_sps_std_A + coded_sps_mean_A\n",
    "                    coded_sp_converted_recon = coded_sp_converted_recon.T\n",
    "                    coded_sp_converted_recon = np.ascontiguousarray(coded_sp_converted_recon)\n",
    "                    # combine converted f0, sp and ap\n",
    "                    decoded_sp_converted = world_decode_spectral_envelop(coded_sp = coded_sp_converted, fs = self.sampling_rate)\n",
    "                    decoded_sp_converted_recon = world_decode_spectral_envelop(coded_sp = coded_sp_converted_recon, fs = self.sampling_rate)                   \n",
    "                    wav_transformed = world_speech_synthesis(f0 = f0_converted, decoded_sp = decoded_sp_converted, ap = ap, fs = self.sampling_rate, frame_period = self.frame_period)\n",
    "                    wav_transformed_recon = world_speech_synthesis(f0 = f0, decoded_sp = decoded_sp_converted_recon, ap = ap, fs = self.sampling_rate, frame_period = self.frame_period)\n",
    "                    # write .wav file\n",
    "                    path_A2B = './{}/fake_A2B_id{:03d}_iter{:03d}K.wav'.format(self.sample_dir, idx_val_A, counter//1000)\n",
    "                    path_A2A = './{}/recon_A2A_id{:03d}_iter{:03d}K.wav'.format(self.sample_dir, idx_val_A, counter//1000)\n",
    "                    save_audio(wav=wav_transformed, path=path_A2B, sr=self.sampling_rate)\n",
    "                    save_audio(wav=wav_transformed_recon, path=path_A2A, sr=self.sampling_rate)\n",
    "                    \n",
    "                \n",
    "                # save checkpoints\n",
    "                if np.mod(counter+1, self.save_freq) == 0 :\n",
    "                    self.save(self.checkpoint_dir, counter)\n",
    "        \n",
    "            # After an epoch, start_batch_id reset to zero\n",
    "            # non-zero value is only for the first epoch after loading pre-trained model\n",
    "            start_batch_id = 0\n",
    "\n",
    "            # save model for final step\n",
    "            self.save(self.checkpoint_dir, counter)\n",
    "            \n",
    "        print(\" [*] Training finished!\")\n",
    "        \n",
    "    \n",
    "    def test(self):\n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "               \n",
    "        # saver to save model\n",
    "        self.saver = tf.train.Saver()\n",
    "    \n",
    "        # load check-point if it exits\n",
    "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
    "    \n",
    "        if could_load :\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "        else :\n",
    "            print(\" [!] Load FAILED...\")\n",
    "    \n",
    "        # check result_dir\n",
    "        check_folder(self.result_dir)\n",
    "        \n",
    "        # write html for visual comparison\n",
    "        \n",
    "        \n",
    "        # Get statistic from train_A, train_B\n",
    "        _, _, log_f0s_mean_A, log_f0s_std_A, coded_sps_A_mean, coded_sps_A_std = vocoder_extract(self.train_A_dir)\n",
    "        _, _, log_f0s_mean_B, log_f0s_std_B, coded_sps_B_mean, coded_sps_B_std = vocoder_extract(self.train_B_dir) \n",
    "        print('std_log_src:', log_f0s_std_A, 'std_log_target', log_f0s_std_B)\n",
    "        \n",
    "        \n",
    "        # A2B\n",
    "        test_files_A = os.listdir(self.validation_A_dir)\n",
    "        for i in range(len(test_files_A)):\n",
    "            file = test_files_A[i]\n",
    "            filepath = os.path.join(self.validation_A_dir, file)\n",
    "            wav, _ = librosa.load(filepath, sr = self.sampling_rate, mono = True)\n",
    "            wav = wav_padding(wav = wav, sr = self.sampling_rate, frame_period = self.frame_period, multiple = 4)\n",
    "            f0, timeaxis, sp, ap = world_decompose(wav = wav, fs = self.sampling_rate, frame_period = self.frame_period)\n",
    "            \n",
    "            # f0 conversion\n",
    "            f0_converted = pitch_conversion(f0 = f0, mean_log_src = log_f0s_mean_A, std_log_src = log_f0s_std_A, mean_log_target = log_f0s_mean_B, std_log_target = log_f0s_std_B)\n",
    "\n",
    "            # sp normalization\n",
    "            coded_sp = world_encode_spectral_envelop(sp = sp, fs = self.sampling_rate, dim = self.num_mcep)\n",
    "            coded_sp_transposed = coded_sp.T\n",
    "            coded_sp_norm = (coded_sp_transposed - coded_sps_A_mean) / coded_sps_A_std\n",
    "            \n",
    "            # random sampled style\n",
    "            test_style_b = np.random.normal(loc=0.0, scale=1.0, size=[1, 1, self.style_dim])\n",
    "            \n",
    "            # sp conversion (A2B)\n",
    "            coded_sp_converted_norm = self.sess.run(self.test_fake_B, feed_dict = {self.test_domain_A: np.array([coded_sp_norm]), self.test_style_b : test_style_b})\n",
    "            # [1,24,None]\n",
    "            \n",
    "            # print('coded_sp_converted_norm', np.shape(coded_sp_converted_norm[0]), 'coded_sps_B_mean', np.shape(coded_sps_B_mean), 'coded_sps_B_std:', np.shape(coded_sps_B_std))          \n",
    "            coded_sp_converted = coded_sp_converted_norm[0] * coded_sps_B_std + coded_sps_B_mean\n",
    "            coded_sp_converted = coded_sp_converted.T\n",
    "            coded_sp_converted = np.ascontiguousarray(coded_sp_converted)\n",
    "            decoded_sp_converted = world_decode_spectral_envelop(coded_sp = coded_sp_converted, fs = self.sampling_rate)\n",
    "            wav_transformed = world_speech_synthesis(f0 = f0_converted, decoded_sp = decoded_sp_converted, ap = ap, fs = self.sampling_rate, frame_period = self.frame_period)\n",
    "            librosa.output.write_wav(os.path.join(self.result_dir, os.path.basename(file)), wav_transformed, self.sampling_rate)\n",
    "            \n",
    "            print('converting test samples: [%d/%d]' %(i+1, len(test_files_A)), end='\\r')\n",
    "            \n",
    "        print(\" [*] Testing finished!\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        return \"{}_{}_{}\".format(self.model_name, self.dataset_name, self.gan_type)\n",
    "    \n",
    "    \n",
    "    def load(self, checkpoint_dir):\n",
    "        import re\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\", ckpt_name)).group(0))\n",
    "            print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "            return True, counter\n",
    "        else:\n",
    "            print(\" [*] Failed to find a checkpoint\")\n",
    "            return False, 0\n",
    "        \n",
    "        \n",
    "    def save(self, checkpoint_dir, step):\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess, os.path.join(checkpoint_dir, self.model_name + '.model'), global_step=step)        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/EmoMUNIT_ang2neu_lsgan/EmoMUNIT.model-1001\n",
      " [*] Success to read EmoMUNIT.model-1001\n",
      " [*] Load SUCCESS\n",
      "std_log_src: 0.385568596071032 std_log_target 0.28322630020087086\n",
      " [*] Testing finished!s: [19/19]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    model = EmoMUNIT(sess)\n",
    "    model.build_model()\n",
    "    model.train()\n",
    "    model.test()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module: CYCLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from ops_c import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(inputs, reuse = False, scope_name = 'generator'):\n",
    "\n",
    "    # inputs has shape [batch_size, num_features, time]\n",
    "    # we need to convert it to [batch_size, time, num_features] for 1D convolution\n",
    "    inputs = tf.transpose(inputs, perm = [0, 2, 1], name = 'input_transpose')\n",
    "\n",
    "    with tf.variable_scope(scope_name) as scope:\n",
    "        # Discriminator would be reused in CycleGAN\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        else:\n",
    "            assert scope.reuse is False\n",
    "\n",
    "        h1 = conv1d_layer(inputs = inputs, filters = 128, kernel_size = 15, strides = 1, activation = None, name = 'h1_conv')\n",
    "        h1_gates = conv1d_layer(inputs = inputs, filters = 128, kernel_size = 15, strides = 1, activation = None, name = 'h1_conv_gates')\n",
    "        h1_glu = gated_linear_layer(inputs = h1, gates = h1_gates, name = 'h1_glu')\n",
    "\n",
    "        # Downsample\n",
    "        d1 = downsample1d_block(inputs = h1_glu, filters = 256, kernel_size = 5, strides = 2, name_prefix = 'downsample1d_block1_')\n",
    "        d2 = downsample1d_block(inputs = d1, filters = 512, kernel_size = 5, strides = 2, name_prefix = 'downsample1d_block2_')\n",
    "\n",
    "        # Residual blocks\n",
    "        r1 = residual1d_block(inputs = d2, filters = 1024, kernel_size = 3, strides = 1, name_prefix = 'residual1d_block1_')\n",
    "        r2 = residual1d_block(inputs = r1, filters = 1024, kernel_size = 3, strides = 1, name_prefix = 'residual1d_block2_')\n",
    "        r3 = residual1d_block(inputs = r2, filters = 1024, kernel_size = 3, strides = 1, name_prefix = 'residual1d_block3_')\n",
    "        r4 = residual1d_block(inputs = r3, filters = 1024, kernel_size = 3, strides = 1, name_prefix = 'residual1d_block4_')\n",
    "        r5 = residual1d_block(inputs = r4, filters = 1024, kernel_size = 3, strides = 1, name_prefix = 'residual1d_block5_')\n",
    "        r6 = residual1d_block(inputs = r5, filters = 1024, kernel_size = 3, strides = 1, name_prefix = 'residual1d_block6_')\n",
    "\n",
    "        # Upsample\n",
    "        u1 = upsample1d_block(inputs = r6, filters = 1024, kernel_size = 5, strides = 1, shuffle_size = 2, name_prefix = 'upsample1d_block1_')\n",
    "        u2 = upsample1d_block(inputs = u1, filters = 512, kernel_size = 5, strides = 1, shuffle_size = 2, name_prefix = 'upsample1d_block2_')\n",
    "\n",
    "        # Output\n",
    "        o1 = conv1d_layer(inputs = u2, filters = 24, kernel_size = 15, strides = 1, activation = None, name = 'o1_conv')\n",
    "        o2 = tf.transpose(o1, perm = [0, 2, 1], name = 'output_transpose')\n",
    "\n",
    "    return o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(inputs, reuse = False, scope_name = 'discriminator'):\n",
    "\n",
    "    # inputs has shape [batch_size, num_features, time]\n",
    "    # we need to add channel for 2D convolution [batch_size, num_features, time, 1]\n",
    "    inputs = tf.expand_dims(inputs, -1)\n",
    "\n",
    "    with tf.variable_scope(scope_name) as scope:\n",
    "        # Discriminator would be reused in CycleGAN\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        else:\n",
    "            assert scope.reuse is False\n",
    "\n",
    "        h1 = conv2d_layer(inputs = inputs, filters = 128, kernel_size = [3, 3], strides = [1, 2], activation = None, name = 'h1_conv')\n",
    "        h1_gates = conv2d_layer(inputs = inputs, filters = 128, kernel_size = [3, 3], strides = [1, 2], activation = None, name = 'h1_conv_gates')\n",
    "        h1_glu = gated_linear_layer(inputs = h1, gates = h1_gates, name = 'h1_glu')\n",
    "\n",
    "        # Downsample\n",
    "        d1 = downsample2d_block(inputs = h1_glu, filters = 256, kernel_size = [3, 3], strides = [2, 2], name_prefix = 'downsample2d_block1_')\n",
    "        d2 = downsample2d_block(inputs = d1, filters = 512, kernel_size = [3, 3], strides = [2, 2], name_prefix = 'downsample2d_block2_')\n",
    "        d3 = downsample2d_block(inputs = d2, filters = 1024, kernel_size = [6, 3], strides = [1, 2], name_prefix = 'downsample2d_block3_')\n",
    "\n",
    "        # Output\n",
    "        o1 = tf.layers.dense(inputs = d3, units = 1, activation = tf.nn.sigmoid)\n",
    "\n",
    "        return o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(type, real, fake):\n",
    "    real = [real]\n",
    "    fake = [fake]\n",
    "    \n",
    "    n_scale = len(real)\n",
    "    loss = []\n",
    "\n",
    "    real_loss = 0\n",
    "    fake_loss = 0\n",
    "\n",
    "    for i in range(n_scale) :\n",
    "        if type == 'lsgan' :\n",
    "            real_loss = tf.reduce_mean(tf.squared_difference(real[i], 1.0))\n",
    "            fake_loss = tf.reduce_mean(tf.square(fake[i]))\n",
    "\n",
    "        if type == 'gan' :\n",
    "            real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(real[i]), logits=real[i]))\n",
    "            fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(fake[i]), logits=fake[i]))\n",
    "\n",
    "        loss.append(real_loss + fake_loss)\n",
    "\n",
    "    return sum(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(type, fake):\n",
    "    fake = [fake]\n",
    "    n_scale = len(fake)\n",
    "    loss = []\n",
    "\n",
    "    fake_loss = 0\n",
    "\n",
    "    for i in range(n_scale) :\n",
    "        if type == 'lsgan' :\n",
    "            fake_loss = tf.reduce_mean(tf.squared_difference(fake[i], 1.0))\n",
    "\n",
    "        if type == 'gan' :\n",
    "            fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(fake[i]), logits=fake[i]))\n",
    "\n",
    "        loss.append(fake_loss)\n",
    "\n",
    "    return sum(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CYCLE(object):\n",
    "    def __init__(self, sess, folder='S01/', source='neu', target='sad'):\n",
    "        \n",
    "        self.train_A_dir = './../../../Database/Emotion/' + folder + source + '_' + target + '/' + source\n",
    "        self.train_B_dir = './../../../Database/Emotion/' + folder + source + '_' + target + '/' + target\n",
    "        self.validation_A_dir = './../../../Database/Emotion/' + folder + source + '_' + target + '/' + 'val_' + source\n",
    "        self.validation_B_dir = './../../../Database/Emotion/' + folder + source + '_' + target + '/' + 'val_' + target\n",
    "        \n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        \n",
    "        self.batch_size = 1\n",
    "        self.max_samples = 1000\n",
    "        \n",
    "        self.frame_period = 5.0\n",
    "        self.num_mcep = 24\n",
    "        self.audio_ch = self.num_mcep\n",
    "        self.audio_len = 128                              # audio_len == n_frames, time_length\n",
    "        self.sampling_rate = 16000\n",
    "        \n",
    "        self.input_shape = [None, self.audio_ch, None]    # [batch_size, num_features, num_frames]\n",
    "    \n",
    "        self.init_lr_G = 0.0002\n",
    "        self.init_lr_D = 0.0001\n",
    "        self.lr_G_decay = self.init_lr_G / 200000\n",
    "        self.lr_D_decay = self.init_lr_D / 200000  \n",
    "        \n",
    "        self.cycle_w = 10.0\n",
    "        self.identity_w = 5.0\n",
    "        \n",
    "        \n",
    "        self.dataset_name = source + '_' + target\n",
    "        self.model_name = 'C'\n",
    "        self.gan_type = 'lsgan'\n",
    "        self.log_dir = \"logs/\" # + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.sample_dir = 'samples'\n",
    "        self.checkpoint_dir = 'checkpoint'\n",
    "        self.A2B_dir = 'results/' + source + '2' + target\n",
    "        self.B2A_dir = 'results/' + target + '2' + source\n",
    "        \n",
    "        self.sample_freq = 1000\n",
    "        self.save_freq = 1000\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.epoch = 200\n",
    "        self.iteration = 1000\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        # Placeholders for learning rate\n",
    "        self.lr_D = tf.placeholder(tf.float32, name='learning_rate_D')\n",
    "        self.lr_G = tf.placeholder(tf.float32, name='learning_rate_G')\n",
    "        \n",
    "        # Placeholders for real training samples\n",
    "        self.x_a = tf.placeholder(tf.float32, shape=self.input_shape, name='real_a')\n",
    "        self.x_b = tf.placeholder(tf.float32, shape=self.input_shape, name='real_b')\n",
    "        \n",
    "        # Placeholder for real test samples\n",
    "        self.x_a_test = tf.placeholder(tf.float32, shape = self.input_shape, name = 'real_a_test')\n",
    "        self.x_b_test = tf.placeholder(tf.float32, shape = self.input_shape, name = 'real_b_test')\n",
    "        \n",
    "        # Generation\n",
    "        x_ba = self.generator(inputs = self.x_a, reuse = False, scope_name = 'generator_A2B')\n",
    "        x_aba = self.generator(inputs = x_ba, reuse = False, scope_name = 'generator_B2A')\n",
    "        \n",
    "        x_ab = self.generator(inputs = self.x_b, reuse = True, scope_name = 'generator_B2A')\n",
    "        x_bab = self.generator(inputs = x_ab, reuse = True, scope_name = 'generator_A2B')        \n",
    "        \n",
    "        x_aa = self.generator(inputs = self.x_a, reuse = True, scope_name = 'generator_B2A')\n",
    "        x_bb = self.generator(inputs = self.x_b, reuse = True, scope_name = 'generator_A2B')\n",
    "        \n",
    "        # Discrimination\n",
    "        fake_A_logit = self.discriminator(inputs = x_ab, reuse = False, scope_name = 'discriminator_A')\n",
    "        fake_B_logit = self.discriminator(inputs = x_ba, reuse = False, scope_name = 'discriminator_B')\n",
    "        \n",
    "        real_A_logit = self.discriminator(inputs = self.x_a, reuse = True, scope_name = 'discriminator_A')\n",
    "        real_B_logit = self.discriminator(inputs = self.x_b, reuse = True, scope_name = 'discriminator_B')\n",
    "        \n",
    "        # Reserved for test\n",
    "        x_ba_test = self.generator(inputs = self.x_a_test, reuse = True, scope_name = 'generator_A2B')\n",
    "        x_aba_test = self.generator(inputs = x_ba_test, reuse = True, scope_name = 'generator_B2A')\n",
    "        \n",
    "        x_ab_test = self.generator(inputs = self.x_b_test, reuse = True, scope_name = 'generator_B2A')\n",
    "        x_bab_test = self.generator(inputs = x_ab_test, reuse = True, scope_name = 'generator_A2B')\n",
    "        \n",
    "        \n",
    "        \"\"\" Define Loss \"\"\"\n",
    "        # Cycle loss\n",
    "        cycle_loss = l1_loss(y = self.x_a, y_hat = x_aba) + l1_loss(y = self.x_b, y_hat = x_bab)\n",
    "        \n",
    "        # Identity loss\n",
    "        identity_loss = l1_loss(y = self.x_a, y_hat = x_aa) + l1_loss(y = self.x_b, y_hat = x_bb)\n",
    "\n",
    "        # adversarial Loss\n",
    "        G_ad_loss_a = l2_loss(y=tf.ones_like(fake_A_logit), y_hat=fake_A_logit)\n",
    "        G_ad_loss_b = l2_loss(y=tf.ones_like(fake_B_logit), y_hat=fake_B_logit)\n",
    "#         G_ad_loss_a = generator_loss(self.gan_type, fake_A_logit)\n",
    "#         G_ad_loss_b = generator_loss(self.gan_type, fake_B_logit)\n",
    "    \n",
    "        # Generator loss\n",
    "        self.Generator_loss = G_ad_loss_a + G_ad_loss_b + self.cycle_w*cycle_loss + self.identity_w*identity_loss\n",
    "    \n",
    "    \n",
    "        # real/fake Loss\n",
    "        D_loss_A_real = l2_loss(y = tf.ones_like(real_A_logit), y_hat = real_A_logit)\n",
    "        D_loss_A_fake = l2_loss(y = tf.zeros_like(fake_A_logit), y_hat = fake_A_logit)\n",
    "        D_ad_loss_a = (D_loss_A_real + D_loss_A_fake) / 2.0\n",
    "        \n",
    "        D_loss_B_real = l2_loss(y = tf.ones_like(real_B_logit), y_hat = real_B_logit)\n",
    "        D_loss_B_fake = l2_loss(y = tf.zeros_like(fake_B_logit), y_hat = fake_B_logit)\n",
    "        D_ad_loss_b = (D_loss_B_real + D_loss_B_fake) / 2.0\n",
    "#         D_ad_loss_a = discriminator_loss(self.gan_type, real_A_logit, fake_A_logit)\n",
    "#         D_ad_loss_b = discriminator_loss(self.gan_type, real_B_logit, fake_B_logit)\n",
    "        \n",
    "        # Discrimination Loss \n",
    "        self.Discriminator_loss = D_ad_loss_a + D_ad_loss_b\n",
    "    \n",
    "    \n",
    "        \"\"\" Training Variables \"\"\"\n",
    "        # Categorize variables to optimize D_vars and G_vars separately\n",
    "        t_vars = tf.trainable_variables()\n",
    "        G_vars = [var for var in t_vars if 'generator' in var.name]\n",
    "        D_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
    "\n",
    "        self.G_optim = tf.train.AdamOptimizer(self.lr_G, beta1=0.5, beta2=0.999).minimize(self.Generator_loss, var_list=G_vars)\n",
    "        self.D_optim = tf.train.AdamOptimizer(self.lr_D, beta1=0.5, beta2=0.999).minimize(self.Discriminator_loss, var_list=D_vars)\n",
    "    \n",
    "    \n",
    "        \"\"\"\" Summary \"\"\"\n",
    "        self.all_G_loss = tf.summary.scalar(\"Generator_loss\", self.Generator_loss)\n",
    "        self.all_D_loss = tf.summary.scalar(\"Discriminator_loss\", self.Discriminator_loss)\n",
    "        \n",
    "        self.Cycle_loss = tf.summary.scalar(\"Cycle_loss\", cycle_loss)\n",
    "        self.Identity_loss = tf.summary.scalar(\"Identity_loss\", identity_loss)\n",
    "\n",
    "        self.G_ad_loss_a = tf.summary.scalar(\"G_ad_loss_a\", G_ad_loss_a)\n",
    "        self.G_ad_loss_b = tf.summary.scalar(\"G_ad_loss_b\", G_ad_loss_b)\n",
    "        \n",
    "        self.D_ad_loss_a = tf.summary.scalar(\"D_ad_loss_a\", D_ad_loss_a)\n",
    "        self.D_ad_loss_b = tf.summary.scalar(\"D_ad_loss_b\", D_ad_loss_b)\n",
    "\n",
    "        self.G_loss = tf.summary.merge([self.G_ad_loss_a, self.G_ad_loss_b, self.Cycle_loss, self.Identity_loss, self.all_G_loss])\n",
    "        self.D_loss = tf.summary.merge([self.D_ad_loss_a, self.D_ad_loss_b, self.all_D_loss])\n",
    "    \n",
    "    \n",
    "        \"\"\" Speech: real and fake \"\"\"\n",
    "        self.real_A = self.x_a\n",
    "        self.real_B = self.x_b\n",
    "\n",
    "        self.fake_A = x_ab\n",
    "        self.fake_B = x_ba\n",
    "        \n",
    "        self.test_fake_A = x_ab_test\n",
    "        self.test_fake_B = x_ba_test\n",
    "        \n",
    "        self.test_cycle_A = x_aba_test\n",
    "        self.test_cycle_B = x_bab_test\n",
    "    \n",
    "    \n",
    "    def data_prepare(self, f0s_A, f0s_B, coded_sps_norm_A, coded_sps_norm_B):\n",
    "        \n",
    "        train_data_A = sample_train_data03(sps=list(coded_sps_norm_A), f0s=list(f0s_A), n_frames=self.audio_len)\n",
    "        train_data_B = sample_train_data03(sps=list(coded_sps_norm_B), f0s=list(f0s_B), n_frames=self.audio_len)\n",
    "\n",
    "        minlen = min(len(train_data_A), len(train_data_B))\n",
    "        np.random.shuffle(train_data_A)\n",
    "        np.random.shuffle(train_data_B)\n",
    "        train_data_A = np.array(train_data_A[0:minlen])\n",
    "        train_data_B = np.array(train_data_B[0:minlen])\n",
    "\n",
    "        return train_data_A, train_data_B\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        # saver to save model\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        # summary writer\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_dir, self.sess.graph)\n",
    "        \n",
    "        # restore check-point if it exits\n",
    "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
    "        if could_load:\n",
    "            start_epoch = (int)(checkpoint_counter / self.iteration)\n",
    "            start_batch_id = checkpoint_counter - start_epoch * self.iteration\n",
    "            counter = checkpoint_counter\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            start_batch_id = 0\n",
    "            counter = 1\n",
    "            print(\" [!] Load FAILED...\")\n",
    "            \n",
    "        # check sample_dir    \n",
    "        check_folder(self.sample_dir)\n",
    "        \n",
    "        \n",
    "        '''Training loop for epoch'''\n",
    "        \n",
    "        # load data and extract features\n",
    "        f0s_A, coded_sps_norm_A, log_f0s_mean_A, log_f0s_std_A, coded_sps_mean_A, coded_sps_std_A = vocoder_extract(self.train_A_dir)\n",
    "        f0s_B, coded_sps_norm_B, log_f0s_mean_B, log_f0s_std_B, coded_sps_mean_B, coded_sps_std_B = vocoder_extract(self.train_B_dir)\n",
    "        \n",
    "        # load validation data\n",
    "        wavs_val_A = load_wavs(wav_dir=self.validation_A_dir, sr=self.sampling_rate)\n",
    "        wavs_val_B = load_wavs(wav_dir=self.validation_B_dir, sr=self.sampling_rate)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(start_epoch, self.epoch):\n",
    "            \n",
    "            train_data_A, train_data_B = self.data_prepare(f0s_A, f0s_B, coded_sps_norm_A, coded_sps_norm_B)\n",
    "            print('Epoch[%d]: Input data sampled from %d A and %d B audio files: train_data_A' %(epoch, len(f0s_A), len(f0s_B)), np.shape(train_data_A), 'train_data_B', np.shape(train_data_B))\n",
    "\n",
    "            lr_D = self.init_lr_D\n",
    "            lr_G = self.init_lr_G\n",
    "            if counter > 20000:\n",
    "                self.identity_w = 0.0                            # deactivate identity loss\n",
    "#             if counter > 150000:\n",
    "#                 lr_G = max(0.0, lr_G - self.lr_G_decay)\n",
    "#                 lr_D = max(0.0, lr_D - self.lr_D_decay)\n",
    "#             lr_D = self.init_lr_D * pow(0.995, epoch)\n",
    "#             lr_G = self.init_lr_G * pow(0.995, epoch)\n",
    "\n",
    "            if counter < 50000:\n",
    "                G_iter = 2\n",
    "            elif 50000 <= counter < 100000:\n",
    "                G_iter = np.random.randint(2) + 1\n",
    "            else:\n",
    "                G_iter = 1\n",
    "\n",
    "#             n_iter = len(train_data_A)\n",
    "            n_iter = self.iteration\n",
    "        \n",
    "            for idx in range(start_batch_id, n_iter):\n",
    "\n",
    "                idx_A = idx%len(train_data_A)\n",
    "                idx_B = idx%len(train_data_B)\n",
    "                domain_A = train_data_A[idx_A:idx_A+1].astype('float32')\n",
    "                domain_B = train_data_B[idx_B:idx_B+1].astype('float32')\n",
    "                \n",
    "                train_feed_dict = {\n",
    "                    self.lr_D : lr_D,\n",
    "                    self.lr_G : lr_G,\n",
    "                    self.x_a : domain_A,\n",
    "                    self.x_b : domain_B\n",
    "                }\n",
    "                \n",
    "                # Update D\n",
    "                if counter % G_iter == G_iter - 1:\n",
    "                    _, d_loss, summary_str = self.sess.run([self.D_optim, self.Discriminator_loss, self.D_loss], feed_dict = train_feed_dict)\n",
    "                    self.writer.add_summary(summary_str, counter)\n",
    "            \n",
    "                # Update G\n",
    "                batch_A_audios, batch_B_audios, fake_A, fake_B, _, g_loss, summary_str = \\\n",
    "                self.sess.run([self.real_A, self.real_B, self.fake_A, self.fake_B, self.G_optim, \\\n",
    "                               self.Generator_loss, self.G_loss], feed_dict = train_feed_dict)\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "            \n",
    "            \n",
    "                ''' Display Training Status '''\n",
    "                counter += 1\n",
    "                print(\"Epoch: [%2d] [%6d/%6d] time: %4.4f d_loss: %.8f, g_loss: %.8f\" \\\n",
    "                      % (epoch, idx, n_iter, time.time() - start_time, d_loss, g_loss), end='\\r')\n",
    "            \n",
    "                # save generated samples\n",
    "                if np.mod(counter+1, self.sample_freq) == 0:\n",
    "                    # A2B\n",
    "                    idx_val_A = (counter//self.sample_freq)%len(wavs_val_A)\n",
    "                    wav = wavs_val_A[idx_val_A]\n",
    "                    wav = wav_padding(wav = wav, sr = self.sampling_rate, frame_period = self.frame_period, multiple = 4)\n",
    "                    \n",
    "                    # f0 conversion\n",
    "                    f0, timeaxis, sp, ap = world_decompose(wav = wav, fs = self.sampling_rate, frame_period = self.frame_period)\n",
    "                    f0_converted = pitch_conversion(f0 = f0, mean_log_src = log_f0s_mean_A, std_log_src = log_f0s_std_A, mean_log_target = log_f0s_mean_B, std_log_target = log_f0s_std_B)\n",
    "                    \n",
    "                    # sp normalization\n",
    "                    coded_sp = world_encode_spectral_envelop(sp = sp, fs = self.sampling_rate, dim = self.num_mcep)\n",
    "                    coded_sp_transposed = coded_sp.T\n",
    "                    coded_sp_norm = (coded_sp_transposed - coded_sps_mean_A) / coded_sps_std_A\n",
    "\n",
    "                    # sp conversion (A2B)\n",
    "                    coded_sp_converted_norm = self.sess.run(self.test_fake_B, feed_dict = {self.x_a_test: np.array([coded_sp_norm])})\n",
    "                    coded_sp_converted_norm_cycle = self.sess.run(self.test_cycle_A, feed_dict = {self.x_a_test: np.array([coded_sp_norm])})\n",
    "                    # [1,24,None]\n",
    "                    \n",
    "                    # de-normalization\n",
    "                    coded_sp_converted = coded_sp_converted_norm[0] * coded_sps_std_B + coded_sps_mean_B\n",
    "                    coded_sp_converted = coded_sp_converted.T\n",
    "                    coded_sp_converted = np.ascontiguousarray(coded_sp_converted)\n",
    "                    coded_sp_converted_cycle = coded_sp_converted_norm_cycle[0] * coded_sps_std_A + coded_sps_mean_A\n",
    "                    coded_sp_converted_cycle = coded_sp_converted_cycle.T\n",
    "                    coded_sp_converted_cycle = np.ascontiguousarray(coded_sp_converted_cycle)\n",
    "                    \n",
    "                    # combine converted f0, sp and ap\n",
    "                    decoded_sp_converted = world_decode_spectral_envelop(coded_sp = coded_sp_converted, fs = self.sampling_rate)\n",
    "                    decoded_sp_converted_cycle = world_decode_spectral_envelop(coded_sp = coded_sp_converted_cycle, fs = self.sampling_rate)                   \n",
    "                    wav_transformed = world_speech_synthesis(f0 = f0_converted, decoded_sp = decoded_sp_converted, ap = ap, fs = self.sampling_rate, frame_period = self.frame_period)\n",
    "                    wav_transformed_cycle = world_speech_synthesis(f0 = f0, decoded_sp = decoded_sp_converted_cycle, ap = ap, fs = self.sampling_rate, frame_period = self.frame_period)\n",
    "                    \n",
    "                    # write .wav file\n",
    "                    path_A2B = './{}/fake_A2B_id{:03d}_iter{:03d}K.wav'.format(self.sample_dir, idx_val_A, counter//1000)\n",
    "                    path_A2A = './{}/cycle_A2A_id{:03d}_iter{:03d}K.wav'.format(self.sample_dir, idx_val_A, counter//1000)\n",
    "                    save_audio(wav=wav_transformed, path=path_A2B, sr=self.sampling_rate)\n",
    "                    save_audio(wav=wav_transformed_cycle, path=path_A2A, sr=self.sampling_rate)\n",
    "                    \n",
    "                \n",
    "                # save checkpoints\n",
    "                if np.mod(counter+1, self.save_freq) == 0 :\n",
    "                    self.save(self.checkpoint_dir, counter)\n",
    "        \n",
    "            # After an epoch, start_batch_id reset to zero\n",
    "            # non-zero value is only for the first epoch after loading pre-trained model\n",
    "            start_batch_id = 0\n",
    "\n",
    "            # save model for final step\n",
    "            self.save(self.checkpoint_dir, counter)\n",
    "            \n",
    "        print(\" [*] Training finished!\")\n",
    "        \n",
    "    \n",
    "    def test(self):\n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "               \n",
    "        # saver to save model\n",
    "        self.saver = tf.train.Saver()\n",
    "    \n",
    "        # load check-point if it exits\n",
    "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
    "    \n",
    "        if could_load :\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "        else :\n",
    "            print(\" [!] Load FAILED...\")\n",
    "    \n",
    "        # check result_dir\n",
    "        check_folder(self.A2B_dir)\n",
    "        check_folder(self.B2A_dir)\n",
    "        \n",
    "        # Get statistic from train_A, train_B\n",
    "        _, _, log_f0s_mean_A, log_f0s_std_A, coded_sps_A_mean, coded_sps_A_std = vocoder_extract(self.train_A_dir)\n",
    "        _, _, log_f0s_mean_B, log_f0s_std_B, coded_sps_B_mean, coded_sps_B_std = vocoder_extract(self.train_B_dir) \n",
    "        print('std_log_src:', log_f0s_std_A, 'std_log_target', log_f0s_std_B)\n",
    "        \n",
    "        \n",
    "        # A2B\n",
    "        test_files_A = os.listdir(self.validation_A_dir)\n",
    "        for i in range(len(test_files_A)):\n",
    "            file = test_files_A[i]\n",
    "            filepath = os.path.join(self.validation_A_dir, file)\n",
    "            wav, _ = librosa.load(filepath, sr = self.sampling_rate, mono = True)\n",
    "            wav = wav_padding(wav = wav, sr = self.sampling_rate, frame_period = self.frame_period, multiple = 4)\n",
    "            f0, timeaxis, sp, ap = world_decompose(wav = wav, fs = self.sampling_rate, frame_period = self.frame_period)\n",
    "            \n",
    "            # f0 conversion\n",
    "            f0_converted = pitch_conversion(f0 = f0, mean_log_src = log_f0s_mean_A, std_log_src = log_f0s_std_A, mean_log_target = log_f0s_mean_B, std_log_target = log_f0s_std_B)\n",
    "\n",
    "            # sp normalization\n",
    "            coded_sp = world_encode_spectral_envelop(sp = sp, fs = self.sampling_rate, dim = self.num_mcep)\n",
    "            coded_sp_transposed = coded_sp.T\n",
    "            coded_sp_norm = (coded_sp_transposed - coded_sps_A_mean) / coded_sps_A_std\n",
    "            \n",
    "            # sp conversion (A2B)\n",
    "            coded_sp_converted_norm = self.sess.run(self.test_fake_B, feed_dict = {self.x_a_test: np.array([coded_sp_norm])})\n",
    "            # [1,24,None]\n",
    "            \n",
    "            # print('coded_sp_converted_norm', np.shape(coded_sp_converted_norm[0]), 'coded_sps_B_mean', np.shape(coded_sps_B_mean), 'coded_sps_B_std:', np.shape(coded_sps_B_std))          \n",
    "            coded_sp_converted = coded_sp_converted_norm[0] * coded_sps_B_std + coded_sps_B_mean\n",
    "            coded_sp_converted = coded_sp_converted.T\n",
    "            coded_sp_converted = np.ascontiguousarray(coded_sp_converted)\n",
    "            decoded_sp_converted = world_decode_spectral_envelop(coded_sp = coded_sp_converted, fs = self.sampling_rate)\n",
    "            wav_transformed = world_speech_synthesis(f0 = f0_converted, decoded_sp = decoded_sp_converted, ap = ap, fs = self.sampling_rate, frame_period = self.frame_period)\n",
    "            librosa.output.write_wav(os.path.join(self.A2B_dir, os.path.basename(file)), wav_transformed, self.sampling_rate)\n",
    "            \n",
    "            print('converting test samples A2B: [%d/%d]' %(i+1, len(test_files_A)), end='\\r')\n",
    "        \n",
    "        # B2A\n",
    "        test_files_B = os.listdir(self.validation_B_dir)\n",
    "        for i in range(len(test_files_B)):\n",
    "            file = test_files_B[i]\n",
    "            filepath = os.path.join(self.validation_B_dir, file)\n",
    "            wav, _ = librosa.load(filepath, sr = self.sampling_rate, mono = True)\n",
    "            wav = wav_padding(wav = wav, sr = self.sampling_rate, frame_period = self.frame_period, multiple = 4)\n",
    "            f0, timeaxis, sp, ap = world_decompose(wav = wav, fs = self.sampling_rate, frame_period = self.frame_period)\n",
    "            \n",
    "            # f0 conversion\n",
    "            f0_converted = pitch_conversion(f0 = f0, mean_log_src = log_f0s_mean_B, std_log_src = log_f0s_std_B, mean_log_target = log_f0s_mean_A, std_log_target = log_f0s_std_A)\n",
    "\n",
    "            # sp normalization\n",
    "            coded_sp = world_encode_spectral_envelop(sp = sp, fs = self.sampling_rate, dim = self.num_mcep)\n",
    "            coded_sp_transposed = coded_sp.T\n",
    "            coded_sp_norm = (coded_sp_transposed - coded_sps_B_mean) / coded_sps_B_std\n",
    "            \n",
    "            # sp conversion (A2B)\n",
    "            coded_sp_converted_norm = self.sess.run(self.test_fake_A, feed_dict = {self.x_b_test: np.array([coded_sp_norm])})\n",
    "            # [1,24,None]\n",
    "            \n",
    "            # print('coded_sp_converted_norm', np.shape(coded_sp_converted_norm[0]), 'coded_sps_A_mean', np.shape(coded_sps_A_mean), 'coded_sps_A_std:', np.shape(coded_sps_A_std))          \n",
    "            coded_sp_converted = coded_sp_converted_norm[0] * coded_sps_A_std + coded_sps_A_mean\n",
    "            coded_sp_converted = coded_sp_converted.T\n",
    "            coded_sp_converted = np.ascontiguousarray(coded_sp_converted)\n",
    "            decoded_sp_converted = world_decode_spectral_envelop(coded_sp = coded_sp_converted, fs = self.sampling_rate)\n",
    "            wav_transformed = world_speech_synthesis(f0 = f0_converted, decoded_sp = decoded_sp_converted, ap = ap, fs = self.sampling_rate, frame_period = self.frame_period)\n",
    "            librosa.output.write_wav(os.path.join(self.B2A_dir, os.path.basename(file)), wav_transformed, self.sampling_rate)\n",
    "            \n",
    "            print('converting test samples B2A: [%d/%d]' %(i+1, len(test_files_B)), end='\\r')\n",
    "        \n",
    "        \n",
    "        print(\" [*] Testing finished!\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        return \"{}_{}_{}\".format(self.model_name, self.dataset_name, self.gan_type)\n",
    "    \n",
    "    \n",
    "    def load(self, checkpoint_dir):\n",
    "        import re\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\", ckpt_name)).group(0))\n",
    "            print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "            return True, counter\n",
    "        else:\n",
    "            print(\" [*] Failed to find a checkpoint\")\n",
    "            return False, 0\n",
    "        \n",
    "        \n",
    "    def save(self, checkpoint_dir, step):\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess, os.path.join(checkpoint_dir, self.model_name + '.model'), global_step=step)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] Reading checkpoints...\n",
      " [*] Failed to find a checkpoint\n",
      " [!] Load FAILED...\n",
      "Epoch[0]: Input data sampled from 133 A and 115 B audio files: train_data_A (493, 24, 128) train_data_B (493, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id000_iter000K.wav shape: (142080,)s: 22.93839264\n",
      "saved file at ./samples/cycle_A2A_id000_iter000K.wav shape: (142080,)\n",
      "Epoch[1]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id001_iter001K.wav shape: (65280,)ss: 18.70155334\n",
      "saved file at ./samples/cycle_A2A_id001_iter001K.wav shape: (65280,)\n",
      "Epoch[2]: Input data sampled from 133 A and 115 B audio files: train_data_A (487, 24, 128) train_data_B (487, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id002_iter002K.wav shape: (23360,)ss: 20.25951195\n",
      "saved file at ./samples/cycle_A2A_id002_iter002K.wav shape: (23360,)\n",
      "Epoch[3]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id003_iter003K.wav shape: (94720,)ss: 18.01861572\n",
      "saved file at ./samples/cycle_A2A_id003_iter003K.wav shape: (94720,)\n",
      "Epoch[4]: Input data sampled from 133 A and 115 B audio files: train_data_A (500, 24, 128) train_data_B (500, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id004_iter004K.wav shape: (30400,)oss: 16.95998001\n",
      "saved file at ./samples/cycle_A2A_id004_iter004K.wav shape: (30400,)\n",
      "Epoch[5]: Input data sampled from 133 A and 115 B audio files: train_data_A (493, 24, 128) train_data_B (493, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id005_iter005K.wav shape: (136000,)ss: 15.22611618\n",
      "saved file at ./samples/cycle_A2A_id005_iter005K.wav shape: (136000,)\n",
      "Epoch[6]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id006_iter006K.wav shape: (93440,)oss: 14.92996311\n",
      "saved file at ./samples/cycle_A2A_id006_iter006K.wav shape: (93440,)\n",
      "Epoch[7]: Input data sampled from 133 A and 115 B audio files: train_data_A (493, 24, 128) train_data_B (493, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id007_iter007K.wav shape: (18240,)oss: 14.34853935\n",
      "saved file at ./samples/cycle_A2A_id007_iter007K.wav shape: (18240,)\n",
      "Epoch[8]: Input data sampled from 133 A and 115 B audio files: train_data_A (493, 24, 128) train_data_B (493, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id008_iter008K.wav shape: (31040,)oss: 18.16377640\n",
      "saved file at ./samples/cycle_A2A_id008_iter008K.wav shape: (31040,)\n",
      "Epoch[9]: Input data sampled from 133 A and 115 B audio files: train_data_A (493, 24, 128) train_data_B (493, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id009_iter009K.wav shape: (83200,)oss: 13.20281410\n",
      "saved file at ./samples/cycle_A2A_id009_iter009K.wav shape: (83200,)\n",
      "Epoch[10]: Input data sampled from 133 A and 115 B audio files: train_data_A (511, 24, 128) train_data_B (511, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id010_iter010K.wav shape: (39360,)oss: 13.48589706\n",
      "saved file at ./samples/cycle_A2A_id010_iter010K.wav shape: (39360,)\n",
      "Epoch[11]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id011_iter011K.wav shape: (27520,)oss: 15.51246071\n",
      "saved file at ./samples/cycle_A2A_id011_iter011K.wav shape: (27520,)\n",
      "Epoch[12]: Input data sampled from 133 A and 115 B audio files: train_data_A (497, 24, 128) train_data_B (497, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id012_iter012K.wav shape: (59200,)oss: 14.32551670\n",
      "saved file at ./samples/cycle_A2A_id012_iter012K.wav shape: (59200,)\n",
      "Epoch[13]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id013_iter013K.wav shape: (39040,)oss: 12.28090858\n",
      "saved file at ./samples/cycle_A2A_id013_iter013K.wav shape: (39040,)\n",
      "Epoch[14]: Input data sampled from 133 A and 115 B audio files: train_data_A (500, 24, 128) train_data_B (500, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id014_iter014K.wav shape: (30400,)oss: 13.88825035\n",
      "saved file at ./samples/cycle_A2A_id014_iter014K.wav shape: (30400,)\n",
      "Epoch[15]: Input data sampled from 133 A and 115 B audio files: train_data_A (500, 24, 128) train_data_B (500, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id015_iter015K.wav shape: (138240,)ss: 13.02771473\n",
      "saved file at ./samples/cycle_A2A_id015_iter015K.wav shape: (138240,)\n",
      "Epoch[16]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id016_iter016K.wav shape: (59200,)oss: 12.60383034\n",
      "saved file at ./samples/cycle_A2A_id016_iter016K.wav shape: (59200,)\n",
      "Epoch[17]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id017_iter017K.wav shape: (32960,)oss: 13.24415588\n",
      "saved file at ./samples/cycle_A2A_id017_iter017K.wav shape: (32960,)\n",
      "Epoch[18]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id018_iter018K.wav shape: (117440,)ss: 13.27175808\n",
      "saved file at ./samples/cycle_A2A_id018_iter018K.wav shape: (117440,)\n",
      "Epoch[19]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id019_iter019K.wav shape: (37440,)oss: 12.20719814\n",
      "saved file at ./samples/cycle_A2A_id019_iter019K.wav shape: (37440,)\n",
      "Epoch[20]: Input data sampled from 133 A and 115 B audio files: train_data_A (488, 24, 128) train_data_B (488, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id020_iter020K.wav shape: (34560,)oss: 12.78025341\n",
      "saved file at ./samples/cycle_A2A_id020_iter020K.wav shape: (34560,)\n",
      "Epoch[21]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id021_iter021K.wav shape: (34240,)oss: 12.11821175\n",
      "saved file at ./samples/cycle_A2A_id021_iter021K.wav shape: (34240,)\n",
      "Epoch[22]: Input data sampled from 133 A and 115 B audio files: train_data_A (501, 24, 128) train_data_B (501, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id022_iter022K.wav shape: (23040,)oss: 12.08212662\n",
      "saved file at ./samples/cycle_A2A_id022_iter022K.wav shape: (23040,)\n",
      "Epoch[23]: Input data sampled from 133 A and 115 B audio files: train_data_A (501, 24, 128) train_data_B (501, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id023_iter023K.wav shape: (36800,)oss: 12.82049179\n",
      "saved file at ./samples/cycle_A2A_id023_iter023K.wav shape: (36800,)\n",
      "Epoch[24]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id024_iter024K.wav shape: (93440,)oss: 12.84952068\n",
      "saved file at ./samples/cycle_A2A_id024_iter024K.wav shape: (93440,)\n",
      "Epoch[25]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id025_iter025K.wav shape: (95360,)oss: 11.60479927\n",
      "saved file at ./samples/cycle_A2A_id025_iter025K.wav shape: (95360,)\n",
      "Epoch[26]: Input data sampled from 133 A and 115 B audio files: train_data_A (504, 24, 128) train_data_B (504, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id026_iter026K.wav shape: (82240,)oss: 11.81621075\n",
      "saved file at ./samples/cycle_A2A_id026_iter026K.wav shape: (82240,)\n",
      "Epoch[27]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id027_iter027K.wav shape: (49280,)oss: 11.64582729\n",
      "saved file at ./samples/cycle_A2A_id027_iter027K.wav shape: (49280,)\n",
      "Epoch[28]: Input data sampled from 133 A and 115 B audio files: train_data_A (502, 24, 128) train_data_B (502, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id028_iter028K.wav shape: (43840,)oss: 11.67549133\n",
      "saved file at ./samples/cycle_A2A_id028_iter028K.wav shape: (43840,)\n",
      "Epoch[29]: Input data sampled from 133 A and 115 B audio files: train_data_A (503, 24, 128) train_data_B (503, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id029_iter029K.wav shape: (44160,)oss: 10.42378044\n",
      "saved file at ./samples/cycle_A2A_id029_iter029K.wav shape: (44160,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[30]: Input data sampled from 133 A and 115 B audio files: train_data_A (491, 24, 128) train_data_B (491, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id030_iter030K.wav shape: (108160,)ss: 10.89126968\n",
      "saved file at ./samples/cycle_A2A_id030_iter030K.wav shape: (108160,)\n",
      "Epoch[31]: Input data sampled from 133 A and 115 B audio files: train_data_A (488, 24, 128) train_data_B (488, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id031_iter031K.wav shape: (73600,)oss: 10.55626297\n",
      "saved file at ./samples/cycle_A2A_id031_iter031K.wav shape: (73600,)\n",
      "Epoch[32]: Input data sampled from 133 A and 115 B audio files: train_data_A (504, 24, 128) train_data_B (504, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id032_iter032K.wav shape: (47680,)oss: 11.50260162\n",
      "saved file at ./samples/cycle_A2A_id032_iter032K.wav shape: (47680,)\n",
      "Epoch[33]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id000_iter033K.wav shape: (142080,)ss: 10.48569679\n",
      "saved file at ./samples/cycle_A2A_id000_iter033K.wav shape: (142080,)\n",
      "Epoch[34]: Input data sampled from 133 A and 115 B audio files: train_data_A (484, 24, 128) train_data_B (484, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id001_iter034K.wav shape: (65280,)oss: 10.42580795\n",
      "saved file at ./samples/cycle_A2A_id001_iter034K.wav shape: (65280,)\n",
      "Epoch[35]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id002_iter035K.wav shape: (23360,)oss: 10.16975307\n",
      "saved file at ./samples/cycle_A2A_id002_iter035K.wav shape: (23360,)\n",
      "Epoch[36]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id003_iter036K.wav shape: (94720,)oss: 10.64789104\n",
      "saved file at ./samples/cycle_A2A_id003_iter036K.wav shape: (94720,)\n",
      "Epoch[37]: Input data sampled from 133 A and 115 B audio files: train_data_A (503, 24, 128) train_data_B (503, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id004_iter037K.wav shape: (30400,)oss: 11.49290276\n",
      "saved file at ./samples/cycle_A2A_id004_iter037K.wav shape: (30400,)\n",
      "Epoch[38]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id005_iter038K.wav shape: (136000,)ss: 9.754440318\n",
      "saved file at ./samples/cycle_A2A_id005_iter038K.wav shape: (136000,)\n",
      "Epoch[39]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id006_iter039K.wav shape: (93440,)oss: 10.12092113\n",
      "saved file at ./samples/cycle_A2A_id006_iter039K.wav shape: (93440,)\n",
      "Epoch[40]: Input data sampled from 133 A and 115 B audio files: train_data_A (499, 24, 128) train_data_B (499, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id007_iter040K.wav shape: (18240,)oss: 10.06640530\n",
      "saved file at ./samples/cycle_A2A_id007_iter040K.wav shape: (18240,)\n",
      "Epoch[41]: Input data sampled from 133 A and 115 B audio files: train_data_A (497, 24, 128) train_data_B (497, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id008_iter041K.wav shape: (31040,)oss: 9.516477581\n",
      "saved file at ./samples/cycle_A2A_id008_iter041K.wav shape: (31040,)\n",
      "Epoch[42]: Input data sampled from 133 A and 115 B audio files: train_data_A (502, 24, 128) train_data_B (502, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id009_iter042K.wav shape: (83200,)oss: 10.64991188\n",
      "saved file at ./samples/cycle_A2A_id009_iter042K.wav shape: (83200,)\n",
      "Epoch[43]: Input data sampled from 133 A and 115 B audio files: train_data_A (504, 24, 128) train_data_B (504, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id010_iter043K.wav shape: (39360,)oss: 9.037676818\n",
      "saved file at ./samples/cycle_A2A_id010_iter043K.wav shape: (39360,)\n",
      "Epoch[44]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id011_iter044K.wav shape: (27520,)oss: 11.00720406\n",
      "saved file at ./samples/cycle_A2A_id011_iter044K.wav shape: (27520,)\n",
      "Epoch[45]: Input data sampled from 133 A and 115 B audio files: train_data_A (502, 24, 128) train_data_B (502, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id012_iter045K.wav shape: (59200,)oss: 10.31624889\n",
      "saved file at ./samples/cycle_A2A_id012_iter045K.wav shape: (59200,)\n",
      "Epoch[46]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id013_iter046K.wav shape: (39040,)oss: 9.892375951\n",
      "saved file at ./samples/cycle_A2A_id013_iter046K.wav shape: (39040,)\n",
      "Epoch[47]: Input data sampled from 133 A and 115 B audio files: train_data_A (493, 24, 128) train_data_B (493, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id014_iter047K.wav shape: (30400,)loss: 10.27256966\n",
      "saved file at ./samples/cycle_A2A_id014_iter047K.wav shape: (30400,)\n",
      "Epoch[48]: Input data sampled from 133 A and 115 B audio files: train_data_A (491, 24, 128) train_data_B (491, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id015_iter048K.wav shape: (138240,)oss: 9.947286616\n",
      "saved file at ./samples/cycle_A2A_id015_iter048K.wav shape: (138240,)\n",
      "Epoch[49]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id016_iter049K.wav shape: (59200,)loss: 9.621824261\n",
      "saved file at ./samples/cycle_A2A_id016_iter049K.wav shape: (59200,)\n",
      "Epoch[50]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id017_iter050K.wav shape: (32960,)loss: 10.44871902\n",
      "saved file at ./samples/cycle_A2A_id017_iter050K.wav shape: (32960,)\n",
      "Epoch[51]: Input data sampled from 133 A and 115 B audio files: train_data_A (506, 24, 128) train_data_B (506, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id018_iter051K.wav shape: (117440,)oss: 9.181879044\n",
      "saved file at ./samples/cycle_A2A_id018_iter051K.wav shape: (117440,)\n",
      "Epoch[52]: Input data sampled from 133 A and 115 B audio files: train_data_A (497, 24, 128) train_data_B (497, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id019_iter052K.wav shape: (37440,)loss: 9.450776107\n",
      "saved file at ./samples/cycle_A2A_id019_iter052K.wav shape: (37440,)\n",
      "Epoch[53]: Input data sampled from 133 A and 115 B audio files: train_data_A (486, 24, 128) train_data_B (486, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id020_iter053K.wav shape: (34560,)loss: 8.707918171\n",
      "saved file at ./samples/cycle_A2A_id020_iter053K.wav shape: (34560,)\n",
      "Epoch[54]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id021_iter054K.wav shape: (34240,)loss: 10.32714653\n",
      "saved file at ./samples/cycle_A2A_id021_iter054K.wav shape: (34240,)\n",
      "Epoch[55]: Input data sampled from 133 A and 115 B audio files: train_data_A (501, 24, 128) train_data_B (501, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id022_iter055K.wav shape: (23040,)loss: 10.67341709\n",
      "saved file at ./samples/cycle_A2A_id022_iter055K.wav shape: (23040,)\n",
      "Epoch[56]: Input data sampled from 133 A and 115 B audio files: train_data_A (499, 24, 128) train_data_B (499, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id023_iter056K.wav shape: (36800,)loss: 10.69507217\n",
      "saved file at ./samples/cycle_A2A_id023_iter056K.wav shape: (36800,)\n",
      "Epoch[57]: Input data sampled from 133 A and 115 B audio files: train_data_A (486, 24, 128) train_data_B (486, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id024_iter057K.wav shape: (93440,)loss: 10.27133369\n",
      "saved file at ./samples/cycle_A2A_id024_iter057K.wav shape: (93440,)\n",
      "Epoch[58]: Input data sampled from 133 A and 115 B audio files: train_data_A (487, 24, 128) train_data_B (487, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id025_iter058K.wav shape: (95360,)loss: 10.21483707\n",
      "saved file at ./samples/cycle_A2A_id025_iter058K.wav shape: (95360,)\n",
      "Epoch[59]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id026_iter059K.wav shape: (82240,)loss: 8.064702035\n",
      "saved file at ./samples/cycle_A2A_id026_iter059K.wav shape: (82240,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[60]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id027_iter060K.wav shape: (49280,)loss: 9.378467569\n",
      "saved file at ./samples/cycle_A2A_id027_iter060K.wav shape: (49280,)\n",
      "Epoch[61]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id028_iter061K.wav shape: (43840,)loss: 9.997386931\n",
      "saved file at ./samples/cycle_A2A_id028_iter061K.wav shape: (43840,)\n",
      "Epoch[62]: Input data sampled from 133 A and 115 B audio files: train_data_A (493, 24, 128) train_data_B (493, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id029_iter062K.wav shape: (44160,)loss: 8.429368975\n",
      "saved file at ./samples/cycle_A2A_id029_iter062K.wav shape: (44160,)\n",
      "Epoch[63]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id030_iter063K.wav shape: (108160,)oss: 9.541911135\n",
      "saved file at ./samples/cycle_A2A_id030_iter063K.wav shape: (108160,)\n",
      "Epoch[64]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id031_iter064K.wav shape: (73600,)loss: 9.673345572\n",
      "saved file at ./samples/cycle_A2A_id031_iter064K.wav shape: (73600,)\n",
      "Epoch[65]: Input data sampled from 133 A and 115 B audio files: train_data_A (491, 24, 128) train_data_B (491, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id032_iter065K.wav shape: (47680,)loss: 9.816300393\n",
      "saved file at ./samples/cycle_A2A_id032_iter065K.wav shape: (47680,)\n",
      "Epoch[66]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id000_iter066K.wav shape: (142080,)oss: 9.235726365\n",
      "saved file at ./samples/cycle_A2A_id000_iter066K.wav shape: (142080,)\n",
      "Epoch[67]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id001_iter067K.wav shape: (65280,)loss: 9.472726828\n",
      "saved file at ./samples/cycle_A2A_id001_iter067K.wav shape: (65280,)\n",
      "Epoch[68]: Input data sampled from 133 A and 115 B audio files: train_data_A (493, 24, 128) train_data_B (493, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id002_iter068K.wav shape: (23360,)loss: 9.044740681\n",
      "saved file at ./samples/cycle_A2A_id002_iter068K.wav shape: (23360,)\n",
      "Epoch[69]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id003_iter069K.wav shape: (94720,)loss: 10.23695564\n",
      "saved file at ./samples/cycle_A2A_id003_iter069K.wav shape: (94720,)\n",
      "Epoch[70]: Input data sampled from 133 A and 115 B audio files: train_data_A (489, 24, 128) train_data_B (489, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id004_iter070K.wav shape: (30400,)loss: 8.110751157\n",
      "saved file at ./samples/cycle_A2A_id004_iter070K.wav shape: (30400,)\n",
      "Epoch[71]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id005_iter071K.wav shape: (136000,)oss: 8.273965843\n",
      "saved file at ./samples/cycle_A2A_id005_iter071K.wav shape: (136000,)\n",
      "Epoch[72]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id006_iter072K.wav shape: (93440,)loss: 9.440748213\n",
      "saved file at ./samples/cycle_A2A_id006_iter072K.wav shape: (93440,)\n",
      "Epoch[73]: Input data sampled from 133 A and 115 B audio files: train_data_A (484, 24, 128) train_data_B (484, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id007_iter073K.wav shape: (18240,)loss: 8.305187233\n",
      "saved file at ./samples/cycle_A2A_id007_iter073K.wav shape: (18240,)\n",
      "Epoch[74]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id008_iter074K.wav shape: (31040,)loss: 8.191565510\n",
      "saved file at ./samples/cycle_A2A_id008_iter074K.wav shape: (31040,)\n",
      "Epoch[75]: Input data sampled from 133 A and 115 B audio files: train_data_A (502, 24, 128) train_data_B (502, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id009_iter075K.wav shape: (83200,)loss: 10.74055576\n",
      "saved file at ./samples/cycle_A2A_id009_iter075K.wav shape: (83200,)\n",
      "Epoch[76]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id010_iter076K.wav shape: (39360,)loss: 8.415634164\n",
      "saved file at ./samples/cycle_A2A_id010_iter076K.wav shape: (39360,)\n",
      "Epoch[77]: Input data sampled from 133 A and 115 B audio files: train_data_A (482, 24, 128) train_data_B (482, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id011_iter077K.wav shape: (27520,)loss: 9.175460823\n",
      "saved file at ./samples/cycle_A2A_id011_iter077K.wav shape: (27520,)\n",
      "Epoch[78]: Input data sampled from 133 A and 115 B audio files: train_data_A (489, 24, 128) train_data_B (489, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id012_iter078K.wav shape: (59200,)loss: 8.691411979\n",
      "saved file at ./samples/cycle_A2A_id012_iter078K.wav shape: (59200,)\n",
      "Epoch[79]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id013_iter079K.wav shape: (39040,)loss: 7.468362330\n",
      "saved file at ./samples/cycle_A2A_id013_iter079K.wav shape: (39040,)\n",
      "Epoch[80]: Input data sampled from 133 A and 115 B audio files: train_data_A (489, 24, 128) train_data_B (489, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id014_iter080K.wav shape: (30400,)loss: 8.850666053\n",
      "saved file at ./samples/cycle_A2A_id014_iter080K.wav shape: (30400,)\n",
      "Epoch[81]: Input data sampled from 133 A and 115 B audio files: train_data_A (488, 24, 128) train_data_B (488, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id015_iter081K.wav shape: (138240,)oss: 8.798624041\n",
      "saved file at ./samples/cycle_A2A_id015_iter081K.wav shape: (138240,)\n",
      "Epoch[82]: Input data sampled from 133 A and 115 B audio files: train_data_A (505, 24, 128) train_data_B (505, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id016_iter082K.wav shape: (59200,)loss: 9.565835956\n",
      "saved file at ./samples/cycle_A2A_id016_iter082K.wav shape: (59200,)\n",
      "Epoch[83]: Input data sampled from 133 A and 115 B audio files: train_data_A (485, 24, 128) train_data_B (485, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id017_iter083K.wav shape: (32960,)loss: 8.933526047\n",
      "saved file at ./samples/cycle_A2A_id017_iter083K.wav shape: (32960,)\n",
      "Epoch[84]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id018_iter084K.wav shape: (117440,)oss: 7.750299457\n",
      "saved file at ./samples/cycle_A2A_id018_iter084K.wav shape: (117440,)\n",
      "Epoch[85]: Input data sampled from 133 A and 115 B audio files: train_data_A (497, 24, 128) train_data_B (497, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id019_iter085K.wav shape: (37440,)loss: 8.700208666\n",
      "saved file at ./samples/cycle_A2A_id019_iter085K.wav shape: (37440,)\n",
      "Epoch[86]: Input data sampled from 133 A and 115 B audio files: train_data_A (502, 24, 128) train_data_B (502, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id020_iter086K.wav shape: (34560,)loss: 8.136560446\n",
      "saved file at ./samples/cycle_A2A_id020_iter086K.wav shape: (34560,)\n",
      "Epoch[87]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id021_iter087K.wav shape: (34240,)loss: 7.986125473\n",
      "saved file at ./samples/cycle_A2A_id021_iter087K.wav shape: (34240,)\n",
      "Epoch[88]: Input data sampled from 133 A and 115 B audio files: train_data_A (499, 24, 128) train_data_B (499, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id022_iter088K.wav shape: (23040,)loss: 9.431053167\n",
      "saved file at ./samples/cycle_A2A_id022_iter088K.wav shape: (23040,)\n",
      "Epoch[89]: Input data sampled from 133 A and 115 B audio files: train_data_A (499, 24, 128) train_data_B (499, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id023_iter089K.wav shape: (36800,)loss: 8.355486874\n",
      "saved file at ./samples/cycle_A2A_id023_iter089K.wav shape: (36800,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[90]: Input data sampled from 133 A and 115 B audio files: train_data_A (504, 24, 128) train_data_B (504, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id024_iter090K.wav shape: (93440,)loss: 8.442532543\n",
      "saved file at ./samples/cycle_A2A_id024_iter090K.wav shape: (93440,)\n",
      "Epoch[91]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id025_iter091K.wav shape: (95360,)loss: 8.222020154\n",
      "saved file at ./samples/cycle_A2A_id025_iter091K.wav shape: (95360,)\n",
      "Epoch[92]: Input data sampled from 133 A and 115 B audio files: train_data_A (491, 24, 128) train_data_B (491, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id026_iter092K.wav shape: (82240,)loss: 8.193159101\n",
      "saved file at ./samples/cycle_A2A_id026_iter092K.wav shape: (82240,)\n",
      "Epoch[93]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id027_iter093K.wav shape: (49280,)loss: 8.827251438\n",
      "saved file at ./samples/cycle_A2A_id027_iter093K.wav shape: (49280,)\n",
      "Epoch[94]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id028_iter094K.wav shape: (43840,)loss: 8.137577065\n",
      "saved file at ./samples/cycle_A2A_id028_iter094K.wav shape: (43840,)\n",
      "Epoch[95]: Input data sampled from 133 A and 115 B audio files: train_data_A (489, 24, 128) train_data_B (489, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id029_iter095K.wav shape: (44160,)loss: 8.213490490\n",
      "saved file at ./samples/cycle_A2A_id029_iter095K.wav shape: (44160,)\n",
      "Epoch[96]: Input data sampled from 133 A and 115 B audio files: train_data_A (485, 24, 128) train_data_B (485, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id030_iter096K.wav shape: (108160,)oss: 8.421983727\n",
      "saved file at ./samples/cycle_A2A_id030_iter096K.wav shape: (108160,)\n",
      "Epoch[97]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id031_iter097K.wav shape: (73600,)loss: 7.646758563\n",
      "saved file at ./samples/cycle_A2A_id031_iter097K.wav shape: (73600,)\n",
      "Epoch[98]: Input data sampled from 133 A and 115 B audio files: train_data_A (481, 24, 128) train_data_B (481, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id032_iter098K.wav shape: (47680,)loss: 8.452005393\n",
      "saved file at ./samples/cycle_A2A_id032_iter098K.wav shape: (47680,)\n",
      "Epoch[99]: Input data sampled from 133 A and 115 B audio files: train_data_A (493, 24, 128) train_data_B (493, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id000_iter099K.wav shape: (142080,)oss: 8.705992701\n",
      "saved file at ./samples/cycle_A2A_id000_iter099K.wav shape: (142080,)\n",
      "Epoch[100]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id001_iter100K.wav shape: (65280,)_loss: 7.826891420\n",
      "saved file at ./samples/cycle_A2A_id001_iter100K.wav shape: (65280,)\n",
      "Epoch[101]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id002_iter101K.wav shape: (23360,)_loss: 8.074192052\n",
      "saved file at ./samples/cycle_A2A_id002_iter101K.wav shape: (23360,)\n",
      "Epoch[102]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id003_iter102K.wav shape: (94720,)_loss: 8.09940434\n",
      "saved file at ./samples/cycle_A2A_id003_iter102K.wav shape: (94720,)\n",
      "Epoch[103]: Input data sampled from 133 A and 115 B audio files: train_data_A (499, 24, 128) train_data_B (499, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id004_iter103K.wav shape: (30400,)_loss: 7.710180284\n",
      "saved file at ./samples/cycle_A2A_id004_iter103K.wav shape: (30400,)\n",
      "Epoch[104]: Input data sampled from 133 A and 115 B audio files: train_data_A (493, 24, 128) train_data_B (493, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id005_iter104K.wav shape: (136000,)loss: 7.678742410\n",
      "saved file at ./samples/cycle_A2A_id005_iter104K.wav shape: (136000,)\n",
      "Epoch[105]: Input data sampled from 133 A and 115 B audio files: train_data_A (482, 24, 128) train_data_B (482, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id006_iter105K.wav shape: (93440,)_loss: 7.939992901\n",
      "saved file at ./samples/cycle_A2A_id006_iter105K.wav shape: (93440,)\n",
      "Epoch[106]: Input data sampled from 133 A and 115 B audio files: train_data_A (491, 24, 128) train_data_B (491, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id007_iter106K.wav shape: (18240,)_loss: 7.621449958\n",
      "saved file at ./samples/cycle_A2A_id007_iter106K.wav shape: (18240,)\n",
      "Epoch[107]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id008_iter107K.wav shape: (31040,)_loss: 7.50817394\n",
      "saved file at ./samples/cycle_A2A_id008_iter107K.wav shape: (31040,)\n",
      "Epoch[108]: Input data sampled from 133 A and 115 B audio files: train_data_A (491, 24, 128) train_data_B (491, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id009_iter108K.wav shape: (83200,)_loss: 7.63649845\n",
      "saved file at ./samples/cycle_A2A_id009_iter108K.wav shape: (83200,)\n",
      "Epoch[109]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id010_iter109K.wav shape: (39360,)_loss: 7.16780853\n",
      "saved file at ./samples/cycle_A2A_id010_iter109K.wav shape: (39360,)\n",
      "Epoch[110]: Input data sampled from 133 A and 115 B audio files: train_data_A (504, 24, 128) train_data_B (504, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id011_iter110K.wav shape: (27520,)_loss: 8.846537593\n",
      "saved file at ./samples/cycle_A2A_id011_iter110K.wav shape: (27520,)\n",
      "Epoch[111]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id012_iter111K.wav shape: (59200,)_loss: 7.49705029\n",
      "saved file at ./samples/cycle_A2A_id012_iter111K.wav shape: (59200,)\n",
      "Epoch[112]: Input data sampled from 133 A and 115 B audio files: train_data_A (486, 24, 128) train_data_B (486, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id013_iter112K.wav shape: (39040,)_loss: 7.50058079\n",
      "saved file at ./samples/cycle_A2A_id013_iter112K.wav shape: (39040,)\n",
      "Epoch[113]: Input data sampled from 133 A and 115 B audio files: train_data_A (505, 24, 128) train_data_B (505, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id014_iter113K.wav shape: (30400,)_loss: 8.23259163\n",
      "saved file at ./samples/cycle_A2A_id014_iter113K.wav shape: (30400,)\n",
      "Epoch[114]: Input data sampled from 133 A and 115 B audio files: train_data_A (487, 24, 128) train_data_B (487, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id015_iter114K.wav shape: (138240,)loss: 7.26435518\n",
      "saved file at ./samples/cycle_A2A_id015_iter114K.wav shape: (138240,)\n",
      "Epoch[115]: Input data sampled from 133 A and 115 B audio files: train_data_A (486, 24, 128) train_data_B (486, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id016_iter115K.wav shape: (59200,)_loss: 7.29170322\n",
      "saved file at ./samples/cycle_A2A_id016_iter115K.wav shape: (59200,)\n",
      "Epoch[116]: Input data sampled from 133 A and 115 B audio files: train_data_A (501, 24, 128) train_data_B (501, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id017_iter116K.wav shape: (32960,)_loss: 7.71550655\n",
      "saved file at ./samples/cycle_A2A_id017_iter116K.wav shape: (32960,)\n",
      "Epoch[117]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id018_iter117K.wav shape: (117440,)loss: 7.78849792\n",
      "saved file at ./samples/cycle_A2A_id018_iter117K.wav shape: (117440,)\n",
      "Epoch[118]: Input data sampled from 133 A and 115 B audio files: train_data_A (502, 24, 128) train_data_B (502, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id019_iter118K.wav shape: (37440,)_loss: 7.73426819\n",
      "saved file at ./samples/cycle_A2A_id019_iter118K.wav shape: (37440,)\n",
      "Epoch[119]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id020_iter119K.wav shape: (34560,)_loss: 7.18790627\n",
      "saved file at ./samples/cycle_A2A_id020_iter119K.wav shape: (34560,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[120]: Input data sampled from 133 A and 115 B audio files: train_data_A (493, 24, 128) train_data_B (493, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id021_iter120K.wav shape: (34240,)_loss: 7.15630722\n",
      "saved file at ./samples/cycle_A2A_id021_iter120K.wav shape: (34240,)\n",
      "Epoch[121]: Input data sampled from 133 A and 115 B audio files: train_data_A (500, 24, 128) train_data_B (500, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id022_iter121K.wav shape: (23040,)_loss: 6.59470797\n",
      "saved file at ./samples/cycle_A2A_id022_iter121K.wav shape: (23040,)\n",
      "Epoch[122]: Input data sampled from 133 A and 115 B audio files: train_data_A (485, 24, 128) train_data_B (485, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id023_iter122K.wav shape: (36800,)_loss: 7.70025253\n",
      "saved file at ./samples/cycle_A2A_id023_iter122K.wav shape: (36800,)\n",
      "Epoch[123]: Input data sampled from 133 A and 115 B audio files: train_data_A (493, 24, 128) train_data_B (493, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id024_iter123K.wav shape: (93440,)_loss: 7.74449015\n",
      "saved file at ./samples/cycle_A2A_id024_iter123K.wav shape: (93440,)\n",
      "Epoch[124]: Input data sampled from 133 A and 115 B audio files: train_data_A (493, 24, 128) train_data_B (493, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id025_iter124K.wav shape: (95360,)_loss: 7.28345394\n",
      "saved file at ./samples/cycle_A2A_id025_iter124K.wav shape: (95360,)\n",
      "Epoch[125]: Input data sampled from 133 A and 115 B audio files: train_data_A (497, 24, 128) train_data_B (497, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id026_iter125K.wav shape: (82240,)_loss: 7.19661570\n",
      "saved file at ./samples/cycle_A2A_id026_iter125K.wav shape: (82240,)\n",
      "Epoch[126]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id027_iter126K.wav shape: (49280,)_loss: 6.92717743\n",
      "saved file at ./samples/cycle_A2A_id027_iter126K.wav shape: (49280,)\n",
      "Epoch[127]: Input data sampled from 133 A and 115 B audio files: train_data_A (489, 24, 128) train_data_B (489, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id028_iter127K.wav shape: (43840,)_loss: 7.74546576\n",
      "saved file at ./samples/cycle_A2A_id028_iter127K.wav shape: (43840,)\n",
      "Epoch[128]: Input data sampled from 133 A and 115 B audio files: train_data_A (481, 24, 128) train_data_B (481, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id029_iter128K.wav shape: (44160,)_loss: 7.30388594\n",
      "saved file at ./samples/cycle_A2A_id029_iter128K.wav shape: (44160,)\n",
      "Epoch[129]: Input data sampled from 133 A and 115 B audio files: train_data_A (497, 24, 128) train_data_B (497, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id030_iter129K.wav shape: (108160,)loss: 7.24419022\n",
      "saved file at ./samples/cycle_A2A_id030_iter129K.wav shape: (108160,)\n",
      "Epoch[130]: Input data sampled from 133 A and 115 B audio files: train_data_A (497, 24, 128) train_data_B (497, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id031_iter130K.wav shape: (73600,)_loss: 7.24455166\n",
      "saved file at ./samples/cycle_A2A_id031_iter130K.wav shape: (73600,)\n",
      "Epoch[131]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id032_iter131K.wav shape: (47680,)_loss: 7.11397648\n",
      "saved file at ./samples/cycle_A2A_id032_iter131K.wav shape: (47680,)\n",
      "Epoch[132]: Input data sampled from 133 A and 115 B audio files: train_data_A (482, 24, 128) train_data_B (482, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id000_iter132K.wav shape: (142080,)loss: 6.83348370\n",
      "saved file at ./samples/cycle_A2A_id000_iter132K.wav shape: (142080,)\n",
      "Epoch[133]: Input data sampled from 133 A and 115 B audio files: train_data_A (488, 24, 128) train_data_B (488, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id001_iter133K.wav shape: (65280,)_loss: 6.44190311\n",
      "saved file at ./samples/cycle_A2A_id001_iter133K.wav shape: (65280,)\n",
      "Epoch[134]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id002_iter134K.wav shape: (23360,)_loss: 6.98915863\n",
      "saved file at ./samples/cycle_A2A_id002_iter134K.wav shape: (23360,)\n",
      "Epoch[135]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id003_iter135K.wav shape: (94720,)_loss: 6.61661339\n",
      "saved file at ./samples/cycle_A2A_id003_iter135K.wav shape: (94720,)\n",
      "Epoch[136]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id004_iter136K.wav shape: (30400,)_loss: 6.85709715\n",
      "saved file at ./samples/cycle_A2A_id004_iter136K.wav shape: (30400,)\n",
      "Epoch[137]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id005_iter137K.wav shape: (136000,)loss: 6.89821625\n",
      "saved file at ./samples/cycle_A2A_id005_iter137K.wav shape: (136000,)\n",
      "Epoch[138]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id006_iter138K.wav shape: (93440,)_loss: 6.95082474\n",
      "saved file at ./samples/cycle_A2A_id006_iter138K.wav shape: (93440,)\n",
      "Epoch[139]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id007_iter139K.wav shape: (18240,)_loss: 6.90384340\n",
      "saved file at ./samples/cycle_A2A_id007_iter139K.wav shape: (18240,)\n",
      "Epoch[140]: Input data sampled from 133 A and 115 B audio files: train_data_A (497, 24, 128) train_data_B (497, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id008_iter140K.wav shape: (31040,)_loss: 6.90331316\n",
      "saved file at ./samples/cycle_A2A_id008_iter140K.wav shape: (31040,)\n",
      "Epoch[141]: Input data sampled from 133 A and 115 B audio files: train_data_A (502, 24, 128) train_data_B (502, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id009_iter141K.wav shape: (83200,)_loss: 7.20210075\n",
      "saved file at ./samples/cycle_A2A_id009_iter141K.wav shape: (83200,)\n",
      "Epoch[142]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id010_iter142K.wav shape: (39360,)_loss: 6.73780107\n",
      "saved file at ./samples/cycle_A2A_id010_iter142K.wav shape: (39360,)\n",
      "Epoch[143]: Input data sampled from 133 A and 115 B audio files: train_data_A (491, 24, 128) train_data_B (491, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id011_iter143K.wav shape: (27520,)_loss: 6.48972082\n",
      "saved file at ./samples/cycle_A2A_id011_iter143K.wav shape: (27520,)\n",
      "Epoch[144]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id012_iter144K.wav shape: (59200,)_loss: 6.30098152\n",
      "saved file at ./samples/cycle_A2A_id012_iter144K.wav shape: (59200,)\n",
      "Epoch[145]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id013_iter145K.wav shape: (39040,)_loss: 6.41842556\n",
      "saved file at ./samples/cycle_A2A_id013_iter145K.wav shape: (39040,)\n",
      "Epoch[146]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id014_iter146K.wav shape: (30400,)_loss: 6.11732006\n",
      "saved file at ./samples/cycle_A2A_id014_iter146K.wav shape: (30400,)\n",
      "Epoch[147]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id015_iter147K.wav shape: (138240,)loss: 6.30153656\n",
      "saved file at ./samples/cycle_A2A_id015_iter147K.wav shape: (138240,)\n",
      "Epoch[148]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id016_iter148K.wav shape: (59200,)_loss: 6.29845333\n",
      "saved file at ./samples/cycle_A2A_id016_iter148K.wav shape: (59200,)\n",
      "Epoch[149]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id017_iter149K.wav shape: (32960,)_loss: 7.22825098\n",
      "saved file at ./samples/cycle_A2A_id017_iter149K.wav shape: (32960,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[150]: Input data sampled from 133 A and 115 B audio files: train_data_A (501, 24, 128) train_data_B (501, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id018_iter150K.wav shape: (117440,)loss: 6.84852791\n",
      "saved file at ./samples/cycle_A2A_id018_iter150K.wav shape: (117440,)\n",
      "Epoch[151]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id019_iter151K.wav shape: (37440,)_loss: 6.04549360\n",
      "saved file at ./samples/cycle_A2A_id019_iter151K.wav shape: (37440,)\n",
      "Epoch[152]: Input data sampled from 133 A and 115 B audio files: train_data_A (488, 24, 128) train_data_B (488, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id020_iter152K.wav shape: (34560,)_loss: 6.50373459\n",
      "saved file at ./samples/cycle_A2A_id020_iter152K.wav shape: (34560,)\n",
      "Epoch[153]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id021_iter153K.wav shape: (34240,)_loss: 6.65391111\n",
      "saved file at ./samples/cycle_A2A_id021_iter153K.wav shape: (34240,)\n",
      "Epoch[154]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id022_iter154K.wav shape: (23040,)_loss: 6.32181311\n",
      "saved file at ./samples/cycle_A2A_id022_iter154K.wav shape: (23040,)\n",
      "Epoch[155]: Input data sampled from 133 A and 115 B audio files: train_data_A (491, 24, 128) train_data_B (491, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id023_iter155K.wav shape: (36800,)_loss: 6.58423424\n",
      "saved file at ./samples/cycle_A2A_id023_iter155K.wav shape: (36800,)\n",
      "Epoch[156]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id024_iter156K.wav shape: (93440,)_loss: 6.21490860\n",
      "saved file at ./samples/cycle_A2A_id024_iter156K.wav shape: (93440,)\n",
      "Epoch[157]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id025_iter157K.wav shape: (95360,)_loss: 6.39519596\n",
      "saved file at ./samples/cycle_A2A_id025_iter157K.wav shape: (95360,)\n",
      "Epoch[158]: Input data sampled from 133 A and 115 B audio files: train_data_A (497, 24, 128) train_data_B (497, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id026_iter158K.wav shape: (82240,)_loss: 6.76802444\n",
      "saved file at ./samples/cycle_A2A_id026_iter158K.wav shape: (82240,)\n",
      "Epoch[159]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id027_iter159K.wav shape: (49280,)_loss: 5.78931952\n",
      "saved file at ./samples/cycle_A2A_id027_iter159K.wav shape: (49280,)\n",
      "Epoch[160]: Input data sampled from 133 A and 115 B audio files: train_data_A (489, 24, 128) train_data_B (489, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id028_iter160K.wav shape: (43840,)_loss: 6.41674948\n",
      "saved file at ./samples/cycle_A2A_id028_iter160K.wav shape: (43840,)\n",
      "Epoch[161]: Input data sampled from 133 A and 115 B audio files: train_data_A (502, 24, 128) train_data_B (502, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id029_iter161K.wav shape: (44160,)_loss: 6.54849720\n",
      "saved file at ./samples/cycle_A2A_id029_iter161K.wav shape: (44160,)\n",
      "Epoch[162]: Input data sampled from 133 A and 115 B audio files: train_data_A (501, 24, 128) train_data_B (501, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id030_iter162K.wav shape: (108160,)loss: 6.42673016\n",
      "saved file at ./samples/cycle_A2A_id030_iter162K.wav shape: (108160,)\n",
      "Epoch[163]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id031_iter163K.wav shape: (73600,)_loss: 6.21640444\n",
      "saved file at ./samples/cycle_A2A_id031_iter163K.wav shape: (73600,)\n",
      "Epoch[164]: Input data sampled from 133 A and 115 B audio files: train_data_A (501, 24, 128) train_data_B (501, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id032_iter164K.wav shape: (47680,)_loss: 6.26816607\n",
      "saved file at ./samples/cycle_A2A_id032_iter164K.wav shape: (47680,)\n",
      "Epoch[165]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id000_iter165K.wav shape: (142080,)loss: 6.32674503\n",
      "saved file at ./samples/cycle_A2A_id000_iter165K.wav shape: (142080,)\n",
      "Epoch[166]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id001_iter166K.wav shape: (65280,)_loss: 6.18242264\n",
      "saved file at ./samples/cycle_A2A_id001_iter166K.wav shape: (65280,)\n",
      "Epoch[167]: Input data sampled from 133 A and 115 B audio files: train_data_A (500, 24, 128) train_data_B (500, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id002_iter167K.wav shape: (23360,)_loss: 6.55219269\n",
      "saved file at ./samples/cycle_A2A_id002_iter167K.wav shape: (23360,)\n",
      "Epoch[168]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id003_iter168K.wav shape: (94720,)_loss: 6.20789814\n",
      "saved file at ./samples/cycle_A2A_id003_iter168K.wav shape: (94720,)\n",
      "Epoch[169]: Input data sampled from 133 A and 115 B audio files: train_data_A (508, 24, 128) train_data_B (508, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id004_iter169K.wav shape: (30400,)_loss: 6.16165066\n",
      "saved file at ./samples/cycle_A2A_id004_iter169K.wav shape: (30400,)\n",
      "Epoch[170]: Input data sampled from 133 A and 115 B audio files: train_data_A (501, 24, 128) train_data_B (501, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id005_iter170K.wav shape: (136000,)loss: 6.52894211\n",
      "saved file at ./samples/cycle_A2A_id005_iter170K.wav shape: (136000,)\n",
      "Epoch[171]: Input data sampled from 133 A and 115 B audio files: train_data_A (503, 24, 128) train_data_B (503, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id006_iter171K.wav shape: (93440,)_loss: 6.30232859\n",
      "saved file at ./samples/cycle_A2A_id006_iter171K.wav shape: (93440,)\n",
      "Epoch[172]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id007_iter172K.wav shape: (18240,)_loss: 5.90129423\n",
      "saved file at ./samples/cycle_A2A_id007_iter172K.wav shape: (18240,)\n",
      "Epoch[173]: Input data sampled from 133 A and 115 B audio files: train_data_A (493, 24, 128) train_data_B (493, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id008_iter173K.wav shape: (31040,)_loss: 6.42122746\n",
      "saved file at ./samples/cycle_A2A_id008_iter173K.wav shape: (31040,)\n",
      "Epoch[174]: Input data sampled from 133 A and 115 B audio files: train_data_A (505, 24, 128) train_data_B (505, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id009_iter174K.wav shape: (83200,)_loss: 5.92491531\n",
      "saved file at ./samples/cycle_A2A_id009_iter174K.wav shape: (83200,)\n",
      "Epoch[175]: Input data sampled from 133 A and 115 B audio files: train_data_A (501, 24, 128) train_data_B (501, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id010_iter175K.wav shape: (39360,)_loss: 6.18932629\n",
      "saved file at ./samples/cycle_A2A_id010_iter175K.wav shape: (39360,)\n",
      "Epoch[176]: Input data sampled from 133 A and 115 B audio files: train_data_A (504, 24, 128) train_data_B (504, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id011_iter176K.wav shape: (27520,)_loss: 6.45290136\n",
      "saved file at ./samples/cycle_A2A_id011_iter176K.wav shape: (27520,)\n",
      "Epoch[177]: Input data sampled from 133 A and 115 B audio files: train_data_A (503, 24, 128) train_data_B (503, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id012_iter177K.wav shape: (59200,)_loss: 5.89279699\n",
      "saved file at ./samples/cycle_A2A_id012_iter177K.wav shape: (59200,)\n",
      "Epoch[178]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id013_iter178K.wav shape: (39040,)_loss: 6.03312731\n",
      "saved file at ./samples/cycle_A2A_id013_iter178K.wav shape: (39040,)\n",
      "Epoch[179]: Input data sampled from 133 A and 115 B audio files: train_data_A (481, 24, 128) train_data_B (481, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id014_iter179K.wav shape: (30400,)_loss: 6.17874479\n",
      "saved file at ./samples/cycle_A2A_id014_iter179K.wav shape: (30400,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[180]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id015_iter180K.wav shape: (138240,)loss: 6.03697872\n",
      "saved file at ./samples/cycle_A2A_id015_iter180K.wav shape: (138240,)\n",
      "Epoch[181]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id016_iter181K.wav shape: (59200,)_loss: 6.25163364\n",
      "saved file at ./samples/cycle_A2A_id016_iter181K.wav shape: (59200,)\n",
      "Epoch[182]: Input data sampled from 133 A and 115 B audio files: train_data_A (501, 24, 128) train_data_B (501, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id017_iter182K.wav shape: (32960,)_loss: 6.17345619\n",
      "saved file at ./samples/cycle_A2A_id017_iter182K.wav shape: (32960,)\n",
      "Epoch[183]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id018_iter183K.wav shape: (117440,)loss: 6.07631016\n",
      "saved file at ./samples/cycle_A2A_id018_iter183K.wav shape: (117440,)\n",
      "Epoch[184]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id019_iter184K.wav shape: (37440,)_loss: 5.66527700\n",
      "saved file at ./samples/cycle_A2A_id019_iter184K.wav shape: (37440,)\n",
      "Epoch[185]: Input data sampled from 133 A and 115 B audio files: train_data_A (485, 24, 128) train_data_B (485, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id020_iter185K.wav shape: (34560,)_loss: 5.84497166\n",
      "saved file at ./samples/cycle_A2A_id020_iter185K.wav shape: (34560,)\n",
      "Epoch[186]: Input data sampled from 133 A and 115 B audio files: train_data_A (500, 24, 128) train_data_B (500, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id021_iter186K.wav shape: (34240,)_loss: 6.08599043\n",
      "saved file at ./samples/cycle_A2A_id021_iter186K.wav shape: (34240,)\n",
      "Epoch[187]: Input data sampled from 133 A and 115 B audio files: train_data_A (502, 24, 128) train_data_B (502, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id022_iter187K.wav shape: (23040,)_loss: 6.24435568\n",
      "saved file at ./samples/cycle_A2A_id022_iter187K.wav shape: (23040,)\n",
      "Epoch[188]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id023_iter188K.wav shape: (36800,)_loss: 5.71435642\n",
      "saved file at ./samples/cycle_A2A_id023_iter188K.wav shape: (36800,)\n",
      "Epoch[189]: Input data sampled from 133 A and 115 B audio files: train_data_A (508, 24, 128) train_data_B (508, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id024_iter189K.wav shape: (93440,)_loss: 6.01814699\n",
      "saved file at ./samples/cycle_A2A_id024_iter189K.wav shape: (93440,)\n",
      "Epoch[190]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id025_iter190K.wav shape: (95360,)_loss: 5.96615601\n",
      "saved file at ./samples/cycle_A2A_id025_iter190K.wav shape: (95360,)\n",
      "Epoch[191]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id026_iter191K.wav shape: (82240,)_loss: 5.80894947\n",
      "saved file at ./samples/cycle_A2A_id026_iter191K.wav shape: (82240,)\n",
      "Epoch[192]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id027_iter192K.wav shape: (49280,)_loss: 5.74592113\n",
      "saved file at ./samples/cycle_A2A_id027_iter192K.wav shape: (49280,)\n",
      "Epoch[193]: Input data sampled from 133 A and 115 B audio files: train_data_A (499, 24, 128) train_data_B (499, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id028_iter193K.wav shape: (43840,)_loss: 6.12048817\n",
      "saved file at ./samples/cycle_A2A_id028_iter193K.wav shape: (43840,)\n",
      "Epoch[194]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id029_iter194K.wav shape: (44160,)_loss: 6.05024338\n",
      "saved file at ./samples/cycle_A2A_id029_iter194K.wav shape: (44160,)\n",
      "Epoch[195]: Input data sampled from 133 A and 115 B audio files: train_data_A (491, 24, 128) train_data_B (491, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id030_iter195K.wav shape: (108160,)loss: 5.72712851\n",
      "saved file at ./samples/cycle_A2A_id030_iter195K.wav shape: (108160,)\n",
      "Epoch[196]: Input data sampled from 133 A and 115 B audio files: train_data_A (500, 24, 128) train_data_B (500, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id031_iter196K.wav shape: (73600,)_loss: 6.12002373\n",
      "saved file at ./samples/cycle_A2A_id031_iter196K.wav shape: (73600,)\n",
      "Epoch[197]: Input data sampled from 133 A and 115 B audio files: train_data_A (487, 24, 128) train_data_B (487, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id032_iter197K.wav shape: (47680,)_loss: 5.97610378\n",
      "saved file at ./samples/cycle_A2A_id032_iter197K.wav shape: (47680,)\n",
      "Epoch[198]: Input data sampled from 133 A and 115 B audio files: train_data_A (491, 24, 128) train_data_B (491, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id000_iter198K.wav shape: (142080,)loss: 5.70006371\n",
      "saved file at ./samples/cycle_A2A_id000_iter198K.wav shape: (142080,)\n",
      "Epoch[199]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id001_iter199K.wav shape: (65280,)_loss: 5.68411350\n",
      "saved file at ./samples/cycle_A2A_id001_iter199K.wav shape: (65280,)\n",
      "Epoch[200]: Input data sampled from 133 A and 115 B audio files: train_data_A (487, 24, 128) train_data_B (487, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id002_iter200K.wav shape: (23360,)_loss: 5.97361469\n",
      "saved file at ./samples/cycle_A2A_id002_iter200K.wav shape: (23360,)\n",
      "Epoch[201]: Input data sampled from 133 A and 115 B audio files: train_data_A (489, 24, 128) train_data_B (489, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id003_iter201K.wav shape: (94720,)_loss: 6.03039074\n",
      "saved file at ./samples/cycle_A2A_id003_iter201K.wav shape: (94720,)\n",
      "Epoch[202]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id004_iter202K.wav shape: (30400,)_loss: 5.45231771\n",
      "saved file at ./samples/cycle_A2A_id004_iter202K.wav shape: (30400,)\n",
      "Epoch[203]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id005_iter203K.wav shape: (136000,)loss: 5.68801355\n",
      "saved file at ./samples/cycle_A2A_id005_iter203K.wav shape: (136000,)\n",
      "Epoch[204]: Input data sampled from 133 A and 115 B audio files: train_data_A (502, 24, 128) train_data_B (502, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id006_iter204K.wav shape: (93440,)_loss: 5.96495056\n",
      "saved file at ./samples/cycle_A2A_id006_iter204K.wav shape: (93440,)\n",
      "Epoch[205]: Input data sampled from 133 A and 115 B audio files: train_data_A (500, 24, 128) train_data_B (500, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id007_iter205K.wav shape: (18240,)_loss: 5.94767380\n",
      "saved file at ./samples/cycle_A2A_id007_iter205K.wav shape: (18240,)\n",
      "Epoch[206]: Input data sampled from 133 A and 115 B audio files: train_data_A (487, 24, 128) train_data_B (487, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id008_iter206K.wav shape: (31040,)_loss: 5.69317341\n",
      "saved file at ./samples/cycle_A2A_id008_iter206K.wav shape: (31040,)\n",
      "Epoch[207]: Input data sampled from 133 A and 115 B audio files: train_data_A (499, 24, 128) train_data_B (499, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id009_iter207K.wav shape: (83200,)_loss: 5.83293104\n",
      "saved file at ./samples/cycle_A2A_id009_iter207K.wav shape: (83200,)\n",
      "Epoch[208]: Input data sampled from 133 A and 115 B audio files: train_data_A (501, 24, 128) train_data_B (501, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id010_iter208K.wav shape: (39360,)_loss: 5.69599438\n",
      "saved file at ./samples/cycle_A2A_id010_iter208K.wav shape: (39360,)\n",
      "Epoch[209]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id011_iter209K.wav shape: (27520,)_loss: 5.81481647\n",
      "saved file at ./samples/cycle_A2A_id011_iter209K.wav shape: (27520,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[210]: Input data sampled from 133 A and 115 B audio files: train_data_A (487, 24, 128) train_data_B (487, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id012_iter210K.wav shape: (59200,)_loss: 5.66375494\n",
      "saved file at ./samples/cycle_A2A_id012_iter210K.wav shape: (59200,)\n",
      "Epoch[211]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id013_iter211K.wav shape: (39040,)_loss: 5.50444651\n",
      "saved file at ./samples/cycle_A2A_id013_iter211K.wav shape: (39040,)\n",
      "Epoch[212]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id014_iter212K.wav shape: (30400,)_loss: 5.55246162\n",
      "saved file at ./samples/cycle_A2A_id014_iter212K.wav shape: (30400,)\n",
      "Epoch[213]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id015_iter213K.wav shape: (138240,)loss: 5.66249657\n",
      "saved file at ./samples/cycle_A2A_id015_iter213K.wav shape: (138240,)\n",
      "Epoch[214]: Input data sampled from 133 A and 115 B audio files: train_data_A (487, 24, 128) train_data_B (487, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id016_iter214K.wav shape: (59200,)_loss: 5.69374132\n",
      "saved file at ./samples/cycle_A2A_id016_iter214K.wav shape: (59200,)\n",
      "Epoch[215]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id017_iter215K.wav shape: (32960,)_loss: 5.37837648\n",
      "saved file at ./samples/cycle_A2A_id017_iter215K.wav shape: (32960,)\n",
      "Epoch[216]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id018_iter216K.wav shape: (117440,)loss: 5.77250099\n",
      "saved file at ./samples/cycle_A2A_id018_iter216K.wav shape: (117440,)\n",
      "Epoch[217]: Input data sampled from 133 A and 115 B audio files: train_data_A (491, 24, 128) train_data_B (491, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id019_iter217K.wav shape: (37440,)_loss: 5.65666151\n",
      "saved file at ./samples/cycle_A2A_id019_iter217K.wav shape: (37440,)\n",
      "Epoch[218]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id020_iter218K.wav shape: (34560,)_loss: 5.56925726\n",
      "saved file at ./samples/cycle_A2A_id020_iter218K.wav shape: (34560,)\n",
      "Epoch[219]: Input data sampled from 133 A and 115 B audio files: train_data_A (484, 24, 128) train_data_B (484, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id021_iter219K.wav shape: (34240,)_loss: 5.81637478\n",
      "saved file at ./samples/cycle_A2A_id021_iter219K.wav shape: (34240,)\n",
      "Epoch[220]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id022_iter220K.wav shape: (23040,)_loss: 5.78524971\n",
      "saved file at ./samples/cycle_A2A_id022_iter220K.wav shape: (23040,)\n",
      "Epoch[221]: Input data sampled from 133 A and 115 B audio files: train_data_A (504, 24, 128) train_data_B (504, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id023_iter221K.wav shape: (36800,)_loss: 5.62287951\n",
      "saved file at ./samples/cycle_A2A_id023_iter221K.wav shape: (36800,)\n",
      "Epoch[222]: Input data sampled from 133 A and 115 B audio files: train_data_A (497, 24, 128) train_data_B (497, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id024_iter222K.wav shape: (93440,)_loss: 5.82907009\n",
      "saved file at ./samples/cycle_A2A_id024_iter222K.wav shape: (93440,)\n",
      "Epoch[223]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id025_iter223K.wav shape: (95360,)_loss: 5.34303379\n",
      "saved file at ./samples/cycle_A2A_id025_iter223K.wav shape: (95360,)\n",
      "Epoch[224]: Input data sampled from 133 A and 115 B audio files: train_data_A (501, 24, 128) train_data_B (501, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id026_iter224K.wav shape: (82240,)_loss: 5.52803516\n",
      "saved file at ./samples/cycle_A2A_id026_iter224K.wav shape: (82240,)\n",
      "Epoch[225]: Input data sampled from 133 A and 115 B audio files: train_data_A (488, 24, 128) train_data_B (488, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id027_iter225K.wav shape: (49280,)_loss: 5.62143087\n",
      "saved file at ./samples/cycle_A2A_id027_iter225K.wav shape: (49280,)\n",
      "Epoch[226]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id028_iter226K.wav shape: (43840,)_loss: 5.32153463\n",
      "saved file at ./samples/cycle_A2A_id028_iter226K.wav shape: (43840,)\n",
      "Epoch[227]: Input data sampled from 133 A and 115 B audio files: train_data_A (488, 24, 128) train_data_B (488, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id029_iter227K.wav shape: (44160,)_loss: 5.62860680\n",
      "saved file at ./samples/cycle_A2A_id029_iter227K.wav shape: (44160,)\n",
      "Epoch[228]: Input data sampled from 133 A and 115 B audio files: train_data_A (482, 24, 128) train_data_B (482, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id030_iter228K.wav shape: (108160,)loss: 5.61442089\n",
      "saved file at ./samples/cycle_A2A_id030_iter228K.wav shape: (108160,)\n",
      "Epoch[229]: Input data sampled from 133 A and 115 B audio files: train_data_A (486, 24, 128) train_data_B (486, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id031_iter229K.wav shape: (73600,)_loss: 5.07508469\n",
      "saved file at ./samples/cycle_A2A_id031_iter229K.wav shape: (73600,)\n",
      "Epoch[230]: Input data sampled from 133 A and 115 B audio files: train_data_A (499, 24, 128) train_data_B (499, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id032_iter230K.wav shape: (47680,)_loss: 6.13652754\n",
      "saved file at ./samples/cycle_A2A_id032_iter230K.wav shape: (47680,)\n",
      "Epoch[231]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id000_iter231K.wav shape: (142080,)loss: 5.32674313\n",
      "saved file at ./samples/cycle_A2A_id000_iter231K.wav shape: (142080,)\n",
      "Epoch[232]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id001_iter232K.wav shape: (65280,)_loss: 5.73921013\n",
      "saved file at ./samples/cycle_A2A_id001_iter232K.wav shape: (65280,)\n",
      "Epoch[233]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id002_iter233K.wav shape: (23360,)_loss: 5.37890482\n",
      "saved file at ./samples/cycle_A2A_id002_iter233K.wav shape: (23360,)\n",
      "Epoch[234]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id003_iter234K.wav shape: (94720,)_loss: 5.41387558\n",
      "saved file at ./samples/cycle_A2A_id003_iter234K.wav shape: (94720,)\n",
      "Epoch[235]: Input data sampled from 133 A and 115 B audio files: train_data_A (488, 24, 128) train_data_B (488, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id004_iter235K.wav shape: (30400,)_loss: 5.33250952\n",
      "saved file at ./samples/cycle_A2A_id004_iter235K.wav shape: (30400,)\n",
      "Epoch[236]: Input data sampled from 133 A and 115 B audio files: train_data_A (491, 24, 128) train_data_B (491, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id005_iter236K.wav shape: (136000,)loss: 5.46693420\n",
      "saved file at ./samples/cycle_A2A_id005_iter236K.wav shape: (136000,)\n",
      "Epoch[237]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id006_iter237K.wav shape: (93440,)_loss: 5.60903168\n",
      "saved file at ./samples/cycle_A2A_id006_iter237K.wav shape: (93440,)\n",
      "Epoch[238]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id007_iter238K.wav shape: (18240,)_loss: 5.71072006\n",
      "saved file at ./samples/cycle_A2A_id007_iter238K.wav shape: (18240,)\n",
      "Epoch[239]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id008_iter239K.wav shape: (31040,)_loss: 5.52721691\n",
      "saved file at ./samples/cycle_A2A_id008_iter239K.wav shape: (31040,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[240]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id009_iter240K.wav shape: (83200,)_loss: 5.55506039\n",
      "saved file at ./samples/cycle_A2A_id009_iter240K.wav shape: (83200,)\n",
      "Epoch[241]: Input data sampled from 133 A and 115 B audio files: train_data_A (500, 24, 128) train_data_B (500, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id010_iter241K.wav shape: (39360,)_loss: 5.59537888\n",
      "saved file at ./samples/cycle_A2A_id010_iter241K.wav shape: (39360,)\n",
      "Epoch[242]: Input data sampled from 133 A and 115 B audio files: train_data_A (502, 24, 128) train_data_B (502, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id011_iter242K.wav shape: (27520,)_loss: 5.32223797\n",
      "saved file at ./samples/cycle_A2A_id011_iter242K.wav shape: (27520,)\n",
      "Epoch[243]: Input data sampled from 133 A and 115 B audio files: train_data_A (486, 24, 128) train_data_B (486, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id012_iter243K.wav shape: (59200,)_loss: 5.27530146\n",
      "saved file at ./samples/cycle_A2A_id012_iter243K.wav shape: (59200,)\n",
      "Epoch[244]: Input data sampled from 133 A and 115 B audio files: train_data_A (497, 24, 128) train_data_B (497, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id013_iter244K.wav shape: (39040,)_loss: 5.52472591\n",
      "saved file at ./samples/cycle_A2A_id013_iter244K.wav shape: (39040,)\n",
      "Epoch[245]: Input data sampled from 133 A and 115 B audio files: train_data_A (506, 24, 128) train_data_B (506, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id014_iter245K.wav shape: (30400,)_loss: 5.08820629\n",
      "saved file at ./samples/cycle_A2A_id014_iter245K.wav shape: (30400,)\n",
      "Epoch[246]: Input data sampled from 133 A and 115 B audio files: train_data_A (486, 24, 128) train_data_B (486, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id015_iter246K.wav shape: (138240,)loss: 5.44797993\n",
      "saved file at ./samples/cycle_A2A_id015_iter246K.wav shape: (138240,)\n",
      "Epoch[247]: Input data sampled from 133 A and 115 B audio files: train_data_A (492, 24, 128) train_data_B (492, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id016_iter247K.wav shape: (59200,)_loss: 5.43386364\n",
      "saved file at ./samples/cycle_A2A_id016_iter247K.wav shape: (59200,)\n",
      "Epoch[248]: Input data sampled from 133 A and 115 B audio files: train_data_A (513, 24, 128) train_data_B (513, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id017_iter248K.wav shape: (32960,)_loss: 5.43979740\n",
      "saved file at ./samples/cycle_A2A_id017_iter248K.wav shape: (32960,)\n",
      "Epoch[249]: Input data sampled from 133 A and 115 B audio files: train_data_A (488, 24, 128) train_data_B (488, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id018_iter249K.wav shape: (117440,)loss: 5.32838488\n",
      "saved file at ./samples/cycle_A2A_id018_iter249K.wav shape: (117440,)\n",
      "Epoch[250]: Input data sampled from 133 A and 115 B audio files: train_data_A (486, 24, 128) train_data_B (486, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id019_iter250K.wav shape: (37440,)_loss: 5.38134861\n",
      "saved file at ./samples/cycle_A2A_id019_iter250K.wav shape: (37440,)\n",
      "Epoch[251]: Input data sampled from 133 A and 115 B audio files: train_data_A (499, 24, 128) train_data_B (499, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id020_iter251K.wav shape: (34560,)_loss: 5.33923149\n",
      "saved file at ./samples/cycle_A2A_id020_iter251K.wav shape: (34560,)\n",
      "Epoch[252]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id021_iter252K.wav shape: (34240,)_loss: 5.37334728\n",
      "saved file at ./samples/cycle_A2A_id021_iter252K.wav shape: (34240,)\n",
      "Epoch[253]: Input data sampled from 133 A and 115 B audio files: train_data_A (504, 24, 128) train_data_B (504, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id022_iter253K.wav shape: (23040,)_loss: 5.37210369\n",
      "saved file at ./samples/cycle_A2A_id022_iter253K.wav shape: (23040,)\n",
      "Epoch[254]: Input data sampled from 133 A and 115 B audio files: train_data_A (500, 24, 128) train_data_B (500, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id023_iter254K.wav shape: (36800,)_loss: 5.27759266\n",
      "saved file at ./samples/cycle_A2A_id023_iter254K.wav shape: (36800,)\n",
      "Epoch[255]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id024_iter255K.wav shape: (93440,)_loss: 5.17203712\n",
      "saved file at ./samples/cycle_A2A_id024_iter255K.wav shape: (93440,)\n",
      "Epoch[256]: Input data sampled from 133 A and 115 B audio files: train_data_A (486, 24, 128) train_data_B (486, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id025_iter256K.wav shape: (95360,)_loss: 5.47753000\n",
      "saved file at ./samples/cycle_A2A_id025_iter256K.wav shape: (95360,)\n",
      "Epoch[257]: Input data sampled from 133 A and 115 B audio files: train_data_A (493, 24, 128) train_data_B (493, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id026_iter257K.wav shape: (82240,)_loss: 5.40171337\n",
      "saved file at ./samples/cycle_A2A_id026_iter257K.wav shape: (82240,)\n",
      "Epoch[258]: Input data sampled from 133 A and 115 B audio files: train_data_A (491, 24, 128) train_data_B (491, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id027_iter258K.wav shape: (49280,)_loss: 5.30578518\n",
      "saved file at ./samples/cycle_A2A_id027_iter258K.wav shape: (49280,)\n",
      "Epoch[259]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id028_iter259K.wav shape: (43840,)_loss: 5.23054409\n",
      "saved file at ./samples/cycle_A2A_id028_iter259K.wav shape: (43840,)\n",
      "Epoch[260]: Input data sampled from 133 A and 115 B audio files: train_data_A (491, 24, 128) train_data_B (491, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id029_iter260K.wav shape: (44160,)_loss: 5.52872086\n",
      "saved file at ./samples/cycle_A2A_id029_iter260K.wav shape: (44160,)\n",
      "Epoch[261]: Input data sampled from 133 A and 115 B audio files: train_data_A (488, 24, 128) train_data_B (488, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id030_iter261K.wav shape: (108160,)loss: 5.06157732\n",
      "saved file at ./samples/cycle_A2A_id030_iter261K.wav shape: (108160,)\n",
      "Epoch[262]: Input data sampled from 133 A and 115 B audio files: train_data_A (497, 24, 128) train_data_B (497, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id031_iter262K.wav shape: (73600,)_loss: 5.63954544\n",
      "saved file at ./samples/cycle_A2A_id031_iter262K.wav shape: (73600,)\n",
      "Epoch[263]: Input data sampled from 133 A and 115 B audio files: train_data_A (500, 24, 128) train_data_B (500, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id032_iter263K.wav shape: (47680,)_loss: 5.52797699\n",
      "saved file at ./samples/cycle_A2A_id032_iter263K.wav shape: (47680,)\n",
      "Epoch[264]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id000_iter264K.wav shape: (142080,)loss: 5.01368475\n",
      "saved file at ./samples/cycle_A2A_id000_iter264K.wav shape: (142080,)\n",
      "Epoch[265]: Input data sampled from 133 A and 115 B audio files: train_data_A (506, 24, 128) train_data_B (506, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id001_iter265K.wav shape: (65280,)_loss: 5.05673313\n",
      "saved file at ./samples/cycle_A2A_id001_iter265K.wav shape: (65280,)\n",
      "Epoch[266]: Input data sampled from 133 A and 115 B audio files: train_data_A (489, 24, 128) train_data_B (489, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id002_iter266K.wav shape: (23360,)_loss: 5.40077543\n",
      "saved file at ./samples/cycle_A2A_id002_iter266K.wav shape: (23360,)\n",
      "Epoch[267]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id003_iter267K.wav shape: (94720,)_loss: 5.09463120\n",
      "saved file at ./samples/cycle_A2A_id003_iter267K.wav shape: (94720,)\n",
      "Epoch[268]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id004_iter268K.wav shape: (30400,)_loss: 4.96534920\n",
      "saved file at ./samples/cycle_A2A_id004_iter268K.wav shape: (30400,)\n",
      "Epoch[269]: Input data sampled from 133 A and 115 B audio files: train_data_A (497, 24, 128) train_data_B (497, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id005_iter269K.wav shape: (136000,)loss: 5.38064003\n",
      "saved file at ./samples/cycle_A2A_id005_iter269K.wav shape: (136000,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[270]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id006_iter270K.wav shape: (93440,)_loss: 5.27463341\n",
      "saved file at ./samples/cycle_A2A_id006_iter270K.wav shape: (93440,)\n",
      "Epoch[271]: Input data sampled from 133 A and 115 B audio files: train_data_A (502, 24, 128) train_data_B (502, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id007_iter271K.wav shape: (18240,)_loss: 5.20949602\n",
      "saved file at ./samples/cycle_A2A_id007_iter271K.wav shape: (18240,)\n",
      "Epoch[272]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id008_iter272K.wav shape: (31040,)_loss: 5.20069981\n",
      "saved file at ./samples/cycle_A2A_id008_iter272K.wav shape: (31040,)\n",
      "Epoch[273]: Input data sampled from 133 A and 115 B audio files: train_data_A (498, 24, 128) train_data_B (498, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id009_iter273K.wav shape: (83200,)_loss: 5.00515413\n",
      "saved file at ./samples/cycle_A2A_id009_iter273K.wav shape: (83200,)\n",
      "Epoch[274]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id010_iter274K.wav shape: (39360,)_loss: 5.54377747\n",
      "saved file at ./samples/cycle_A2A_id010_iter274K.wav shape: (39360,)\n",
      "Epoch[275]: Input data sampled from 133 A and 115 B audio files: train_data_A (489, 24, 128) train_data_B (489, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id011_iter275K.wav shape: (27520,)_loss: 4.83543539\n",
      "saved file at ./samples/cycle_A2A_id011_iter275K.wav shape: (27520,)\n",
      "Epoch[276]: Input data sampled from 133 A and 115 B audio files: train_data_A (500, 24, 128) train_data_B (500, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id012_iter276K.wav shape: (59200,)_loss: 5.09652805\n",
      "saved file at ./samples/cycle_A2A_id012_iter276K.wav shape: (59200,)\n",
      "Epoch[277]: Input data sampled from 133 A and 115 B audio files: train_data_A (497, 24, 128) train_data_B (497, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id013_iter277K.wav shape: (39040,)_loss: 5.40972090\n",
      "saved file at ./samples/cycle_A2A_id013_iter277K.wav shape: (39040,)\n",
      "Epoch[278]: Input data sampled from 133 A and 115 B audio files: train_data_A (500, 24, 128) train_data_B (500, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id014_iter278K.wav shape: (30400,)_loss: 5.43365288\n",
      "saved file at ./samples/cycle_A2A_id014_iter278K.wav shape: (30400,)\n",
      "Epoch[279]: Input data sampled from 133 A and 115 B audio files: train_data_A (487, 24, 128) train_data_B (487, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id015_iter279K.wav shape: (138240,)loss: 5.04585171\n",
      "saved file at ./samples/cycle_A2A_id015_iter279K.wav shape: (138240,)\n",
      "Epoch[280]: Input data sampled from 133 A and 115 B audio files: train_data_A (497, 24, 128) train_data_B (497, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id016_iter280K.wav shape: (59200,)_loss: 5.12623215\n",
      "saved file at ./samples/cycle_A2A_id016_iter280K.wav shape: (59200,)\n",
      "Epoch[281]: Input data sampled from 133 A and 115 B audio files: train_data_A (488, 24, 128) train_data_B (488, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id017_iter281K.wav shape: (32960,)_loss: 5.11963320\n",
      "saved file at ./samples/cycle_A2A_id017_iter281K.wav shape: (32960,)\n",
      "Epoch[282]: Input data sampled from 133 A and 115 B audio files: train_data_A (489, 24, 128) train_data_B (489, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id018_iter282K.wav shape: (117440,)loss: 5.03117085\n",
      "saved file at ./samples/cycle_A2A_id018_iter282K.wav shape: (117440,)\n",
      "Epoch[283]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id019_iter283K.wav shape: (37440,)_loss: 5.10791588\n",
      "saved file at ./samples/cycle_A2A_id019_iter283K.wav shape: (37440,)\n",
      "Epoch[284]: Input data sampled from 133 A and 115 B audio files: train_data_A (502, 24, 128) train_data_B (502, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id020_iter284K.wav shape: (34560,)_loss: 4.85984135\n",
      "saved file at ./samples/cycle_A2A_id020_iter284K.wav shape: (34560,)\n",
      "Epoch[285]: Input data sampled from 133 A and 115 B audio files: train_data_A (494, 24, 128) train_data_B (494, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id021_iter285K.wav shape: (34240,)_loss: 5.05809784\n",
      "saved file at ./samples/cycle_A2A_id021_iter285K.wav shape: (34240,)\n",
      "Epoch[286]: Input data sampled from 133 A and 115 B audio files: train_data_A (490, 24, 128) train_data_B (490, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id022_iter286K.wav shape: (23040,)_loss: 4.89135551\n",
      "saved file at ./samples/cycle_A2A_id022_iter286K.wav shape: (23040,)\n",
      "Epoch[287]: Input data sampled from 133 A and 115 B audio files: train_data_A (486, 24, 128) train_data_B (486, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id023_iter287K.wav shape: (36800,)_loss: 5.02583790\n",
      "saved file at ./samples/cycle_A2A_id023_iter287K.wav shape: (36800,)\n",
      "Epoch[288]: Input data sampled from 133 A and 115 B audio files: train_data_A (499, 24, 128) train_data_B (499, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id024_iter288K.wav shape: (93440,)_loss: 5.10939550\n",
      "saved file at ./samples/cycle_A2A_id024_iter288K.wav shape: (93440,)\n",
      "Epoch[289]: Input data sampled from 133 A and 115 B audio files: train_data_A (496, 24, 128) train_data_B (496, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id025_iter289K.wav shape: (95360,)_loss: 5.16326857\n",
      "saved file at ./samples/cycle_A2A_id025_iter289K.wav shape: (95360,)\n",
      "Epoch[290]: Input data sampled from 133 A and 115 B audio files: train_data_A (491, 24, 128) train_data_B (491, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id026_iter290K.wav shape: (82240,)_loss: 5.19942188\n",
      "saved file at ./samples/cycle_A2A_id026_iter290K.wav shape: (82240,)\n",
      "Epoch[291]: Input data sampled from 133 A and 115 B audio files: train_data_A (499, 24, 128) train_data_B (499, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id027_iter291K.wav shape: (49280,)_loss: 5.37335014\n",
      "saved file at ./samples/cycle_A2A_id027_iter291K.wav shape: (49280,)\n",
      "Epoch[292]: Input data sampled from 133 A and 115 B audio files: train_data_A (484, 24, 128) train_data_B (484, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id028_iter292K.wav shape: (43840,)_loss: 5.27812529\n",
      "saved file at ./samples/cycle_A2A_id028_iter292K.wav shape: (43840,)\n",
      "Epoch[293]: Input data sampled from 133 A and 115 B audio files: train_data_A (501, 24, 128) train_data_B (501, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id029_iter293K.wav shape: (44160,)_loss: 5.22180986\n",
      "saved file at ./samples/cycle_A2A_id029_iter293K.wav shape: (44160,)\n",
      "Epoch[294]: Input data sampled from 133 A and 115 B audio files: train_data_A (497, 24, 128) train_data_B (497, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id030_iter294K.wav shape: (108160,)loss: 4.86833477\n",
      "saved file at ./samples/cycle_A2A_id030_iter294K.wav shape: (108160,)\n",
      "Epoch[295]: Input data sampled from 133 A and 115 B audio files: train_data_A (502, 24, 128) train_data_B (502, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id031_iter295K.wav shape: (73600,)_loss: 5.38249445\n",
      "saved file at ./samples/cycle_A2A_id031_iter295K.wav shape: (73600,)\n",
      "Epoch[296]: Input data sampled from 133 A and 115 B audio files: train_data_A (500, 24, 128) train_data_B (500, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id032_iter296K.wav shape: (47680,)_loss: 4.88976479\n",
      "saved file at ./samples/cycle_A2A_id032_iter296K.wav shape: (47680,)\n",
      "Epoch[297]: Input data sampled from 133 A and 115 B audio files: train_data_A (495, 24, 128) train_data_B (495, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id000_iter297K.wav shape: (142080,)loss: 4.86337757\n",
      "saved file at ./samples/cycle_A2A_id000_iter297K.wav shape: (142080,)\n",
      "Epoch[298]: Input data sampled from 133 A and 115 B audio files: train_data_A (503, 24, 128) train_data_B (503, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id001_iter298K.wav shape: (65280,)_loss: 5.06435728\n",
      "saved file at ./samples/cycle_A2A_id001_iter298K.wav shape: (65280,)\n",
      "Epoch[299]: Input data sampled from 133 A and 115 B audio files: train_data_A (506, 24, 128) train_data_B (506, 24, 128)\n",
      "saved file at ./samples/fake_A2B_id002_iter299K.wav shape: (23360,)_loss: 5.11679840\n",
      "saved file at ./samples/cycle_A2A_id002_iter299K.wav shape: (23360,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] Training finished!1000] time: 73376.3204 d_loss: 0.00000043, g_loss: 4.92802477\n",
      " [*] Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/C_hap_neu_lsgan/C.model-300001\n",
      " [*] Success to read C.model-300001\n",
      " [*] Load SUCCESS\n",
      "std_log_src: 0.39167277853304816 std_log_target 0.2822559084794371\n",
      " [*] Testing finished!s B2A: [28/28]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    model = CYCLE(sess, folder='S02/', source='hap', target='sad')\n",
    "    model.build_model()\n",
    "    model.train()\n",
    "    model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
